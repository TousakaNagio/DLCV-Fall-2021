{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLCV_HW2_ACGAN_ver2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FCZW65R4oGL",
        "outputId": "345a21a9-9989-4bb5-8dde-55febf119012"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 21 14:49:33 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShPnkT2y4tWH",
        "outputId": "ed1c318e-4a87-4aad-d0d2-deab2a97251b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_maq07rE4v75",
        "outputId": "7b4a17e2-3875-4cfd-d65a-0e0a9a831fce"
      },
      "source": [
        "!gdown --id 1Z_RSO1UKRfJXdL47MthB0iPpSXRNqd1D --output \"data.zip\"\n",
        "!unzip -q \"data.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Z_RSO1UKRfJXdL47MthB0iPpSXRNqd1D\n",
            "To: /content/data.zip\n",
            "100% 642M/642M [00:03<00:00, 205MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp3DvCr44yd5",
        "outputId": "f0ca8356-a6a8-44c7-9ddb-376088391f09"
      },
      "source": [
        "! git clone https://github.com/eriklindernoren/PyTorch-GAN\n",
        "# cd PyTorch-GAN/\n",
        "# ! sudo pip3 install -r requirements.txt/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PyTorch-GAN'...\n",
            "remote: Enumerating objects: 1283, done.\u001b[K\n",
            "remote: Total 1283 (delta 0), reused 0 (delta 0), pack-reused 1283\u001b[K\n",
            "Receiving objects: 100% (1283/1283), 68.04 MiB | 9.82 MiB/s, done.\n",
            "Resolving deltas: 100% (751/751), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tykfdIaLixGa",
        "outputId": "38f3c32b-f1a9-4ac2-c56e-639cfac36b09"
      },
      "source": [
        "# ! git clone https://github.com/eriklindernoren/PyTorch-GAN\n",
        "%cd PyTorch-GAN/\n",
        "! sudo pip3 install -r requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'PyTorch-GAN/'\n",
            "/content/PyTorch-GAN\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.10.0+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.16.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->-r requirements.txt (line 8)) (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq4anzFH5gqX",
        "outputId": "00fb84b7-8f32-4387-e274-a7c849fad33f"
      },
      "source": [
        "%cd PyTorch-GAN/\n",
        "! sudo pip3 install -r requirements"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PyTorch-GAN\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "oAxZQFyZ42fL",
        "outputId": "6baa2e2e-db13-4b08-897b-d3269a5cc764"
      },
      "source": [
        "%cd implementations/acgan/\n",
        "# $ python3 acgan.py√∑\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-98ac22bde3c7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    $ cd implementations/acgan/\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDmumwt3-dcg"
      },
      "source": [
        "! cp -r /content/drive/MyDrive/DLCV/HW2/models_3 /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrItZJoDxS2-",
        "outputId": "1b76bf5d-0621-46dd-d8c0-d6ad9ce6d3c0"
      },
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "import os, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "train_data = sorted(glob.glob(os.path.join('/content/hw2_data/digits/mnistm/train', '*.png')))\n",
        "test_data = sorted(glob.glob(os.path.join('/content/hw2_data/digits/mnistm/test', '*.png')))\n",
        "\n",
        "train_X = []\n",
        "\n",
        "for fn in train_data:\n",
        "  image = Image.open(fn)\n",
        "  train_X.append(image)\n",
        "\n",
        "for fn in test_data:\n",
        "  image = Image.open(fn)\n",
        "  train_X.append(image)\n",
        "\n",
        "# for fn in train_data:\n",
        "#   image = Image.open(fn)\n",
        "#   train_X.append(np.array(image))\n",
        "\n",
        "# for fn in test_data:\n",
        "#   image = Image.open(fn)\n",
        "#   train_X.append(np.array(image))\n",
        "\n",
        "# load data\n",
        "# train_X = np.load(\"all_img_flip.npy\")\n",
        "train_attr = pd.read_csv(\"/content/hw2_data/digits/mnistm/train.csv\")\n",
        "test_attr = pd.read_csv(\"/content/hw2_data/digits/mnistm/test.csv\")\n",
        "\n",
        "print(type(train_attr.iloc[2,1]))\n",
        "\n",
        "smiling_attr = []\n",
        "\n",
        "for i in range(60000):\n",
        "  smiling_attr.append(train_attr.iloc[i,1])\n",
        "\n",
        "for j in range(10000):\n",
        "  smiling_attr.append(test_attr.iloc[j,1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(len(smiling_attr), type(smiling_attr))\n",
        "print(type(smiling_attr))\n",
        "# print(smiling_attr.shape)\n",
        "print(smiling_attr[0:6])\n",
        "\n",
        "# smiling_attr = np.hstack((np.repeat(np.array(train_attr[\"Smiling\"]),2) \n",
        "#                           ,np.repeat(np.array(test_attr[\"Smiling\"]),2)\n",
        "#                          ))\n",
        "# smiling_attr = np.hstack((np.array(train_attr[\"Smiling\"])\n",
        "#                           ,np.array(test_attr[\"Smiling\"]))\n",
        "#                          )\n",
        "\n",
        "# type transform\n",
        "# img_X = torch.from_numpy(train_X).type(torch.FloatTensor)\n",
        "# class_X = torch.from_numpy(smiling_attr).type(torch.FloatTensor).view(-1,1,1,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.int64'>\n",
            "70000 <class 'list'>\n",
            "<class 'list'>\n",
            "[5, 0, 4, 1, 9, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qGqx1l6-BRL",
        "outputId": "f4d07eb0-8038-437f-837d-c6270cff7235"
      },
      "source": [
        "print(len(train_X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmuI1yUMz3Q8"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, dir, csv):\n",
        "    self.imgs = dir\n",
        "    self.labels = csv\n",
        "    self.transform = transforms.Compose([\n",
        "      transforms.Resize(opt.img_size),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.5], [0.5])                                         \n",
        "    ])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.transform(self.imgs[index]), self.labels[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKz8vu6Q6C12"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "uJbtak6PikGP",
        "outputId": "d927af00-7f33-4e07-8331-e1143b6e21fc"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from argparse import Namespace\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "same_seeds(1126)\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
        "# parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
        "# parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "# parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "# parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "# parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "# parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
        "# parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
        "# parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
        "# parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "# parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
        "# opt = parser.parse_args()\n",
        "# print(opt)\n",
        "\n",
        "parameters = {\n",
        "    \"n_epochs\": 200,\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": 0.0002,\n",
        "    \"b1\": 0.5,\n",
        "    \"b2\": 0.999,\n",
        "    \"n_cpu\": 8,\n",
        "    \"latent_dim\": 100,\n",
        "    \"n_classes\": 10,\n",
        "    \"img_size\": 64,\n",
        "    \"channels\": 3,\n",
        "    \"sample_interval\": 400,\n",
        "}\n",
        "opt = Namespace(**parameters)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n",
        "\n",
        "        self.init_size = opt.img_size // 4  # Initial size before upsampling\n",
        "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
        "        out = self.l1(gen_input)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, bn=True):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
        "            if bn:\n",
        "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "            return block\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            *discriminator_block(opt.channels, 16, bn=False),\n",
        "            *discriminator_block(16, 32),\n",
        "            *discriminator_block(32, 64),\n",
        "            *discriminator_block(64, 128),\n",
        "        )\n",
        "\n",
        "        # The height and width of downsampled image\n",
        "        ds_size = opt.img_size // 2 ** 4\n",
        "\n",
        "        # Output layers\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
        "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.conv_blocks(img)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        label = self.aux_layer(out)\n",
        "\n",
        "        return validity, label\n",
        "\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    auxiliary_loss.cuda()\n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n",
        "\n",
        "# Configure data loader\n",
        "# os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
        "# dataloader = torch.utils.data.DataLoader(\n",
        "#     datasets.MNIST(\n",
        "#         \"../../data/mnist\",\n",
        "#         train=True,\n",
        "#         download=True,\n",
        "#         transform=transforms.Compose(\n",
        "#             [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "#         ),\n",
        "#     ),\n",
        "#     batch_size=opt.batch_size,\n",
        "#     shuffle=True,\n",
        "# )\n",
        "myset = MyDataset(train_X, smiling_attr)\n",
        "dataloader = DataLoader(myset)\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "\n",
        "\n",
        "def sample_image(n_row, batches_done):\n",
        "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
        "    # Sample noise\n",
        "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
        "    # Get labels ranging from 0 to n_classes for n rows\n",
        "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
        "    labels = Variable(LongTensor(labels))\n",
        "    gen_imgs = generator(z, labels)\n",
        "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
        "\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, labels) in enumerate(dataloader):\n",
        "\n",
        "        batch_size = imgs.shape[0]\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(FloatTensor))\n",
        "        labels = Variable(labels.type(LongTensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise and labels as generator input\n",
        "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
        "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        validity, pred_label = discriminator(gen_imgs)\n",
        "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Loss for real images\n",
        "        real_pred, real_aux = discriminator(real_imgs)\n",
        "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
        "\n",
        "        # Loss for fake images\n",
        "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
        "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        # Calculate discriminator accuracy\n",
        "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
        "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
        "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\"\n",
        "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, g_loss.item())\n",
        "        )\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % opt.sample_interval == 0:\n",
        "            sample_image(n_row=10, batches_done=batches_done)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ef4ee39d3828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/models_3/26_generator.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/models_3/26_discriminator.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tsize mismatch for l1.0.weight: copying a param with shape torch.Size([8192, 100]) from checkpoint, the shape in current model is torch.Size([32768, 100]).\n\tsize mismatch for l1.0.bias: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([32768])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUMuQ5aB5Zr1"
      },
      "source": [
        "torch.save(generator.state_dict(), \"/content/models/ACG2_model_g.pkt\")\n",
        "torch.save(discriminator.state_dict(), \"/content/models/ACG2_model_d.pkt\")\n",
        "\n",
        "# model = Generator().to('cuda')\n",
        "# state = torch.load('/content/models_2/models_2/ACG2_model.pkt')\n",
        "# model.load_state_dict(state)\n",
        "# model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEDWvdMz_nLr"
      },
      "source": [
        "torch.save(generator, \"/content/models/ACG2_model_g_state.pkt\")\n",
        "torch.save(discriminator, \"/content/models/ACG2_model_d_state.pkt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1nuvVAofs65"
      },
      "source": [
        "def load_checkpoint(checkpoint_path, model):\n",
        "    state = torch.load(checkpoint_path, map_location = \"cuda\")\n",
        "    # model.load_state_dict(state['state_dict'])\n",
        "    model.load_state_dict(state)\n",
        "    print('model loaded from %s' % checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW4jhqdi-rcb",
        "outputId": "6eaad90e-786c-442c-810a-3c4171420267"
      },
      "source": [
        "load_checkpoint('/content/models/ACG2_model_g.pkt', generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded from /content/models/ACG2_model_g.pkt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu-ve3DM8iY5"
      },
      "source": [
        "# ! cp -r /content/images /content/drive/MyDrive/DLCV/HW2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg_23aYJnoal"
      },
      "source": [
        "! cp -r /content/drive/MyDrive/DLCV/HW2/models_2 /content/models_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP8EQsJ0nmM0"
      },
      "source": [
        "model = Generator().to('cuda')\n",
        "state = torch.load('/content/models_2/models_2/ACG2_model.pkt')\n",
        "model.load_state_dict(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayDP5f1BhFPN"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model):\n",
        "    state = torch.load(checkpoint_path, map_location = \"cuda\")\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    print('model loaded from %s' % checkpoint_path)\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # load digit classifier\n",
        "    net = Classifier()\n",
        "    path = \"Classifier.pth\"\n",
        "    load_checkpoint(path, net)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "      transforms.Resize(28),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.5], [0.5])                                         \n",
        "    ])\n",
        "\n",
        "    # GPU enable\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print('Device used:', device)\n",
        "    if torch.cuda.is_available():\n",
        "        net = net.to(device)\n",
        "\n",
        "    print(net)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}