{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLCV_SSL2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04ebf98-8e7a-4c38-db79-58e8e95e94db",
        "id": "mzFqwx8V6buj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 29 14:42:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a080f6fe-30b6-4d3e-a0ef-6e01ad31bff8",
        "id": "iWSfLulz6buk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YuzCW6s56tV409JRXaAMKFdIkK9xIACw\n",
            "To: /content/data.zip\n",
            "100% 1.13G/1.13G [00:04<00:00, 231MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1YuzCW6s56tV409JRXaAMKFdIkK9xIACw --output \"data.zip\"\n",
        "!unzip -q \"data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640e5fc5-e471-43c2-8250-c33c08169c00",
        "id": "GYtD3j7h6buk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910e8db6-2881-4fed-9984-9ae8a09d9b25",
        "id": "4Viidxq86buk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting byol-pytorch\n",
            "  Downloading byol_pytorch-0.5.7-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: torchvision>=0.8 in /usr/local/lib/python3.7/dist-packages (from byol-pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from byol-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->byol-pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8->byol-pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8->byol-pytorch) (7.1.2)\n",
            "Installing collected packages: byol-pytorch\n",
            "Successfully installed byol-pytorch-0.5.7\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 715 kB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "! pip install byol-pytorch\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r /content/drive/MyDrive/DLCV/HW4/improved-net4.pt /content/\n",
        "# ! cp -r /content/drive/MyDrive/DLCV/HW4/checkpoint_p2/model2511.pth /content"
      ],
      "metadata": {
        "id": "mpoRMG5SEwM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2A0UaNO6buk"
      },
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(1126)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq3YIj_J6bul"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrp5Nd9L6bul"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch import optim\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "filenameToPILImage = lambda x: Image.open(x)\n",
        "\n",
        "import torch\n",
        "# from byol_pytorch import BYOL\n",
        "from torchvision import models\n",
        "\n",
        "# mini-Imagenet dataset\n",
        "class OfficeDataset(Dataset):\n",
        "    def __init__(self, csv_path, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.data_df = pd.read_csv(csv_path).set_index(\"id\")\n",
        "        self.labels = list(sorted(set(pd.read_csv(csv_path).iloc[:,2])))\n",
        "\n",
        "        # self.transform = transforms.Compose([\n",
        "        #     filenameToPILImage,\n",
        "        #     transforms.ToTensor(),\n",
        "        #     transforms.Resize((128, 128)),\n",
        "        #     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        #     ])\n",
        "        self.transform = transforms\n",
        "        \n",
        "        self.label_dict = {}\n",
        "        for id, lab in enumerate(self.labels):\n",
        "          self.label_dict[lab] = id\n",
        "        \n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.data_df.loc[index, \"filename\"]\n",
        "        label_str = self.data_df.loc[index, \"label\"]\n",
        "        image = self.transform(os.path.join(self.data_dir, path))\n",
        "        label = self.label_dict[label_str]\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "        filenameToPILImage,\n",
        "        transforms.AutoAugment(),\n",
        "        transforms.ToTensor(),\n",
        "        \n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "        filenameToPILImage,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "Mn_wwC3zlYQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOF6loUa6bul"
      },
      "outputs": [],
      "source": [
        "test_set = OfficeDataset('/content/hw4_data/office/val.csv', '/content/hw4_data/office/val', transforms = val_transform)\n",
        "train_set = OfficeDataset('/content/hw4_data/office/train.csv', '/content/hw4_data/office/train', transforms = train_transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set.label_dict)\n",
        "print(test_set.label_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5808799b-c2b0-4e5c-9bb2-eb8f44f18584",
        "id": "By7xQFUQ6bum"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Alarm_Clock': 0, 'Backpack': 1, 'Batteries': 2, 'Bed': 3, 'Bike': 4, 'Bottle': 5, 'Bucket': 6, 'Calculator': 7, 'Calendar': 8, 'Candles': 9, 'Chair': 10, 'Clipboards': 11, 'Computer': 12, 'Couch': 13, 'Curtains': 14, 'Desk_Lamp': 15, 'Drill': 16, 'Eraser': 17, 'Exit_Sign': 18, 'Fan': 19, 'File_Cabinet': 20, 'Flipflops': 21, 'Flowers': 22, 'Folder': 23, 'Fork': 24, 'Glasses': 25, 'Hammer': 26, 'Helmet': 27, 'Kettle': 28, 'Keyboard': 29, 'Knives': 30, 'Lamp_Shade': 31, 'Laptop': 32, 'Marker': 33, 'Monitor': 34, 'Mop': 35, 'Mouse': 36, 'Mug': 37, 'Notebook': 38, 'Oven': 39, 'Pan': 40, 'Paper_Clip': 41, 'Pen': 42, 'Pencil': 43, 'Postit_Notes': 44, 'Printer': 45, 'Push_Pin': 46, 'Radio': 47, 'Refrigerator': 48, 'Ruler': 49, 'Scissors': 50, 'Screwdriver': 51, 'Shelf': 52, 'Sink': 53, 'Sneakers': 54, 'Soda': 55, 'Speaker': 56, 'Spoon': 57, 'TV': 58, 'Table': 59, 'Telephone': 60, 'ToothBrush': 61, 'Toys': 62, 'Trash_Can': 63, 'Webcam': 64}\n",
            "{'Alarm_Clock': 0, 'Backpack': 1, 'Batteries': 2, 'Bed': 3, 'Bike': 4, 'Bottle': 5, 'Bucket': 6, 'Calculator': 7, 'Calendar': 8, 'Candles': 9, 'Chair': 10, 'Clipboards': 11, 'Computer': 12, 'Couch': 13, 'Curtains': 14, 'Desk_Lamp': 15, 'Drill': 16, 'Eraser': 17, 'Exit_Sign': 18, 'Fan': 19, 'File_Cabinet': 20, 'Flipflops': 21, 'Flowers': 22, 'Folder': 23, 'Fork': 24, 'Glasses': 25, 'Hammer': 26, 'Helmet': 27, 'Kettle': 28, 'Keyboard': 29, 'Knives': 30, 'Lamp_Shade': 31, 'Laptop': 32, 'Marker': 33, 'Monitor': 34, 'Mop': 35, 'Mouse': 36, 'Mug': 37, 'Notebook': 38, 'Oven': 39, 'Pan': 40, 'Paper_Clip': 41, 'Pen': 42, 'Pencil': 43, 'Postit_Notes': 44, 'Printer': 45, 'Push_Pin': 46, 'Radio': 47, 'Refrigerator': 48, 'Ruler': 49, 'Scissors': 50, 'Screwdriver': 51, 'Shelf': 52, 'Sink': 53, 'Sneakers': 54, 'Soda': 55, 'Speaker': 56, 'Spoon': 57, 'TV': 58, 'Table': 59, 'Telephone': 60, 'ToothBrush': 61, 'Toys': 62, 'Trash_Can': 63, 'Webcam': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8723b07e-ab16-4d0c-fcd5-da65f9f078d7",
        "id": "ezfEDWS46bum"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 128, 128]) 59\n"
          ]
        }
      ],
      "source": [
        "a, b = train_set[100]\n",
        "print(a.shape,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d2bcca-5830-44c7-f856-e44bd297997e",
        "id": "TmeYgmRj6bum"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "valid_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d147571e-2449-4dce-bd38-0d88adc12bf0",
        "id": "fQFgVSE26bum"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      # TODO\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      # self.load_model = models.wide_resnet50_2(pretrained = True, progress=True)\n",
        "      self.resnet = models.resnet50(pretrained=False).to('cuda')\n",
        "      self.resnet.load_state_dict(torch.load('/content/hw4_data/pretrain_model_SL.pt'))\n",
        "      # print(self.load_model.image_size)\n",
        "      # self.classifier = nn.Linear(1000, 65)\n",
        "      # self.classifier = nn.Sequential(\n",
        "      #       nn.Linear(1000, 80),\n",
        "      #       nn.ReLU(),\n",
        "      #       nn.Dropout(),\n",
        "      #       nn.Linear(80, 80),\n",
        "      #       nn.ReLU(),\n",
        "      #       nn.Dropout(),\n",
        "      #       nn.Linear(80, 65)\n",
        "      # )\n",
        "      \n",
        "      \n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO\n",
        "      x = self.resnet(x)\n",
        "      # x = self.classifier(x)\n",
        "      return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "      # TODO\n",
        "      super(Classifier, self).__init__()\n",
        "\n",
        "      # self.load_model = models.wide_resnet50_2(pretrained = True, progress=True)\n",
        "      # self.resnet = models.resnet50(pretrained=False).to('cuda')\n",
        "      # self.resnet.load_state_dict(torch.load('/content/improved-net4.pt'))\n",
        "      # print(self.load_model.image_size)\n",
        "      self.classifier = nn.Linear(1000, 65)\n",
        "      # self.classifier = nn.Sequential(\n",
        "      #       nn.Linear(1000, 80),\n",
        "      #       nn.ReLU(),\n",
        "      #       nn.Dropout(),\n",
        "      #       nn.Linear(80, 80),\n",
        "      #       nn.ReLU(),\n",
        "      #       nn.Dropout(),\n",
        "      #       nn.Linear(80, 65)\n",
        "      # )\n",
        "      \n",
        "      \n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO\n",
        "      # x = self.resnet(x)\n",
        "      x = self.classifier(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "0Qk5ZD6_6bun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to('cuda')\n",
        "classifier = Classifier().to('cuda')"
      ],
      "metadata": {
        "id": "kH3tXXIK6bun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup, get_constant_schedule, get_cosine_schedule_with_warmup\n",
        "\n",
        "def save_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = {'state_dict': model.state_dict(),\n",
        "             'optimizer' : optimizer.state_dict()}\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print('model saved to %s' % checkpoint_path)\n",
        "    \n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer'])\n",
        "    print('model loaded from %s' % checkpoint_path)\n",
        "\n",
        "def train_save(model, epoch, save_interval, log_interval=31):\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-4)(adam 3e-5 cosine)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr = 0.0002, momentum = 0.9)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # load_checkpoint('/content/drive/MyDrive/DLCV/HW4/checkpoint_p2/model2511.pth', model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # max_steps = len(train_loader) / batch_size\n",
        "    # scheduler = get_linear_schedule_with_warmup(optimizer, 5, 30)\n",
        "    # scheduler = get_constant_schedule(optimizer)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, 20, 100)\n",
        "    \n",
        "    iteration = 0\n",
        "    for ep in range(epoch):\n",
        "        # if ep%5 == 0:\n",
        "        model.train()\n",
        "        scheduler.step()\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data).squeeze(0)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # if iteration % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Lr: {}'.format(\n",
        "                ep, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), optimizer.param_groups[0]['lr']))\n",
        "            if iteration % save_interval == 0 and iteration > 0:\n",
        "                validate(model)\n",
        "                save_checkpoint('/content/checkpoint/model%i.pth' % iteration, model, optimizer)\n",
        "            iteration += 1\n",
        "        validate(model)\n",
        "        save_checkpoint('/content/checkpoint/model%i.pth' % iteration, model, optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "HU40YYe46bun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_save_fix(model, classifier, epoch, save_interval, log_interval=31):\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-4)(adam 3e-5 cosine)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr = 0.0002, momentum = 0.9)\n",
        "    optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # load_checkpoint('/content/drive/MyDrive/DLCV/HW4/checkpoint_p2/model2511.pth', model, optimizer)\n",
        "\n",
        "    classifier.train()\n",
        "\n",
        "    # max_steps = len(train_loader) / batch_size\n",
        "    # scheduler = get_linear_schedule_with_warmup(optimizer, 5, 30)\n",
        "    # scheduler = get_constant_schedule(optimizer)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, 20, 100)\n",
        "    \n",
        "    iteration = 0\n",
        "    for ep in range(epoch):\n",
        "        # if ep%5 == 0:\n",
        "        classifier.train()\n",
        "        scheduler.step()\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            output = classifier(output).squeeze(0)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # if iteration % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Lr: {}'.format(\n",
        "                ep, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), optimizer.param_groups[0]['lr']))\n",
        "            if iteration % save_interval == 0 and iteration > 0:\n",
        "                validate_fix(model, classifier)\n",
        "                # save_checkpoint('/content/checkpoint/model%i.pth' % iteration, model, optimizer)\n",
        "            iteration += 1\n",
        "        validate_fix(model, classifier)\n",
        "        # save_checkpoint('/content/checkpoint/model%i.pth' % iteration, model, optimizer)\n",
        "\n",
        "def validate_fix(model, classifier):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            output = model(data)\n",
        "            output = classifier(output)\n",
        "            # print(output.shape, target.shape)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(valid_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(valid_loader.dataset),\n",
        "        100. * correct / len(valid_loader.dataset)))"
      ],
      "metadata": {
        "id": "UJp9xjOnQaj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu3p6zbh6bun"
      },
      "source": [
        "! mkdir -p /content/checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b73b3be2-5c5c-426c-ffdf-5d330d069b8d",
        "id": "anrvmUef6bun"
      },
      "source": [
        "train_save_fix(model, classifier, 100, 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/3951 (0%)]\tLoss: 4.184492 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [128/3951 (3%)]\tLoss: 4.181221 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [256/3951 (6%)]\tLoss: 4.225677 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [384/3951 (10%)]\tLoss: 4.247908 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [512/3951 (13%)]\tLoss: 4.239679 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [640/3951 (16%)]\tLoss: 4.193351 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [768/3951 (19%)]\tLoss: 4.246045 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [896/3951 (23%)]\tLoss: 4.203335 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1024/3951 (26%)]\tLoss: 4.191077 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1152/3951 (29%)]\tLoss: 4.231047 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1280/3951 (32%)]\tLoss: 4.193359 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1408/3951 (35%)]\tLoss: 4.207572 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1536/3951 (39%)]\tLoss: 4.187987 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1664/3951 (42%)]\tLoss: 4.242010 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1792/3951 (45%)]\tLoss: 4.277505 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [1920/3951 (48%)]\tLoss: 4.221697 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2048/3951 (52%)]\tLoss: 4.224656 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2176/3951 (55%)]\tLoss: 4.218845 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2304/3951 (58%)]\tLoss: 4.223762 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2432/3951 (61%)]\tLoss: 4.220929 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2560/3951 (65%)]\tLoss: 4.242845 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2688/3951 (68%)]\tLoss: 4.250972 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2816/3951 (71%)]\tLoss: 4.180131 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [2944/3951 (74%)]\tLoss: 4.269426 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3072/3951 (77%)]\tLoss: 4.212790 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3200/3951 (81%)]\tLoss: 4.228243 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3328/3951 (84%)]\tLoss: 4.237769 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3456/3951 (87%)]\tLoss: 4.244449 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3584/3951 (90%)]\tLoss: 4.187369 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3712/3951 (94%)]\tLoss: 4.237517 Lr: 1.0000000000000002e-06\n",
            "Train Epoch: 0 [3330/3951 (97%)]\tLoss: 4.166941 Lr: 1.0000000000000002e-06\n",
            "\n",
            "Test set: Average loss: 0.0419, Accuracy: 3/406 (1%)\n",
            "\n",
            "Train Epoch: 1 [0/3951 (0%)]\tLoss: 4.219623 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [128/3951 (3%)]\tLoss: 4.195395 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [256/3951 (6%)]\tLoss: 4.266865 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [384/3951 (10%)]\tLoss: 4.257533 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [512/3951 (13%)]\tLoss: 4.216383 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [640/3951 (16%)]\tLoss: 4.209413 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [768/3951 (19%)]\tLoss: 4.233833 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [896/3951 (23%)]\tLoss: 4.263584 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1024/3951 (26%)]\tLoss: 4.193333 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1152/3951 (29%)]\tLoss: 4.243463 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1280/3951 (32%)]\tLoss: 4.246127 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1408/3951 (35%)]\tLoss: 4.206382 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1536/3951 (39%)]\tLoss: 4.199697 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1664/3951 (42%)]\tLoss: 4.207218 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1792/3951 (45%)]\tLoss: 4.227143 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [1920/3951 (48%)]\tLoss: 4.227720 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2048/3951 (52%)]\tLoss: 4.218597 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2176/3951 (55%)]\tLoss: 4.235021 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2304/3951 (58%)]\tLoss: 4.237602 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2432/3951 (61%)]\tLoss: 4.202444 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2560/3951 (65%)]\tLoss: 4.235674 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2688/3951 (68%)]\tLoss: 4.207295 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2816/3951 (71%)]\tLoss: 4.206773 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [2944/3951 (74%)]\tLoss: 4.211710 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3072/3951 (77%)]\tLoss: 4.257914 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3200/3951 (81%)]\tLoss: 4.218240 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3328/3951 (84%)]\tLoss: 4.229270 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3456/3951 (87%)]\tLoss: 4.215040 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3584/3951 (90%)]\tLoss: 4.216079 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3712/3951 (94%)]\tLoss: 4.346312 Lr: 2.0000000000000003e-06\n",
            "Train Epoch: 1 [3330/3951 (97%)]\tLoss: 4.198198 Lr: 2.0000000000000003e-06\n",
            "\n",
            "Test set: Average loss: 0.0419, Accuracy: 3/406 (1%)\n",
            "\n",
            "Train Epoch: 2 [0/3951 (0%)]\tLoss: 4.243490 Lr: 3e-06\n",
            "Train Epoch: 2 [128/3951 (3%)]\tLoss: 4.221673 Lr: 3e-06\n",
            "Train Epoch: 2 [256/3951 (6%)]\tLoss: 4.213044 Lr: 3e-06\n",
            "Train Epoch: 2 [384/3951 (10%)]\tLoss: 4.207274 Lr: 3e-06\n",
            "Train Epoch: 2 [512/3951 (13%)]\tLoss: 4.208866 Lr: 3e-06\n",
            "Train Epoch: 2 [640/3951 (16%)]\tLoss: 4.173711 Lr: 3e-06\n",
            "Train Epoch: 2 [768/3951 (19%)]\tLoss: 4.249372 Lr: 3e-06\n",
            "Train Epoch: 2 [896/3951 (23%)]\tLoss: 4.236896 Lr: 3e-06\n",
            "Train Epoch: 2 [1024/3951 (26%)]\tLoss: 4.232623 Lr: 3e-06\n",
            "Train Epoch: 2 [1152/3951 (29%)]\tLoss: 4.177394 Lr: 3e-06\n",
            "Train Epoch: 2 [1280/3951 (32%)]\tLoss: 4.197210 Lr: 3e-06\n",
            "Train Epoch: 2 [1408/3951 (35%)]\tLoss: 4.207660 Lr: 3e-06\n",
            "Train Epoch: 2 [1536/3951 (39%)]\tLoss: 4.219378 Lr: 3e-06\n",
            "Train Epoch: 2 [1664/3951 (42%)]\tLoss: 4.222494 Lr: 3e-06\n",
            "Train Epoch: 2 [1792/3951 (45%)]\tLoss: 4.218599 Lr: 3e-06\n",
            "Train Epoch: 2 [1920/3951 (48%)]\tLoss: 4.192211 Lr: 3e-06\n",
            "Train Epoch: 2 [2048/3951 (52%)]\tLoss: 4.214938 Lr: 3e-06\n",
            "Train Epoch: 2 [2176/3951 (55%)]\tLoss: 4.192543 Lr: 3e-06\n",
            "Train Epoch: 2 [2304/3951 (58%)]\tLoss: 4.192591 Lr: 3e-06\n",
            "Train Epoch: 2 [2432/3951 (61%)]\tLoss: 4.257209 Lr: 3e-06\n",
            "Train Epoch: 2 [2560/3951 (65%)]\tLoss: 4.244860 Lr: 3e-06\n",
            "Train Epoch: 2 [2688/3951 (68%)]\tLoss: 4.234467 Lr: 3e-06\n",
            "Train Epoch: 2 [2816/3951 (71%)]\tLoss: 4.179843 Lr: 3e-06\n",
            "Train Epoch: 2 [2944/3951 (74%)]\tLoss: 4.209351 Lr: 3e-06\n",
            "Train Epoch: 2 [3072/3951 (77%)]\tLoss: 4.269015 Lr: 3e-06\n",
            "Train Epoch: 2 [3200/3951 (81%)]\tLoss: 4.216644 Lr: 3e-06\n",
            "Train Epoch: 2 [3328/3951 (84%)]\tLoss: 4.515532 Lr: 3e-06\n",
            "Train Epoch: 2 [3456/3951 (87%)]\tLoss: 4.224494 Lr: 3e-06\n",
            "Train Epoch: 2 [3584/3951 (90%)]\tLoss: 4.233146 Lr: 3e-06\n",
            "Train Epoch: 2 [3712/3951 (94%)]\tLoss: 4.205103 Lr: 3e-06\n",
            "Train Epoch: 2 [3330/3951 (97%)]\tLoss: 4.180403 Lr: 3e-06\n",
            "\n",
            "Test set: Average loss: 0.0418, Accuracy: 4/406 (1%)\n",
            "\n",
            "Train Epoch: 3 [0/3951 (0%)]\tLoss: 4.176676 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [128/3951 (3%)]\tLoss: 4.225391 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [256/3951 (6%)]\tLoss: 4.170698 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [384/3951 (10%)]\tLoss: 4.248447 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [512/3951 (13%)]\tLoss: 4.216700 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [640/3951 (16%)]\tLoss: 4.220119 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [768/3951 (19%)]\tLoss: 4.229352 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [896/3951 (23%)]\tLoss: 4.211442 Lr: 4.000000000000001e-06\n",
            "\n",
            "Test set: Average loss: 0.0418, Accuracy: 4/406 (1%)\n",
            "\n",
            "Train Epoch: 3 [1024/3951 (26%)]\tLoss: 4.237965 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1152/3951 (29%)]\tLoss: 4.250949 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1280/3951 (32%)]\tLoss: 4.200538 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1408/3951 (35%)]\tLoss: 4.207878 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1536/3951 (39%)]\tLoss: 4.208893 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1664/3951 (42%)]\tLoss: 4.232606 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1792/3951 (45%)]\tLoss: 4.210337 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [1920/3951 (48%)]\tLoss: 4.170857 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2048/3951 (52%)]\tLoss: 4.186606 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2176/3951 (55%)]\tLoss: 4.241161 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2304/3951 (58%)]\tLoss: 4.201724 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2432/3951 (61%)]\tLoss: 4.171638 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2560/3951 (65%)]\tLoss: 4.205698 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2688/3951 (68%)]\tLoss: 4.265727 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2816/3951 (71%)]\tLoss: 4.182267 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [2944/3951 (74%)]\tLoss: 4.192607 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3072/3951 (77%)]\tLoss: 4.208263 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3200/3951 (81%)]\tLoss: 4.215140 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3328/3951 (84%)]\tLoss: 4.200655 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3456/3951 (87%)]\tLoss: 4.216277 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3584/3951 (90%)]\tLoss: 4.236067 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3712/3951 (94%)]\tLoss: 4.173636 Lr: 4.000000000000001e-06\n",
            "Train Epoch: 3 [3330/3951 (97%)]\tLoss: 4.214900 Lr: 4.000000000000001e-06\n",
            "\n",
            "Test set: Average loss: 0.0418, Accuracy: 4/406 (1%)\n",
            "\n",
            "Train Epoch: 4 [0/3951 (0%)]\tLoss: 4.198842 Lr: 5e-06\n",
            "Train Epoch: 4 [128/3951 (3%)]\tLoss: 4.237277 Lr: 5e-06\n",
            "Train Epoch: 4 [256/3951 (6%)]\tLoss: 4.196940 Lr: 5e-06\n",
            "Train Epoch: 4 [384/3951 (10%)]\tLoss: 4.190932 Lr: 5e-06\n",
            "Train Epoch: 4 [512/3951 (13%)]\tLoss: 4.185012 Lr: 5e-06\n",
            "Train Epoch: 4 [640/3951 (16%)]\tLoss: 4.247061 Lr: 5e-06\n",
            "Train Epoch: 4 [768/3951 (19%)]\tLoss: 4.212081 Lr: 5e-06\n",
            "Train Epoch: 4 [896/3951 (23%)]\tLoss: 4.194633 Lr: 5e-06\n",
            "Train Epoch: 4 [1024/3951 (26%)]\tLoss: 4.209354 Lr: 5e-06\n",
            "Train Epoch: 4 [1152/3951 (29%)]\tLoss: 4.211047 Lr: 5e-06\n",
            "Train Epoch: 4 [1280/3951 (32%)]\tLoss: 4.171132 Lr: 5e-06\n",
            "Train Epoch: 4 [1408/3951 (35%)]\tLoss: 4.275798 Lr: 5e-06\n",
            "Train Epoch: 4 [1536/3951 (39%)]\tLoss: 4.227131 Lr: 5e-06\n",
            "Train Epoch: 4 [1664/3951 (42%)]\tLoss: 4.182008 Lr: 5e-06\n",
            "Train Epoch: 4 [1792/3951 (45%)]\tLoss: 4.227657 Lr: 5e-06\n",
            "Train Epoch: 4 [1920/3951 (48%)]\tLoss: 4.293725 Lr: 5e-06\n",
            "Train Epoch: 4 [2048/3951 (52%)]\tLoss: 4.232445 Lr: 5e-06\n",
            "Train Epoch: 4 [2176/3951 (55%)]\tLoss: 4.232726 Lr: 5e-06\n",
            "Train Epoch: 4 [2304/3951 (58%)]\tLoss: 4.229778 Lr: 5e-06\n",
            "Train Epoch: 4 [2432/3951 (61%)]\tLoss: 4.189044 Lr: 5e-06\n",
            "Train Epoch: 4 [2560/3951 (65%)]\tLoss: 4.182729 Lr: 5e-06\n",
            "Train Epoch: 4 [2688/3951 (68%)]\tLoss: 4.187481 Lr: 5e-06\n",
            "Train Epoch: 4 [2816/3951 (71%)]\tLoss: 4.188561 Lr: 5e-06\n",
            "Train Epoch: 4 [2944/3951 (74%)]\tLoss: 4.230708 Lr: 5e-06\n",
            "Train Epoch: 4 [3072/3951 (77%)]\tLoss: 4.189832 Lr: 5e-06\n",
            "Train Epoch: 4 [3200/3951 (81%)]\tLoss: 4.185596 Lr: 5e-06\n",
            "Train Epoch: 4 [3328/3951 (84%)]\tLoss: 4.234832 Lr: 5e-06\n",
            "Train Epoch: 4 [3456/3951 (87%)]\tLoss: 4.203357 Lr: 5e-06\n",
            "Train Epoch: 4 [3584/3951 (90%)]\tLoss: 4.183685 Lr: 5e-06\n",
            "Train Epoch: 4 [3712/3951 (94%)]\tLoss: 4.191041 Lr: 5e-06\n",
            "Train Epoch: 4 [3330/3951 (97%)]\tLoss: 4.178730 Lr: 5e-06\n",
            "\n",
            "Test set: Average loss: 0.0417, Accuracy: 4/406 (1%)\n",
            "\n",
            "Train Epoch: 5 [0/3951 (0%)]\tLoss: 4.163859 Lr: 6e-06\n",
            "Train Epoch: 5 [128/3951 (3%)]\tLoss: 4.187392 Lr: 6e-06\n",
            "Train Epoch: 5 [256/3951 (6%)]\tLoss: 4.220886 Lr: 6e-06\n",
            "Train Epoch: 5 [384/3951 (10%)]\tLoss: 4.207204 Lr: 6e-06\n",
            "Train Epoch: 5 [512/3951 (13%)]\tLoss: 4.246935 Lr: 6e-06\n",
            "Train Epoch: 5 [640/3951 (16%)]\tLoss: 4.217297 Lr: 6e-06\n",
            "Train Epoch: 5 [768/3951 (19%)]\tLoss: 4.209218 Lr: 6e-06\n",
            "Train Epoch: 5 [896/3951 (23%)]\tLoss: 4.216024 Lr: 6e-06\n",
            "Train Epoch: 5 [1024/3951 (26%)]\tLoss: 4.190506 Lr: 6e-06\n",
            "Train Epoch: 5 [1152/3951 (29%)]\tLoss: 4.211991 Lr: 6e-06\n",
            "Train Epoch: 5 [1280/3951 (32%)]\tLoss: 4.188629 Lr: 6e-06\n",
            "Train Epoch: 5 [1408/3951 (35%)]\tLoss: 4.247624 Lr: 6e-06\n",
            "Train Epoch: 5 [1536/3951 (39%)]\tLoss: 4.169734 Lr: 6e-06\n",
            "Train Epoch: 5 [1664/3951 (42%)]\tLoss: 4.201370 Lr: 6e-06\n",
            "Train Epoch: 5 [1792/3951 (45%)]\tLoss: 4.166596 Lr: 6e-06\n",
            "Train Epoch: 5 [1920/3951 (48%)]\tLoss: 4.215583 Lr: 6e-06\n",
            "Train Epoch: 5 [2048/3951 (52%)]\tLoss: 4.196286 Lr: 6e-06\n",
            "Train Epoch: 5 [2176/3951 (55%)]\tLoss: 4.209730 Lr: 6e-06\n",
            "Train Epoch: 5 [2304/3951 (58%)]\tLoss: 4.221207 Lr: 6e-06\n",
            "Train Epoch: 5 [2432/3951 (61%)]\tLoss: 4.218807 Lr: 6e-06\n",
            "Train Epoch: 5 [2560/3951 (65%)]\tLoss: 4.173187 Lr: 6e-06\n",
            "Train Epoch: 5 [2688/3951 (68%)]\tLoss: 4.189554 Lr: 6e-06\n",
            "Train Epoch: 5 [2816/3951 (71%)]\tLoss: 4.226069 Lr: 6e-06\n",
            "Train Epoch: 5 [2944/3951 (74%)]\tLoss: 4.209217 Lr: 6e-06\n",
            "Train Epoch: 5 [3072/3951 (77%)]\tLoss: 4.205685 Lr: 6e-06\n",
            "Train Epoch: 5 [3200/3951 (81%)]\tLoss: 4.210544 Lr: 6e-06\n",
            "Train Epoch: 5 [3328/3951 (84%)]\tLoss: 4.170705 Lr: 6e-06\n",
            "Train Epoch: 5 [3456/3951 (87%)]\tLoss: 4.209911 Lr: 6e-06\n",
            "Train Epoch: 5 [3584/3951 (90%)]\tLoss: 4.180701 Lr: 6e-06\n",
            "Train Epoch: 5 [3712/3951 (94%)]\tLoss: 4.207442 Lr: 6e-06\n",
            "Train Epoch: 5 [3330/3951 (97%)]\tLoss: 4.181322 Lr: 6e-06\n",
            "\n",
            "Test set: Average loss: 0.0416, Accuracy: 4/406 (1%)\n",
            "\n",
            "Train Epoch: 6 [0/3951 (0%)]\tLoss: 4.237358 Lr: 7e-06\n",
            "Train Epoch: 6 [128/3951 (3%)]\tLoss: 4.191636 Lr: 7e-06\n",
            "Train Epoch: 6 [256/3951 (6%)]\tLoss: 4.213186 Lr: 7e-06\n",
            "Train Epoch: 6 [384/3951 (10%)]\tLoss: 4.173231 Lr: 7e-06\n",
            "Train Epoch: 6 [512/3951 (13%)]\tLoss: 4.226978 Lr: 7e-06\n",
            "Train Epoch: 6 [640/3951 (16%)]\tLoss: 4.189819 Lr: 7e-06\n",
            "Train Epoch: 6 [768/3951 (19%)]\tLoss: 4.214397 Lr: 7e-06\n",
            "Train Epoch: 6 [896/3951 (23%)]\tLoss: 4.182301 Lr: 7e-06\n",
            "Train Epoch: 6 [1024/3951 (26%)]\tLoss: 4.220248 Lr: 7e-06\n",
            "Train Epoch: 6 [1152/3951 (29%)]\tLoss: 4.262028 Lr: 7e-06\n",
            "Train Epoch: 6 [1280/3951 (32%)]\tLoss: 4.181730 Lr: 7e-06\n",
            "Train Epoch: 6 [1408/3951 (35%)]\tLoss: 4.156926 Lr: 7e-06\n",
            "Train Epoch: 6 [1536/3951 (39%)]\tLoss: 4.216312 Lr: 7e-06\n",
            "Train Epoch: 6 [1664/3951 (42%)]\tLoss: 4.196938 Lr: 7e-06\n",
            "Train Epoch: 6 [1792/3951 (45%)]\tLoss: 4.200866 Lr: 7e-06\n",
            "\n",
            "Test set: Average loss: 0.0415, Accuracy: 4/406 (1%)\n",
            "\n",
            "Train Epoch: 6 [1920/3951 (48%)]\tLoss: 4.205445 Lr: 7e-06\n",
            "Train Epoch: 6 [2048/3951 (52%)]\tLoss: 4.217190 Lr: 7e-06\n",
            "Train Epoch: 6 [2176/3951 (55%)]\tLoss: 4.170904 Lr: 7e-06\n",
            "Train Epoch: 6 [2304/3951 (58%)]\tLoss: 4.191102 Lr: 7e-06\n",
            "Train Epoch: 6 [2432/3951 (61%)]\tLoss: 4.170061 Lr: 7e-06\n",
            "Train Epoch: 6 [2560/3951 (65%)]\tLoss: 4.184735 Lr: 7e-06\n",
            "Train Epoch: 6 [2688/3951 (68%)]\tLoss: 4.181946 Lr: 7e-06\n",
            "Train Epoch: 6 [2816/3951 (71%)]\tLoss: 4.151010 Lr: 7e-06\n",
            "Train Epoch: 6 [2944/3951 (74%)]\tLoss: 4.145191 Lr: 7e-06\n",
            "Train Epoch: 6 [3072/3951 (77%)]\tLoss: 4.188743 Lr: 7e-06\n",
            "Train Epoch: 6 [3200/3951 (81%)]\tLoss: 4.197132 Lr: 7e-06\n",
            "Train Epoch: 6 [3328/3951 (84%)]\tLoss: 4.164961 Lr: 7e-06\n",
            "Train Epoch: 6 [3456/3951 (87%)]\tLoss: 4.193956 Lr: 7e-06\n",
            "Train Epoch: 6 [3584/3951 (90%)]\tLoss: 4.169623 Lr: 7e-06\n",
            "Train Epoch: 6 [3712/3951 (94%)]\tLoss: 4.174120 Lr: 7e-06\n",
            "Train Epoch: 6 [3330/3951 (97%)]\tLoss: 4.153556 Lr: 7e-06\n",
            "\n",
            "Test set: Average loss: 0.0415, Accuracy: 5/406 (1%)\n",
            "\n",
            "Train Epoch: 7 [0/3951 (0%)]\tLoss: 4.176234 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [128/3951 (3%)]\tLoss: 4.154502 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [256/3951 (6%)]\tLoss: 4.170319 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [384/3951 (10%)]\tLoss: 4.192714 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [512/3951 (13%)]\tLoss: 4.145239 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [640/3951 (16%)]\tLoss: 4.180292 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [768/3951 (19%)]\tLoss: 4.204757 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [896/3951 (23%)]\tLoss: 4.161184 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1024/3951 (26%)]\tLoss: 4.224047 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1152/3951 (29%)]\tLoss: 4.239735 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1280/3951 (32%)]\tLoss: 4.158944 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1408/3951 (35%)]\tLoss: 4.136917 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1536/3951 (39%)]\tLoss: 4.182408 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1664/3951 (42%)]\tLoss: 4.169150 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1792/3951 (45%)]\tLoss: 4.209965 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [1920/3951 (48%)]\tLoss: 4.157439 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2048/3951 (52%)]\tLoss: 4.214913 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2176/3951 (55%)]\tLoss: 4.170544 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2304/3951 (58%)]\tLoss: 4.201834 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2432/3951 (61%)]\tLoss: 4.141360 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2560/3951 (65%)]\tLoss: 4.171461 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2688/3951 (68%)]\tLoss: 4.209692 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2816/3951 (71%)]\tLoss: 4.160104 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [2944/3951 (74%)]\tLoss: 4.196957 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3072/3951 (77%)]\tLoss: 4.202131 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3200/3951 (81%)]\tLoss: 4.169775 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3328/3951 (84%)]\tLoss: 4.145825 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3456/3951 (87%)]\tLoss: 4.180335 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3584/3951 (90%)]\tLoss: 4.209826 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3712/3951 (94%)]\tLoss: 4.151710 Lr: 8.000000000000001e-06\n",
            "Train Epoch: 7 [3330/3951 (97%)]\tLoss: 4.169748 Lr: 8.000000000000001e-06\n",
            "\n",
            "Test set: Average loss: 0.0414, Accuracy: 5/406 (1%)\n",
            "\n",
            "Train Epoch: 8 [0/3951 (0%)]\tLoss: 4.173459 Lr: 9e-06\n",
            "Train Epoch: 8 [128/3951 (3%)]\tLoss: 4.159030 Lr: 9e-06\n",
            "Train Epoch: 8 [256/3951 (6%)]\tLoss: 4.163903 Lr: 9e-06\n",
            "Train Epoch: 8 [384/3951 (10%)]\tLoss: 4.186824 Lr: 9e-06\n",
            "Train Epoch: 8 [512/3951 (13%)]\tLoss: 4.295214 Lr: 9e-06\n",
            "Train Epoch: 8 [640/3951 (16%)]\tLoss: 4.140853 Lr: 9e-06\n",
            "Train Epoch: 8 [768/3951 (19%)]\tLoss: 4.162617 Lr: 9e-06\n",
            "Train Epoch: 8 [896/3951 (23%)]\tLoss: 4.171220 Lr: 9e-06\n",
            "Train Epoch: 8 [1024/3951 (26%)]\tLoss: 4.161348 Lr: 9e-06\n",
            "Train Epoch: 8 [1152/3951 (29%)]\tLoss: 4.224300 Lr: 9e-06\n",
            "Train Epoch: 8 [1280/3951 (32%)]\tLoss: 4.135556 Lr: 9e-06\n",
            "Train Epoch: 8 [1408/3951 (35%)]\tLoss: 4.110551 Lr: 9e-06\n",
            "Train Epoch: 8 [1536/3951 (39%)]\tLoss: 4.116194 Lr: 9e-06\n",
            "Train Epoch: 8 [1664/3951 (42%)]\tLoss: 4.191271 Lr: 9e-06\n",
            "Train Epoch: 8 [1792/3951 (45%)]\tLoss: 4.157955 Lr: 9e-06\n",
            "Train Epoch: 8 [1920/3951 (48%)]\tLoss: 4.186742 Lr: 9e-06\n",
            "Train Epoch: 8 [2048/3951 (52%)]\tLoss: 4.156042 Lr: 9e-06\n",
            "Train Epoch: 8 [2176/3951 (55%)]\tLoss: 4.139641 Lr: 9e-06\n",
            "Train Epoch: 8 [2304/3951 (58%)]\tLoss: 4.184115 Lr: 9e-06\n",
            "Train Epoch: 8 [2432/3951 (61%)]\tLoss: 4.189327 Lr: 9e-06\n",
            "Train Epoch: 8 [2560/3951 (65%)]\tLoss: 4.196819 Lr: 9e-06\n",
            "Train Epoch: 8 [2688/3951 (68%)]\tLoss: 4.189870 Lr: 9e-06\n",
            "Train Epoch: 8 [2816/3951 (71%)]\tLoss: 4.154062 Lr: 9e-06\n",
            "Train Epoch: 8 [2944/3951 (74%)]\tLoss: 4.176973 Lr: 9e-06\n",
            "Train Epoch: 8 [3072/3951 (77%)]\tLoss: 4.150194 Lr: 9e-06\n",
            "Train Epoch: 8 [3200/3951 (81%)]\tLoss: 4.151920 Lr: 9e-06\n",
            "Train Epoch: 8 [3328/3951 (84%)]\tLoss: 4.181796 Lr: 9e-06\n",
            "Train Epoch: 8 [3456/3951 (87%)]\tLoss: 4.131659 Lr: 9e-06\n",
            "Train Epoch: 8 [3584/3951 (90%)]\tLoss: 4.192632 Lr: 9e-06\n",
            "Train Epoch: 8 [3712/3951 (94%)]\tLoss: 4.157136 Lr: 9e-06\n",
            "Train Epoch: 8 [3330/3951 (97%)]\tLoss: 4.122474 Lr: 9e-06\n",
            "\n",
            "Test set: Average loss: 0.0413, Accuracy: 7/406 (2%)\n",
            "\n",
            "Train Epoch: 9 [0/3951 (0%)]\tLoss: 4.141998 Lr: 1e-05\n",
            "Train Epoch: 9 [128/3951 (3%)]\tLoss: 4.100123 Lr: 1e-05\n",
            "Train Epoch: 9 [256/3951 (6%)]\tLoss: 4.153502 Lr: 1e-05\n",
            "Train Epoch: 9 [384/3951 (10%)]\tLoss: 4.157919 Lr: 1e-05\n",
            "Train Epoch: 9 [512/3951 (13%)]\tLoss: 4.151347 Lr: 1e-05\n",
            "Train Epoch: 9 [640/3951 (16%)]\tLoss: 4.137272 Lr: 1e-05\n",
            "Train Epoch: 9 [768/3951 (19%)]\tLoss: 4.167066 Lr: 1e-05\n",
            "Train Epoch: 9 [896/3951 (23%)]\tLoss: 4.136716 Lr: 1e-05\n",
            "Train Epoch: 9 [1024/3951 (26%)]\tLoss: 4.122364 Lr: 1e-05\n",
            "Train Epoch: 9 [1152/3951 (29%)]\tLoss: 4.128971 Lr: 1e-05\n",
            "Train Epoch: 9 [1280/3951 (32%)]\tLoss: 4.194485 Lr: 1e-05\n",
            "Train Epoch: 9 [1408/3951 (35%)]\tLoss: 4.156530 Lr: 1e-05\n",
            "Train Epoch: 9 [1536/3951 (39%)]\tLoss: 4.126275 Lr: 1e-05\n",
            "Train Epoch: 9 [1664/3951 (42%)]\tLoss: 4.166993 Lr: 1e-05\n",
            "Train Epoch: 9 [1792/3951 (45%)]\tLoss: 4.183667 Lr: 1e-05\n",
            "Train Epoch: 9 [1920/3951 (48%)]\tLoss: 4.169554 Lr: 1e-05\n",
            "Train Epoch: 9 [2048/3951 (52%)]\tLoss: 4.168168 Lr: 1e-05\n",
            "Train Epoch: 9 [2176/3951 (55%)]\tLoss: 4.155635 Lr: 1e-05\n",
            "Train Epoch: 9 [2304/3951 (58%)]\tLoss: 4.125670 Lr: 1e-05\n",
            "Train Epoch: 9 [2432/3951 (61%)]\tLoss: 4.169239 Lr: 1e-05\n",
            "Train Epoch: 9 [2560/3951 (65%)]\tLoss: 4.162412 Lr: 1e-05\n",
            "Train Epoch: 9 [2688/3951 (68%)]\tLoss: 4.148046 Lr: 1e-05\n",
            "\n",
            "Test set: Average loss: 0.0412, Accuracy: 7/406 (2%)\n",
            "\n",
            "Train Epoch: 9 [2816/3951 (71%)]\tLoss: 4.200547 Lr: 1e-05\n",
            "Train Epoch: 9 [2944/3951 (74%)]\tLoss: 4.165721 Lr: 1e-05\n",
            "Train Epoch: 9 [3072/3951 (77%)]\tLoss: 4.154565 Lr: 1e-05\n",
            "Train Epoch: 9 [3200/3951 (81%)]\tLoss: 4.156516 Lr: 1e-05\n",
            "Train Epoch: 9 [3328/3951 (84%)]\tLoss: 4.152795 Lr: 1e-05\n",
            "Train Epoch: 9 [3456/3951 (87%)]\tLoss: 4.162960 Lr: 1e-05\n",
            "Train Epoch: 9 [3584/3951 (90%)]\tLoss: 4.125254 Lr: 1e-05\n",
            "Train Epoch: 9 [3712/3951 (94%)]\tLoss: 4.123613 Lr: 1e-05\n",
            "Train Epoch: 9 [3330/3951 (97%)]\tLoss: 4.151999 Lr: 1e-05\n",
            "\n",
            "Test set: Average loss: 0.0411, Accuracy: 7/406 (2%)\n",
            "\n",
            "Train Epoch: 10 [0/3951 (0%)]\tLoss: 4.175534 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [128/3951 (3%)]\tLoss: 4.128551 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [256/3951 (6%)]\tLoss: 4.127108 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [384/3951 (10%)]\tLoss: 4.149066 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [512/3951 (13%)]\tLoss: 4.144451 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [640/3951 (16%)]\tLoss: 4.163569 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [768/3951 (19%)]\tLoss: 4.164382 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [896/3951 (23%)]\tLoss: 4.123055 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1024/3951 (26%)]\tLoss: 4.129073 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1152/3951 (29%)]\tLoss: 4.150900 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1280/3951 (32%)]\tLoss: 4.149004 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1408/3951 (35%)]\tLoss: 4.142940 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1536/3951 (39%)]\tLoss: 4.159615 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1664/3951 (42%)]\tLoss: 4.138239 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1792/3951 (45%)]\tLoss: 4.166018 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [1920/3951 (48%)]\tLoss: 4.120368 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2048/3951 (52%)]\tLoss: 4.112331 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2176/3951 (55%)]\tLoss: 4.185423 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2304/3951 (58%)]\tLoss: 4.098636 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2432/3951 (61%)]\tLoss: 4.125601 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2560/3951 (65%)]\tLoss: 4.146845 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2688/3951 (68%)]\tLoss: 4.079539 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2816/3951 (71%)]\tLoss: 4.146003 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [2944/3951 (74%)]\tLoss: 4.111382 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3072/3951 (77%)]\tLoss: 4.139368 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3200/3951 (81%)]\tLoss: 4.141564 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3328/3951 (84%)]\tLoss: 4.129300 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3456/3951 (87%)]\tLoss: 4.150464 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3584/3951 (90%)]\tLoss: 4.125035 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3712/3951 (94%)]\tLoss: 4.098674 Lr: 1.1000000000000001e-05\n",
            "Train Epoch: 10 [3330/3951 (97%)]\tLoss: 4.181534 Lr: 1.1000000000000001e-05\n",
            "\n",
            "Test set: Average loss: 0.0410, Accuracy: 7/406 (2%)\n",
            "\n",
            "Train Epoch: 11 [0/3951 (0%)]\tLoss: 4.131699 Lr: 1.2e-05\n",
            "Train Epoch: 11 [128/3951 (3%)]\tLoss: 4.100909 Lr: 1.2e-05\n",
            "Train Epoch: 11 [256/3951 (6%)]\tLoss: 4.204507 Lr: 1.2e-05\n",
            "Train Epoch: 11 [384/3951 (10%)]\tLoss: 4.158099 Lr: 1.2e-05\n",
            "Train Epoch: 11 [512/3951 (13%)]\tLoss: 4.125712 Lr: 1.2e-05\n",
            "Train Epoch: 11 [640/3951 (16%)]\tLoss: 4.146773 Lr: 1.2e-05\n",
            "Train Epoch: 11 [768/3951 (19%)]\tLoss: 4.142617 Lr: 1.2e-05\n",
            "Train Epoch: 11 [896/3951 (23%)]\tLoss: 4.098663 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1024/3951 (26%)]\tLoss: 4.173552 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1152/3951 (29%)]\tLoss: 4.162081 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1280/3951 (32%)]\tLoss: 4.131434 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1408/3951 (35%)]\tLoss: 4.112983 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1536/3951 (39%)]\tLoss: 4.128140 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1664/3951 (42%)]\tLoss: 4.127008 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1792/3951 (45%)]\tLoss: 4.200093 Lr: 1.2e-05\n",
            "Train Epoch: 11 [1920/3951 (48%)]\tLoss: 4.109802 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2048/3951 (52%)]\tLoss: 4.107446 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2176/3951 (55%)]\tLoss: 4.100149 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2304/3951 (58%)]\tLoss: 4.122610 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2432/3951 (61%)]\tLoss: 4.046065 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2560/3951 (65%)]\tLoss: 4.096874 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2688/3951 (68%)]\tLoss: 4.157231 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2816/3951 (71%)]\tLoss: 4.120033 Lr: 1.2e-05\n",
            "Train Epoch: 11 [2944/3951 (74%)]\tLoss: 4.175542 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3072/3951 (77%)]\tLoss: 4.140555 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3200/3951 (81%)]\tLoss: 4.137411 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3328/3951 (84%)]\tLoss: 4.126284 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3456/3951 (87%)]\tLoss: 4.157842 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3584/3951 (90%)]\tLoss: 4.134592 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3712/3951 (94%)]\tLoss: 4.105217 Lr: 1.2e-05\n",
            "Train Epoch: 11 [3330/3951 (97%)]\tLoss: 4.087523 Lr: 1.2e-05\n",
            "\n",
            "Test set: Average loss: 0.0408, Accuracy: 12/406 (3%)\n",
            "\n",
            "Train Epoch: 12 [0/3951 (0%)]\tLoss: 4.143112 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [128/3951 (3%)]\tLoss: 4.132548 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [256/3951 (6%)]\tLoss: 4.126434 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [384/3951 (10%)]\tLoss: 4.067276 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [512/3951 (13%)]\tLoss: 4.132651 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [640/3951 (16%)]\tLoss: 4.105364 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [768/3951 (19%)]\tLoss: 4.106765 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [896/3951 (23%)]\tLoss: 4.136690 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1024/3951 (26%)]\tLoss: 4.097983 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1152/3951 (29%)]\tLoss: 4.106861 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1280/3951 (32%)]\tLoss: 4.110711 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1408/3951 (35%)]\tLoss: 4.110127 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1536/3951 (39%)]\tLoss: 4.079618 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1664/3951 (42%)]\tLoss: 4.069672 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1792/3951 (45%)]\tLoss: 4.114151 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [1920/3951 (48%)]\tLoss: 4.072254 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2048/3951 (52%)]\tLoss: 4.150440 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2176/3951 (55%)]\tLoss: 4.108451 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2304/3951 (58%)]\tLoss: 4.124956 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2432/3951 (61%)]\tLoss: 4.113023 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2560/3951 (65%)]\tLoss: 4.122660 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2688/3951 (68%)]\tLoss: 4.101302 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2816/3951 (71%)]\tLoss: 4.120615 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [2944/3951 (74%)]\tLoss: 4.096759 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [3072/3951 (77%)]\tLoss: 4.121947 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [3200/3951 (81%)]\tLoss: 4.072159 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [3328/3951 (84%)]\tLoss: 4.080207 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [3456/3951 (87%)]\tLoss: 4.130928 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [3584/3951 (90%)]\tLoss: 4.050157 Lr: 1.3000000000000001e-05\n",
            "\n",
            "Test set: Average loss: 0.0407, Accuracy: 14/406 (3%)\n",
            "\n",
            "Train Epoch: 12 [3712/3951 (94%)]\tLoss: 4.108847 Lr: 1.3000000000000001e-05\n",
            "Train Epoch: 12 [3330/3951 (97%)]\tLoss: 4.086165 Lr: 1.3000000000000001e-05\n",
            "\n",
            "Test set: Average loss: 0.0407, Accuracy: 14/406 (3%)\n",
            "\n",
            "Train Epoch: 13 [0/3951 (0%)]\tLoss: 4.113795 Lr: 1.4e-05\n",
            "Train Epoch: 13 [128/3951 (3%)]\tLoss: 4.090193 Lr: 1.4e-05\n",
            "Train Epoch: 13 [256/3951 (6%)]\tLoss: 4.087569 Lr: 1.4e-05\n",
            "Train Epoch: 13 [384/3951 (10%)]\tLoss: 4.109056 Lr: 1.4e-05\n",
            "Train Epoch: 13 [512/3951 (13%)]\tLoss: 4.046117 Lr: 1.4e-05\n",
            "Train Epoch: 13 [640/3951 (16%)]\tLoss: 4.092440 Lr: 1.4e-05\n",
            "Train Epoch: 13 [768/3951 (19%)]\tLoss: 4.120144 Lr: 1.4e-05\n",
            "Train Epoch: 13 [896/3951 (23%)]\tLoss: 4.103266 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1024/3951 (26%)]\tLoss: 4.082694 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1152/3951 (29%)]\tLoss: 4.094428 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1280/3951 (32%)]\tLoss: 4.110750 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1408/3951 (35%)]\tLoss: 4.109914 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1536/3951 (39%)]\tLoss: 4.079630 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1664/3951 (42%)]\tLoss: 4.147379 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1792/3951 (45%)]\tLoss: 4.062329 Lr: 1.4e-05\n",
            "Train Epoch: 13 [1920/3951 (48%)]\tLoss: 4.084930 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2048/3951 (52%)]\tLoss: 4.121476 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2176/3951 (55%)]\tLoss: 4.077980 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2304/3951 (58%)]\tLoss: 4.099221 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2432/3951 (61%)]\tLoss: 4.113384 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2560/3951 (65%)]\tLoss: 4.076297 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2688/3951 (68%)]\tLoss: 4.094370 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2816/3951 (71%)]\tLoss: 4.090509 Lr: 1.4e-05\n",
            "Train Epoch: 13 [2944/3951 (74%)]\tLoss: 4.120199 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3072/3951 (77%)]\tLoss: 4.046924 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3200/3951 (81%)]\tLoss: 4.099824 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3328/3951 (84%)]\tLoss: 4.094260 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3456/3951 (87%)]\tLoss: 4.094459 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3584/3951 (90%)]\tLoss: 4.077779 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3712/3951 (94%)]\tLoss: 4.114765 Lr: 1.4e-05\n",
            "Train Epoch: 13 [3330/3951 (97%)]\tLoss: 4.125906 Lr: 1.4e-05\n",
            "\n",
            "Test set: Average loss: 0.0405, Accuracy: 20/406 (5%)\n",
            "\n",
            "Train Epoch: 14 [0/3951 (0%)]\tLoss: 4.066048 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [128/3951 (3%)]\tLoss: 4.099939 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [256/3951 (6%)]\tLoss: 4.127308 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [384/3951 (10%)]\tLoss: 4.050256 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [512/3951 (13%)]\tLoss: 4.031215 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [640/3951 (16%)]\tLoss: 4.093987 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [768/3951 (19%)]\tLoss: 4.102392 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [896/3951 (23%)]\tLoss: 4.080822 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1024/3951 (26%)]\tLoss: 4.029246 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1152/3951 (29%)]\tLoss: 4.101296 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1280/3951 (32%)]\tLoss: 4.033728 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1408/3951 (35%)]\tLoss: 4.084112 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1536/3951 (39%)]\tLoss: 4.037518 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1664/3951 (42%)]\tLoss: 4.078107 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1792/3951 (45%)]\tLoss: 4.111254 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [1920/3951 (48%)]\tLoss: 4.076068 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2048/3951 (52%)]\tLoss: 4.100794 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2176/3951 (55%)]\tLoss: 4.128137 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2304/3951 (58%)]\tLoss: 4.109755 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2432/3951 (61%)]\tLoss: 4.053618 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2560/3951 (65%)]\tLoss: 4.034545 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2688/3951 (68%)]\tLoss: 4.064157 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2816/3951 (71%)]\tLoss: 4.088151 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [2944/3951 (74%)]\tLoss: 4.068677 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3072/3951 (77%)]\tLoss: 4.077087 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3200/3951 (81%)]\tLoss: 4.103884 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3328/3951 (84%)]\tLoss: 4.065020 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3456/3951 (87%)]\tLoss: 4.007569 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3584/3951 (90%)]\tLoss: 4.095282 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3712/3951 (94%)]\tLoss: 4.074926 Lr: 1.5000000000000002e-05\n",
            "Train Epoch: 14 [3330/3951 (97%)]\tLoss: 4.023689 Lr: 1.5000000000000002e-05\n",
            "\n",
            "Test set: Average loss: 0.0403, Accuracy: 23/406 (6%)\n",
            "\n",
            "Train Epoch: 15 [0/3951 (0%)]\tLoss: 4.037461 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [128/3951 (3%)]\tLoss: 4.025927 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [256/3951 (6%)]\tLoss: 4.059882 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [384/3951 (10%)]\tLoss: 4.084122 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [512/3951 (13%)]\tLoss: 4.055477 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [640/3951 (16%)]\tLoss: 4.086432 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [768/3951 (19%)]\tLoss: 4.076764 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [896/3951 (23%)]\tLoss: 4.061752 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1024/3951 (26%)]\tLoss: 4.068449 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1152/3951 (29%)]\tLoss: 4.010250 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1280/3951 (32%)]\tLoss: 4.084748 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1408/3951 (35%)]\tLoss: 4.091665 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1536/3951 (39%)]\tLoss: 4.075611 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1664/3951 (42%)]\tLoss: 4.031285 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1792/3951 (45%)]\tLoss: 4.041850 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [1920/3951 (48%)]\tLoss: 4.056615 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2048/3951 (52%)]\tLoss: 4.066778 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2176/3951 (55%)]\tLoss: 4.063087 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2304/3951 (58%)]\tLoss: 4.049696 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2432/3951 (61%)]\tLoss: 4.068131 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2560/3951 (65%)]\tLoss: 4.097982 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2688/3951 (68%)]\tLoss: 4.011491 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2816/3951 (71%)]\tLoss: 4.037936 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [2944/3951 (74%)]\tLoss: 4.030166 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3072/3951 (77%)]\tLoss: 4.067662 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3200/3951 (81%)]\tLoss: 3.990889 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3328/3951 (84%)]\tLoss: 4.031567 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3456/3951 (87%)]\tLoss: 4.059805 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3584/3951 (90%)]\tLoss: 4.080955 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3712/3951 (94%)]\tLoss: 4.077193 Lr: 1.6000000000000003e-05\n",
            "Train Epoch: 15 [3330/3951 (97%)]\tLoss: 4.069906 Lr: 1.6000000000000003e-05\n",
            "\n",
            "Test set: Average loss: 0.0401, Accuracy: 23/406 (6%)\n",
            "\n",
            "Train Epoch: 16 [0/3951 (0%)]\tLoss: 3.969946 Lr: 1.7e-05\n",
            "Train Epoch: 16 [128/3951 (3%)]\tLoss: 4.034841 Lr: 1.7e-05\n",
            "Train Epoch: 16 [256/3951 (6%)]\tLoss: 4.052197 Lr: 1.7e-05\n",
            "Train Epoch: 16 [384/3951 (10%)]\tLoss: 4.040789 Lr: 1.7e-05\n",
            "Train Epoch: 16 [512/3951 (13%)]\tLoss: 4.049292 Lr: 1.7e-05\n",
            "\n",
            "Test set: Average loss: 0.0401, Accuracy: 23/406 (6%)\n",
            "\n",
            "Train Epoch: 16 [640/3951 (16%)]\tLoss: 4.030195 Lr: 1.7e-05\n",
            "Train Epoch: 16 [768/3951 (19%)]\tLoss: 4.014159 Lr: 1.7e-05\n",
            "Train Epoch: 16 [896/3951 (23%)]\tLoss: 4.051712 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1024/3951 (26%)]\tLoss: 4.077183 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1152/3951 (29%)]\tLoss: 4.057750 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1280/3951 (32%)]\tLoss: 3.991377 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1408/3951 (35%)]\tLoss: 4.039265 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1536/3951 (39%)]\tLoss: 4.036840 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1664/3951 (42%)]\tLoss: 4.040238 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1792/3951 (45%)]\tLoss: 4.027505 Lr: 1.7e-05\n",
            "Train Epoch: 16 [1920/3951 (48%)]\tLoss: 4.064938 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2048/3951 (52%)]\tLoss: 4.142088 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2176/3951 (55%)]\tLoss: 4.064041 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2304/3951 (58%)]\tLoss: 4.064810 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2432/3951 (61%)]\tLoss: 4.017519 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2560/3951 (65%)]\tLoss: 4.071062 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2688/3951 (68%)]\tLoss: 4.028077 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2816/3951 (71%)]\tLoss: 4.059175 Lr: 1.7e-05\n",
            "Train Epoch: 16 [2944/3951 (74%)]\tLoss: 3.998835 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3072/3951 (77%)]\tLoss: 4.028898 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3200/3951 (81%)]\tLoss: 4.044296 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3328/3951 (84%)]\tLoss: 4.029851 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3456/3951 (87%)]\tLoss: 3.989946 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3584/3951 (90%)]\tLoss: 4.053985 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3712/3951 (94%)]\tLoss: 4.025653 Lr: 1.7e-05\n",
            "Train Epoch: 16 [3330/3951 (97%)]\tLoss: 4.016943 Lr: 1.7e-05\n",
            "\n",
            "Test set: Average loss: 0.0399, Accuracy: 23/406 (6%)\n",
            "\n",
            "Train Epoch: 17 [0/3951 (0%)]\tLoss: 4.061799 Lr: 1.8e-05\n",
            "Train Epoch: 17 [128/3951 (3%)]\tLoss: 4.008022 Lr: 1.8e-05\n",
            "Train Epoch: 17 [256/3951 (6%)]\tLoss: 4.051350 Lr: 1.8e-05\n",
            "Train Epoch: 17 [384/3951 (10%)]\tLoss: 3.980385 Lr: 1.8e-05\n",
            "Train Epoch: 17 [512/3951 (13%)]\tLoss: 4.057560 Lr: 1.8e-05\n",
            "Train Epoch: 17 [640/3951 (16%)]\tLoss: 3.995116 Lr: 1.8e-05\n",
            "Train Epoch: 17 [768/3951 (19%)]\tLoss: 4.035924 Lr: 1.8e-05\n",
            "Train Epoch: 17 [896/3951 (23%)]\tLoss: 4.059489 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1024/3951 (26%)]\tLoss: 3.973231 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1152/3951 (29%)]\tLoss: 4.010172 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1280/3951 (32%)]\tLoss: 4.020523 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1408/3951 (35%)]\tLoss: 4.018174 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1536/3951 (39%)]\tLoss: 4.068493 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1664/3951 (42%)]\tLoss: 3.994344 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1792/3951 (45%)]\tLoss: 3.963332 Lr: 1.8e-05\n",
            "Train Epoch: 17 [1920/3951 (48%)]\tLoss: 4.036340 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2048/3951 (52%)]\tLoss: 4.001309 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2176/3951 (55%)]\tLoss: 4.057420 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2304/3951 (58%)]\tLoss: 4.012256 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2432/3951 (61%)]\tLoss: 4.048851 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2560/3951 (65%)]\tLoss: 4.017413 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2688/3951 (68%)]\tLoss: 3.945226 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2816/3951 (71%)]\tLoss: 4.021028 Lr: 1.8e-05\n",
            "Train Epoch: 17 [2944/3951 (74%)]\tLoss: 3.963242 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3072/3951 (77%)]\tLoss: 3.997182 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3200/3951 (81%)]\tLoss: 4.039675 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3328/3951 (84%)]\tLoss: 4.017240 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3456/3951 (87%)]\tLoss: 4.025102 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3584/3951 (90%)]\tLoss: 4.108458 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3712/3951 (94%)]\tLoss: 4.019341 Lr: 1.8e-05\n",
            "Train Epoch: 17 [3330/3951 (97%)]\tLoss: 3.980586 Lr: 1.8e-05\n",
            "\n",
            "Test set: Average loss: 0.0397, Accuracy: 30/406 (7%)\n",
            "\n",
            "Train Epoch: 18 [0/3951 (0%)]\tLoss: 4.000728 Lr: 1.9e-05\n",
            "Train Epoch: 18 [128/3951 (3%)]\tLoss: 4.041726 Lr: 1.9e-05\n",
            "Train Epoch: 18 [256/3951 (6%)]\tLoss: 4.048012 Lr: 1.9e-05\n",
            "Train Epoch: 18 [384/3951 (10%)]\tLoss: 3.955849 Lr: 1.9e-05\n",
            "Train Epoch: 18 [512/3951 (13%)]\tLoss: 4.018830 Lr: 1.9e-05\n",
            "Train Epoch: 18 [640/3951 (16%)]\tLoss: 4.005219 Lr: 1.9e-05\n",
            "Train Epoch: 18 [768/3951 (19%)]\tLoss: 3.970662 Lr: 1.9e-05\n",
            "Train Epoch: 18 [896/3951 (23%)]\tLoss: 4.016697 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1024/3951 (26%)]\tLoss: 4.009996 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1152/3951 (29%)]\tLoss: 3.981012 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1280/3951 (32%)]\tLoss: 4.057764 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1408/3951 (35%)]\tLoss: 4.003476 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1536/3951 (39%)]\tLoss: 3.982965 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1664/3951 (42%)]\tLoss: 4.010869 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1792/3951 (45%)]\tLoss: 3.983804 Lr: 1.9e-05\n",
            "Train Epoch: 18 [1920/3951 (48%)]\tLoss: 3.976930 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2048/3951 (52%)]\tLoss: 3.940762 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2176/3951 (55%)]\tLoss: 4.029833 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2304/3951 (58%)]\tLoss: 4.017716 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2432/3951 (61%)]\tLoss: 3.975321 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2560/3951 (65%)]\tLoss: 3.971278 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2688/3951 (68%)]\tLoss: 4.076426 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2816/3951 (71%)]\tLoss: 4.012686 Lr: 1.9e-05\n",
            "Train Epoch: 18 [2944/3951 (74%)]\tLoss: 4.040020 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3072/3951 (77%)]\tLoss: 3.966451 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3200/3951 (81%)]\tLoss: 4.015052 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3328/3951 (84%)]\tLoss: 3.979391 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3456/3951 (87%)]\tLoss: 4.006383 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3584/3951 (90%)]\tLoss: 3.960570 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3712/3951 (94%)]\tLoss: 4.008109 Lr: 1.9e-05\n",
            "Train Epoch: 18 [3330/3951 (97%)]\tLoss: 3.987664 Lr: 1.9e-05\n",
            "\n",
            "Test set: Average loss: 0.0395, Accuracy: 32/406 (8%)\n",
            "\n",
            "Train Epoch: 19 [0/3951 (0%)]\tLoss: 3.928288 Lr: 2e-05\n",
            "Train Epoch: 19 [128/3951 (3%)]\tLoss: 4.015709 Lr: 2e-05\n",
            "Train Epoch: 19 [256/3951 (6%)]\tLoss: 4.004161 Lr: 2e-05\n",
            "Train Epoch: 19 [384/3951 (10%)]\tLoss: 3.967651 Lr: 2e-05\n",
            "Train Epoch: 19 [512/3951 (13%)]\tLoss: 3.956479 Lr: 2e-05\n",
            "Train Epoch: 19 [640/3951 (16%)]\tLoss: 4.016850 Lr: 2e-05\n",
            "Train Epoch: 19 [768/3951 (19%)]\tLoss: 3.963478 Lr: 2e-05\n",
            "Train Epoch: 19 [896/3951 (23%)]\tLoss: 3.929434 Lr: 2e-05\n",
            "Train Epoch: 19 [1024/3951 (26%)]\tLoss: 3.970654 Lr: 2e-05\n",
            "Train Epoch: 19 [1152/3951 (29%)]\tLoss: 3.929065 Lr: 2e-05\n",
            "Train Epoch: 19 [1280/3951 (32%)]\tLoss: 4.056836 Lr: 2e-05\n",
            "Train Epoch: 19 [1408/3951 (35%)]\tLoss: 4.021933 Lr: 2e-05\n",
            "\n",
            "Test set: Average loss: 0.0394, Accuracy: 33/406 (8%)\n",
            "\n",
            "Train Epoch: 19 [1536/3951 (39%)]\tLoss: 3.996443 Lr: 2e-05\n",
            "Train Epoch: 19 [1664/3951 (42%)]\tLoss: 3.969860 Lr: 2e-05\n",
            "Train Epoch: 19 [1792/3951 (45%)]\tLoss: 3.992847 Lr: 2e-05\n",
            "Train Epoch: 19 [1920/3951 (48%)]\tLoss: 3.974789 Lr: 2e-05\n",
            "Train Epoch: 19 [2048/3951 (52%)]\tLoss: 3.964644 Lr: 2e-05\n",
            "Train Epoch: 19 [2176/3951 (55%)]\tLoss: 4.020097 Lr: 2e-05\n",
            "Train Epoch: 19 [2304/3951 (58%)]\tLoss: 3.928384 Lr: 2e-05\n",
            "Train Epoch: 19 [2432/3951 (61%)]\tLoss: 3.993930 Lr: 2e-05\n",
            "Train Epoch: 19 [2560/3951 (65%)]\tLoss: 3.935809 Lr: 2e-05\n",
            "Train Epoch: 19 [2688/3951 (68%)]\tLoss: 3.939956 Lr: 2e-05\n",
            "Train Epoch: 19 [2816/3951 (71%)]\tLoss: 3.926172 Lr: 2e-05\n",
            "Train Epoch: 19 [2944/3951 (74%)]\tLoss: 3.929215 Lr: 2e-05\n",
            "Train Epoch: 19 [3072/3951 (77%)]\tLoss: 3.998573 Lr: 2e-05\n",
            "Train Epoch: 19 [3200/3951 (81%)]\tLoss: 3.942036 Lr: 2e-05\n",
            "Train Epoch: 19 [3328/3951 (84%)]\tLoss: 3.952050 Lr: 2e-05\n",
            "Train Epoch: 19 [3456/3951 (87%)]\tLoss: 3.978247 Lr: 2e-05\n",
            "Train Epoch: 19 [3584/3951 (90%)]\tLoss: 3.914772 Lr: 2e-05\n",
            "Train Epoch: 19 [3712/3951 (94%)]\tLoss: 3.997844 Lr: 2e-05\n",
            "Train Epoch: 19 [3330/3951 (97%)]\tLoss: 3.996739 Lr: 2e-05\n",
            "\n",
            "Test set: Average loss: 0.0393, Accuracy: 34/406 (8%)\n",
            "\n",
            "Train Epoch: 20 [0/3951 (0%)]\tLoss: 3.949374 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [128/3951 (3%)]\tLoss: 4.013205 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [256/3951 (6%)]\tLoss: 3.941482 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [384/3951 (10%)]\tLoss: 3.977399 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [512/3951 (13%)]\tLoss: 3.969388 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [640/3951 (16%)]\tLoss: 3.997313 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [768/3951 (19%)]\tLoss: 3.995329 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [896/3951 (23%)]\tLoss: 3.919662 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1024/3951 (26%)]\tLoss: 3.960572 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1152/3951 (29%)]\tLoss: 3.975117 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1280/3951 (32%)]\tLoss: 3.976042 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1408/3951 (35%)]\tLoss: 3.974495 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1536/3951 (39%)]\tLoss: 4.005460 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1664/3951 (42%)]\tLoss: 3.959281 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1792/3951 (45%)]\tLoss: 3.986259 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [1920/3951 (48%)]\tLoss: 3.980921 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2048/3951 (52%)]\tLoss: 3.939656 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2176/3951 (55%)]\tLoss: 3.977639 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2304/3951 (58%)]\tLoss: 3.937960 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2432/3951 (61%)]\tLoss: 3.947664 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2560/3951 (65%)]\tLoss: 3.961475 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2688/3951 (68%)]\tLoss: 4.006284 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2816/3951 (71%)]\tLoss: 3.923350 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [2944/3951 (74%)]\tLoss: 4.017989 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3072/3951 (77%)]\tLoss: 3.944679 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3200/3951 (81%)]\tLoss: 3.960675 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3328/3951 (84%)]\tLoss: 3.980027 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3456/3951 (87%)]\tLoss: 3.952693 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3584/3951 (90%)]\tLoss: 3.923795 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3712/3951 (94%)]\tLoss: 3.934309 Lr: 1.9992290362407232e-05\n",
            "Train Epoch: 20 [3330/3951 (97%)]\tLoss: 3.958749 Lr: 1.9992290362407232e-05\n",
            "\n",
            "Test set: Average loss: 0.0391, Accuracy: 39/406 (10%)\n",
            "\n",
            "Train Epoch: 21 [0/3951 (0%)]\tLoss: 3.930302 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [128/3951 (3%)]\tLoss: 3.961007 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [256/3951 (6%)]\tLoss: 3.935419 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [384/3951 (10%)]\tLoss: 3.997609 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [512/3951 (13%)]\tLoss: 3.962142 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [640/3951 (16%)]\tLoss: 3.931668 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [768/3951 (19%)]\tLoss: 3.962008 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [896/3951 (23%)]\tLoss: 3.910195 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1024/3951 (26%)]\tLoss: 3.975216 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1152/3951 (29%)]\tLoss: 3.914141 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1280/3951 (32%)]\tLoss: 3.909505 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1408/3951 (35%)]\tLoss: 4.016010 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1536/3951 (39%)]\tLoss: 3.905946 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1664/3951 (42%)]\tLoss: 3.998160 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1792/3951 (45%)]\tLoss: 3.940181 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [1920/3951 (48%)]\tLoss: 3.947669 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2048/3951 (52%)]\tLoss: 3.918391 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2176/3951 (55%)]\tLoss: 3.935356 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2304/3951 (58%)]\tLoss: 3.917233 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2432/3951 (61%)]\tLoss: 3.982366 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2560/3951 (65%)]\tLoss: 3.879133 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2688/3951 (68%)]\tLoss: 3.934964 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2816/3951 (71%)]\tLoss: 3.925320 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [2944/3951 (74%)]\tLoss: 3.946944 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3072/3951 (77%)]\tLoss: 4.008536 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3200/3951 (81%)]\tLoss: 3.982775 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3328/3951 (84%)]\tLoss: 3.980003 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3456/3951 (87%)]\tLoss: 3.844357 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3584/3951 (90%)]\tLoss: 3.928646 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3712/3951 (94%)]\tLoss: 3.930656 Lr: 1.9969173337331283e-05\n",
            "Train Epoch: 21 [3330/3951 (97%)]\tLoss: 3.944476 Lr: 1.9969173337331283e-05\n",
            "\n",
            "Test set: Average loss: 0.0389, Accuracy: 44/406 (11%)\n",
            "\n",
            "Train Epoch: 22 [0/3951 (0%)]\tLoss: 3.896804 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [128/3951 (3%)]\tLoss: 4.015845 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [256/3951 (6%)]\tLoss: 3.992384 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [384/3951 (10%)]\tLoss: 3.947729 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [512/3951 (13%)]\tLoss: 3.907234 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [640/3951 (16%)]\tLoss: 3.908696 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [768/3951 (19%)]\tLoss: 3.943824 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [896/3951 (23%)]\tLoss: 3.938885 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1024/3951 (26%)]\tLoss: 3.931120 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1152/3951 (29%)]\tLoss: 3.934559 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1280/3951 (32%)]\tLoss: 3.929486 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1408/3951 (35%)]\tLoss: 3.924701 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1536/3951 (39%)]\tLoss: 3.898848 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1664/3951 (42%)]\tLoss: 3.907995 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1792/3951 (45%)]\tLoss: 3.932199 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [1920/3951 (48%)]\tLoss: 3.894744 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2048/3951 (52%)]\tLoss: 3.904496 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2176/3951 (55%)]\tLoss: 3.910505 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2304/3951 (58%)]\tLoss: 3.933433 Lr: 1.9930684569549265e-05\n",
            "\n",
            "Test set: Average loss: 0.0388, Accuracy: 45/406 (11%)\n",
            "\n",
            "Train Epoch: 22 [2432/3951 (61%)]\tLoss: 3.984206 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2560/3951 (65%)]\tLoss: 3.964232 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2688/3951 (68%)]\tLoss: 3.904178 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2816/3951 (71%)]\tLoss: 3.929686 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [2944/3951 (74%)]\tLoss: 3.960862 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3072/3951 (77%)]\tLoss: 3.928404 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3200/3951 (81%)]\tLoss: 3.920870 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3328/3951 (84%)]\tLoss: 3.885715 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3456/3951 (87%)]\tLoss: 3.903175 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3584/3951 (90%)]\tLoss: 3.934067 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3712/3951 (94%)]\tLoss: 3.944039 Lr: 1.9930684569549265e-05\n",
            "Train Epoch: 22 [3330/3951 (97%)]\tLoss: 3.907573 Lr: 1.9930684569549265e-05\n",
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 48/406 (12%)\n",
            "\n",
            "Train Epoch: 23 [0/3951 (0%)]\tLoss: 3.916403 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [128/3951 (3%)]\tLoss: 3.885373 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [256/3951 (6%)]\tLoss: 3.895435 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [384/3951 (10%)]\tLoss: 3.927922 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [512/3951 (13%)]\tLoss: 3.909036 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [640/3951 (16%)]\tLoss: 3.906837 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [768/3951 (19%)]\tLoss: 3.925434 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [896/3951 (23%)]\tLoss: 3.891701 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1024/3951 (26%)]\tLoss: 3.923278 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1152/3951 (29%)]\tLoss: 3.842499 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1280/3951 (32%)]\tLoss: 3.926638 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1408/3951 (35%)]\tLoss: 3.987583 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1536/3951 (39%)]\tLoss: 3.873082 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1664/3951 (42%)]\tLoss: 3.857577 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1792/3951 (45%)]\tLoss: 3.934506 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [1920/3951 (48%)]\tLoss: 3.902275 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2048/3951 (52%)]\tLoss: 4.003744 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2176/3951 (55%)]\tLoss: 3.959105 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2304/3951 (58%)]\tLoss: 3.917276 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2432/3951 (61%)]\tLoss: 3.879734 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2560/3951 (65%)]\tLoss: 3.968722 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2688/3951 (68%)]\tLoss: 3.903262 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2816/3951 (71%)]\tLoss: 3.879375 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [2944/3951 (74%)]\tLoss: 3.946453 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3072/3951 (77%)]\tLoss: 3.906756 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3200/3951 (81%)]\tLoss: 3.911901 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3328/3951 (84%)]\tLoss: 3.941044 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3456/3951 (87%)]\tLoss: 3.818917 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3584/3951 (90%)]\tLoss: 3.922815 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3712/3951 (94%)]\tLoss: 3.896473 Lr: 1.9876883405951378e-05\n",
            "Train Epoch: 23 [3330/3951 (97%)]\tLoss: 3.899866 Lr: 1.9876883405951378e-05\n",
            "\n",
            "Test set: Average loss: 0.0386, Accuracy: 53/406 (13%)\n",
            "\n",
            "Train Epoch: 24 [0/3951 (0%)]\tLoss: 3.908181 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [128/3951 (3%)]\tLoss: 3.883425 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [256/3951 (6%)]\tLoss: 3.910167 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [384/3951 (10%)]\tLoss: 3.913551 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [512/3951 (13%)]\tLoss: 3.927516 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [640/3951 (16%)]\tLoss: 3.831932 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [768/3951 (19%)]\tLoss: 3.887851 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [896/3951 (23%)]\tLoss: 3.886310 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1024/3951 (26%)]\tLoss: 3.866549 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1152/3951 (29%)]\tLoss: 3.906156 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1280/3951 (32%)]\tLoss: 3.916506 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1408/3951 (35%)]\tLoss: 3.921206 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1536/3951 (39%)]\tLoss: 3.915440 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1664/3951 (42%)]\tLoss: 3.867288 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1792/3951 (45%)]\tLoss: 3.872608 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [1920/3951 (48%)]\tLoss: 3.868627 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2048/3951 (52%)]\tLoss: 3.938138 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2176/3951 (55%)]\tLoss: 3.885458 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2304/3951 (58%)]\tLoss: 3.947107 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2432/3951 (61%)]\tLoss: 3.952660 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2560/3951 (65%)]\tLoss: 3.844334 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2688/3951 (68%)]\tLoss: 3.988179 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2816/3951 (71%)]\tLoss: 3.983916 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [2944/3951 (74%)]\tLoss: 3.871540 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3072/3951 (77%)]\tLoss: 3.816600 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3200/3951 (81%)]\tLoss: 3.823360 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3328/3951 (84%)]\tLoss: 3.833755 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3456/3951 (87%)]\tLoss: 3.941256 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3584/3951 (90%)]\tLoss: 3.922099 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3712/3951 (94%)]\tLoss: 3.889426 Lr: 1.9807852804032306e-05\n",
            "Train Epoch: 24 [3330/3951 (97%)]\tLoss: 3.934957 Lr: 1.9807852804032306e-05\n",
            "\n",
            "Test set: Average loss: 0.0384, Accuracy: 52/406 (13%)\n",
            "\n",
            "Train Epoch: 25 [0/3951 (0%)]\tLoss: 3.867152 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [128/3951 (3%)]\tLoss: 3.884387 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [256/3951 (6%)]\tLoss: 3.956134 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [384/3951 (10%)]\tLoss: 3.932932 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [512/3951 (13%)]\tLoss: 3.867361 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [640/3951 (16%)]\tLoss: 3.888450 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [768/3951 (19%)]\tLoss: 3.948031 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [896/3951 (23%)]\tLoss: 3.818324 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1024/3951 (26%)]\tLoss: 3.887504 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1152/3951 (29%)]\tLoss: 3.874280 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1280/3951 (32%)]\tLoss: 3.831752 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1408/3951 (35%)]\tLoss: 3.864521 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1536/3951 (39%)]\tLoss: 3.903231 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1664/3951 (42%)]\tLoss: 3.835360 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1792/3951 (45%)]\tLoss: 3.837233 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [1920/3951 (48%)]\tLoss: 3.913190 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2048/3951 (52%)]\tLoss: 3.843328 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2176/3951 (55%)]\tLoss: 3.843190 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2304/3951 (58%)]\tLoss: 3.836663 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2432/3951 (61%)]\tLoss: 3.895354 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2560/3951 (65%)]\tLoss: 3.980414 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2688/3951 (68%)]\tLoss: 3.841419 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2816/3951 (71%)]\tLoss: 3.927370 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [2944/3951 (74%)]\tLoss: 3.879418 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [3072/3951 (77%)]\tLoss: 3.829657 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [3200/3951 (81%)]\tLoss: 3.837806 Lr: 1.9723699203976768e-05\n",
            "\n",
            "Test set: Average loss: 0.0382, Accuracy: 53/406 (13%)\n",
            "\n",
            "Train Epoch: 25 [3328/3951 (84%)]\tLoss: 3.941292 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [3456/3951 (87%)]\tLoss: 3.905454 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [3584/3951 (90%)]\tLoss: 3.808980 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [3712/3951 (94%)]\tLoss: 3.954682 Lr: 1.9723699203976768e-05\n",
            "Train Epoch: 25 [3330/3951 (97%)]\tLoss: 3.917869 Lr: 1.9723699203976768e-05\n",
            "\n",
            "Test set: Average loss: 0.0382, Accuracy: 52/406 (13%)\n",
            "\n",
            "Train Epoch: 26 [0/3951 (0%)]\tLoss: 3.915084 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [128/3951 (3%)]\tLoss: 3.874145 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [256/3951 (6%)]\tLoss: 3.853212 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [384/3951 (10%)]\tLoss: 3.828802 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [512/3951 (13%)]\tLoss: 3.837986 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [640/3951 (16%)]\tLoss: 3.887212 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [768/3951 (19%)]\tLoss: 3.898607 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [896/3951 (23%)]\tLoss: 3.921324 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1024/3951 (26%)]\tLoss: 3.876625 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1152/3951 (29%)]\tLoss: 3.875821 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1280/3951 (32%)]\tLoss: 3.888927 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1408/3951 (35%)]\tLoss: 3.874769 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1536/3951 (39%)]\tLoss: 3.816749 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1664/3951 (42%)]\tLoss: 3.929274 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1792/3951 (45%)]\tLoss: 3.810549 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [1920/3951 (48%)]\tLoss: 3.905481 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2048/3951 (52%)]\tLoss: 3.940849 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2176/3951 (55%)]\tLoss: 3.882479 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2304/3951 (58%)]\tLoss: 3.775277 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2432/3951 (61%)]\tLoss: 3.810774 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2560/3951 (65%)]\tLoss: 3.941651 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2688/3951 (68%)]\tLoss: 3.858894 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2816/3951 (71%)]\tLoss: 3.863165 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [2944/3951 (74%)]\tLoss: 3.831590 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3072/3951 (77%)]\tLoss: 3.921241 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3200/3951 (81%)]\tLoss: 3.900020 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3328/3951 (84%)]\tLoss: 3.867115 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3456/3951 (87%)]\tLoss: 3.833976 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3584/3951 (90%)]\tLoss: 3.850874 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3712/3951 (94%)]\tLoss: 3.818874 Lr: 1.9624552364536472e-05\n",
            "Train Epoch: 26 [3330/3951 (97%)]\tLoss: 3.852929 Lr: 1.9624552364536472e-05\n",
            "\n",
            "Test set: Average loss: 0.0381, Accuracy: 51/406 (13%)\n",
            "\n",
            "Train Epoch: 27 [0/3951 (0%)]\tLoss: 3.817896 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [128/3951 (3%)]\tLoss: 3.804318 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [256/3951 (6%)]\tLoss: 3.864826 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [384/3951 (10%)]\tLoss: 3.818087 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [512/3951 (13%)]\tLoss: 3.812419 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [640/3951 (16%)]\tLoss: 3.842374 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [768/3951 (19%)]\tLoss: 3.789321 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [896/3951 (23%)]\tLoss: 3.852891 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1024/3951 (26%)]\tLoss: 3.764354 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1152/3951 (29%)]\tLoss: 3.828259 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1280/3951 (32%)]\tLoss: 3.907552 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1408/3951 (35%)]\tLoss: 3.946993 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1536/3951 (39%)]\tLoss: 3.847568 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1664/3951 (42%)]\tLoss: 3.964684 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1792/3951 (45%)]\tLoss: 3.786398 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [1920/3951 (48%)]\tLoss: 3.863877 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2048/3951 (52%)]\tLoss: 3.791366 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2176/3951 (55%)]\tLoss: 3.788531 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2304/3951 (58%)]\tLoss: 3.818189 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2432/3951 (61%)]\tLoss: 3.904737 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2560/3951 (65%)]\tLoss: 3.829353 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2688/3951 (68%)]\tLoss: 3.896962 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2816/3951 (71%)]\tLoss: 3.878976 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [2944/3951 (74%)]\tLoss: 3.773110 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3072/3951 (77%)]\tLoss: 3.885105 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3200/3951 (81%)]\tLoss: 3.881539 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3328/3951 (84%)]\tLoss: 3.842316 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3456/3951 (87%)]\tLoss: 3.905692 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3584/3951 (90%)]\tLoss: 3.892699 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3712/3951 (94%)]\tLoss: 3.808448 Lr: 1.9510565162951538e-05\n",
            "Train Epoch: 27 [3330/3951 (97%)]\tLoss: 3.878497 Lr: 1.9510565162951538e-05\n",
            "\n",
            "Test set: Average loss: 0.0379, Accuracy: 52/406 (13%)\n",
            "\n",
            "Train Epoch: 28 [0/3951 (0%)]\tLoss: 3.867357 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [128/3951 (3%)]\tLoss: 3.884696 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [256/3951 (6%)]\tLoss: 3.907060 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [384/3951 (10%)]\tLoss: 3.853765 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [512/3951 (13%)]\tLoss: 3.855400 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [640/3951 (16%)]\tLoss: 3.763949 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [768/3951 (19%)]\tLoss: 3.862959 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [896/3951 (23%)]\tLoss: 3.825108 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1024/3951 (26%)]\tLoss: 3.762827 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1152/3951 (29%)]\tLoss: 3.868386 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1280/3951 (32%)]\tLoss: 3.766752 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1408/3951 (35%)]\tLoss: 3.842644 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1536/3951 (39%)]\tLoss: 3.847020 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1664/3951 (42%)]\tLoss: 3.860866 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1792/3951 (45%)]\tLoss: 3.830991 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [1920/3951 (48%)]\tLoss: 3.893147 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2048/3951 (52%)]\tLoss: 3.880048 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2176/3951 (55%)]\tLoss: 3.831677 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2304/3951 (58%)]\tLoss: 3.755008 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2432/3951 (61%)]\tLoss: 3.872454 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2560/3951 (65%)]\tLoss: 3.807117 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2688/3951 (68%)]\tLoss: 3.815324 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2816/3951 (71%)]\tLoss: 3.790649 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [2944/3951 (74%)]\tLoss: 3.842579 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3072/3951 (77%)]\tLoss: 3.817165 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3200/3951 (81%)]\tLoss: 3.774076 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3328/3951 (84%)]\tLoss: 3.849833 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3456/3951 (87%)]\tLoss: 3.854214 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3584/3951 (90%)]\tLoss: 3.766341 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3712/3951 (94%)]\tLoss: 3.822840 Lr: 1.9381913359224844e-05\n",
            "Train Epoch: 28 [3330/3951 (97%)]\tLoss: 3.871853 Lr: 1.9381913359224844e-05\n",
            "\n",
            "Test set: Average loss: 0.0378, Accuracy: 53/406 (13%)\n",
            "\n",
            "Train Epoch: 29 [0/3951 (0%)]\tLoss: 3.830744 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [128/3951 (3%)]\tLoss: 3.922447 Lr: 1.9238795325112867e-05\n",
            "\n",
            "Test set: Average loss: 0.0378, Accuracy: 53/406 (13%)\n",
            "\n",
            "Train Epoch: 29 [256/3951 (6%)]\tLoss: 3.825270 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [384/3951 (10%)]\tLoss: 3.827742 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [512/3951 (13%)]\tLoss: 3.836814 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [640/3951 (16%)]\tLoss: 3.840147 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [768/3951 (19%)]\tLoss: 3.827982 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [896/3951 (23%)]\tLoss: 3.724517 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1024/3951 (26%)]\tLoss: 3.889488 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1152/3951 (29%)]\tLoss: 3.825823 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1280/3951 (32%)]\tLoss: 3.851398 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1408/3951 (35%)]\tLoss: 3.858373 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1536/3951 (39%)]\tLoss: 3.800048 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1664/3951 (42%)]\tLoss: 3.772396 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1792/3951 (45%)]\tLoss: 3.846979 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [1920/3951 (48%)]\tLoss: 3.841344 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2048/3951 (52%)]\tLoss: 3.875559 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2176/3951 (55%)]\tLoss: 3.874163 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2304/3951 (58%)]\tLoss: 3.770766 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2432/3951 (61%)]\tLoss: 3.834729 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2560/3951 (65%)]\tLoss: 3.768885 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2688/3951 (68%)]\tLoss: 3.828065 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2816/3951 (71%)]\tLoss: 3.801540 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [2944/3951 (74%)]\tLoss: 3.786617 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3072/3951 (77%)]\tLoss: 3.760149 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3200/3951 (81%)]\tLoss: 3.866680 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3328/3951 (84%)]\tLoss: 3.815854 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3456/3951 (87%)]\tLoss: 3.731301 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3584/3951 (90%)]\tLoss: 3.769912 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3712/3951 (94%)]\tLoss: 3.800869 Lr: 1.9238795325112867e-05\n",
            "Train Epoch: 29 [3330/3951 (97%)]\tLoss: 3.803804 Lr: 1.9238795325112867e-05\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 55/406 (14%)\n",
            "\n",
            "Train Epoch: 30 [0/3951 (0%)]\tLoss: 3.865374 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [128/3951 (3%)]\tLoss: 3.758747 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [256/3951 (6%)]\tLoss: 3.817311 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [384/3951 (10%)]\tLoss: 3.794038 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [512/3951 (13%)]\tLoss: 3.752166 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [640/3951 (16%)]\tLoss: 3.774059 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [768/3951 (19%)]\tLoss: 3.848047 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [896/3951 (23%)]\tLoss: 3.833537 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1024/3951 (26%)]\tLoss: 3.774003 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1152/3951 (29%)]\tLoss: 3.731797 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1280/3951 (32%)]\tLoss: 3.859512 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1408/3951 (35%)]\tLoss: 3.779059 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1536/3951 (39%)]\tLoss: 3.825173 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1664/3951 (42%)]\tLoss: 3.832920 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1792/3951 (45%)]\tLoss: 3.813184 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [1920/3951 (48%)]\tLoss: 3.886029 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2048/3951 (52%)]\tLoss: 3.890415 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2176/3951 (55%)]\tLoss: 3.767403 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2304/3951 (58%)]\tLoss: 3.838929 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2432/3951 (61%)]\tLoss: 3.760344 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2560/3951 (65%)]\tLoss: 3.827226 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2688/3951 (68%)]\tLoss: 3.897216 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2816/3951 (71%)]\tLoss: 3.724234 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [2944/3951 (74%)]\tLoss: 3.785312 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3072/3951 (77%)]\tLoss: 3.790199 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3200/3951 (81%)]\tLoss: 3.763896 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3328/3951 (84%)]\tLoss: 3.791200 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3456/3951 (87%)]\tLoss: 3.760453 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3584/3951 (90%)]\tLoss: 3.753327 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3712/3951 (94%)]\tLoss: 3.966177 Lr: 1.9081431738250815e-05\n",
            "Train Epoch: 30 [3330/3951 (97%)]\tLoss: 3.879440 Lr: 1.9081431738250815e-05\n",
            "\n",
            "Test set: Average loss: 0.0375, Accuracy: 55/406 (14%)\n",
            "\n",
            "Train Epoch: 31 [0/3951 (0%)]\tLoss: 3.859061 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [128/3951 (3%)]\tLoss: 3.757993 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [256/3951 (6%)]\tLoss: 3.718057 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [384/3951 (10%)]\tLoss: 3.796153 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [512/3951 (13%)]\tLoss: 3.840297 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [640/3951 (16%)]\tLoss: 3.774735 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [768/3951 (19%)]\tLoss: 3.728995 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [896/3951 (23%)]\tLoss: 3.880090 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1024/3951 (26%)]\tLoss: 3.816002 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1152/3951 (29%)]\tLoss: 3.864364 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1280/3951 (32%)]\tLoss: 3.789393 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1408/3951 (35%)]\tLoss: 3.636504 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1536/3951 (39%)]\tLoss: 3.725352 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1664/3951 (42%)]\tLoss: 3.827355 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1792/3951 (45%)]\tLoss: 3.834928 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [1920/3951 (48%)]\tLoss: 3.867551 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2048/3951 (52%)]\tLoss: 3.760092 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2176/3951 (55%)]\tLoss: 3.868517 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2304/3951 (58%)]\tLoss: 3.810218 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2432/3951 (61%)]\tLoss: 3.825144 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2560/3951 (65%)]\tLoss: 3.789754 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2688/3951 (68%)]\tLoss: 3.842692 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2816/3951 (71%)]\tLoss: 3.796211 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [2944/3951 (74%)]\tLoss: 3.761138 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3072/3951 (77%)]\tLoss: 3.769930 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3200/3951 (81%)]\tLoss: 3.748858 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3328/3951 (84%)]\tLoss: 3.856379 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3456/3951 (87%)]\tLoss: 3.734466 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3584/3951 (90%)]\tLoss: 3.841280 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3712/3951 (94%)]\tLoss: 3.885706 Lr: 1.891006524188368e-05\n",
            "Train Epoch: 31 [3330/3951 (97%)]\tLoss: 3.781698 Lr: 1.891006524188368e-05\n",
            "\n",
            "Test set: Average loss: 0.0373, Accuracy: 58/406 (14%)\n",
            "\n",
            "Train Epoch: 32 [0/3951 (0%)]\tLoss: 3.837746 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [128/3951 (3%)]\tLoss: 3.791147 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [256/3951 (6%)]\tLoss: 3.743721 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [384/3951 (10%)]\tLoss: 3.825178 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [512/3951 (13%)]\tLoss: 3.831094 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [640/3951 (16%)]\tLoss: 3.782290 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [768/3951 (19%)]\tLoss: 3.790735 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [896/3951 (23%)]\tLoss: 3.777562 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1024/3951 (26%)]\tLoss: 3.832677 Lr: 1.8724960070727974e-05\n",
            "\n",
            "Test set: Average loss: 0.0373, Accuracy: 58/406 (14%)\n",
            "\n",
            "Train Epoch: 32 [1152/3951 (29%)]\tLoss: 3.774161 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1280/3951 (32%)]\tLoss: 3.786005 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1408/3951 (35%)]\tLoss: 3.730319 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1536/3951 (39%)]\tLoss: 3.807943 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1664/3951 (42%)]\tLoss: 3.756225 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1792/3951 (45%)]\tLoss: 3.758813 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [1920/3951 (48%)]\tLoss: 3.833884 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2048/3951 (52%)]\tLoss: 3.826870 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2176/3951 (55%)]\tLoss: 3.833119 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2304/3951 (58%)]\tLoss: 3.732039 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2432/3951 (61%)]\tLoss: 3.813349 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2560/3951 (65%)]\tLoss: 3.748946 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2688/3951 (68%)]\tLoss: 3.776953 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2816/3951 (71%)]\tLoss: 3.726682 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [2944/3951 (74%)]\tLoss: 3.841727 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3072/3951 (77%)]\tLoss: 3.789612 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3200/3951 (81%)]\tLoss: 3.750311 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3328/3951 (84%)]\tLoss: 3.805854 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3456/3951 (87%)]\tLoss: 3.762247 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3584/3951 (90%)]\tLoss: 3.712377 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3712/3951 (94%)]\tLoss: 3.711027 Lr: 1.8724960070727974e-05\n",
            "Train Epoch: 32 [3330/3951 (97%)]\tLoss: 3.825551 Lr: 1.8724960070727974e-05\n",
            "\n",
            "Test set: Average loss: 0.0372, Accuracy: 58/406 (14%)\n",
            "\n",
            "Train Epoch: 33 [0/3951 (0%)]\tLoss: 3.891034 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [128/3951 (3%)]\tLoss: 3.801252 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [256/3951 (6%)]\tLoss: 3.837965 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [384/3951 (10%)]\tLoss: 3.677966 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [512/3951 (13%)]\tLoss: 3.831339 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [640/3951 (16%)]\tLoss: 3.781188 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [768/3951 (19%)]\tLoss: 3.810088 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [896/3951 (23%)]\tLoss: 3.750852 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1024/3951 (26%)]\tLoss: 3.738097 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1152/3951 (29%)]\tLoss: 3.818122 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1280/3951 (32%)]\tLoss: 3.795529 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1408/3951 (35%)]\tLoss: 3.777613 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1536/3951 (39%)]\tLoss: 3.761929 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1664/3951 (42%)]\tLoss: 3.770114 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1792/3951 (45%)]\tLoss: 3.937250 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [1920/3951 (48%)]\tLoss: 3.788586 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2048/3951 (52%)]\tLoss: 3.775507 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2176/3951 (55%)]\tLoss: 3.782481 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2304/3951 (58%)]\tLoss: 3.762183 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2432/3951 (61%)]\tLoss: 3.723909 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2560/3951 (65%)]\tLoss: 3.720260 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2688/3951 (68%)]\tLoss: 3.713838 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2816/3951 (71%)]\tLoss: 3.780962 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [2944/3951 (74%)]\tLoss: 3.714543 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3072/3951 (77%)]\tLoss: 3.727414 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3200/3951 (81%)]\tLoss: 3.688940 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3328/3951 (84%)]\tLoss: 3.709515 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3456/3951 (87%)]\tLoss: 3.785152 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3584/3951 (90%)]\tLoss: 3.739627 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3712/3951 (94%)]\tLoss: 3.780545 Lr: 1.8526401643540924e-05\n",
            "Train Epoch: 33 [3330/3951 (97%)]\tLoss: 3.734269 Lr: 1.8526401643540924e-05\n",
            "\n",
            "Test set: Average loss: 0.0371, Accuracy: 59/406 (15%)\n",
            "\n",
            "Train Epoch: 34 [0/3951 (0%)]\tLoss: 3.733960 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [128/3951 (3%)]\tLoss: 3.780888 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [256/3951 (6%)]\tLoss: 3.742013 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [384/3951 (10%)]\tLoss: 3.732870 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [512/3951 (13%)]\tLoss: 3.710751 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [640/3951 (16%)]\tLoss: 3.768509 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [768/3951 (19%)]\tLoss: 3.860141 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [896/3951 (23%)]\tLoss: 3.732270 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1024/3951 (26%)]\tLoss: 3.800022 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1152/3951 (29%)]\tLoss: 3.775227 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1280/3951 (32%)]\tLoss: 3.706176 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1408/3951 (35%)]\tLoss: 3.749040 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1536/3951 (39%)]\tLoss: 3.787318 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1664/3951 (42%)]\tLoss: 3.662844 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1792/3951 (45%)]\tLoss: 3.692403 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [1920/3951 (48%)]\tLoss: 3.749931 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2048/3951 (52%)]\tLoss: 3.711594 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2176/3951 (55%)]\tLoss: 3.771069 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2304/3951 (58%)]\tLoss: 3.740354 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2432/3951 (61%)]\tLoss: 3.732420 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2560/3951 (65%)]\tLoss: 3.774161 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2688/3951 (68%)]\tLoss: 3.737930 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2816/3951 (71%)]\tLoss: 3.806991 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [2944/3951 (74%)]\tLoss: 3.785666 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3072/3951 (77%)]\tLoss: 3.800316 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3200/3951 (81%)]\tLoss: 3.768066 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3328/3951 (84%)]\tLoss: 3.749104 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3456/3951 (87%)]\tLoss: 3.825882 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3584/3951 (90%)]\tLoss: 3.721079 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3712/3951 (94%)]\tLoss: 3.763916 Lr: 1.8314696123025456e-05\n",
            "Train Epoch: 34 [3330/3951 (97%)]\tLoss: 3.736576 Lr: 1.8314696123025456e-05\n",
            "\n",
            "Test set: Average loss: 0.0370, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 35 [0/3951 (0%)]\tLoss: 3.743832 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [128/3951 (3%)]\tLoss: 3.774809 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [256/3951 (6%)]\tLoss: 3.696573 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [384/3951 (10%)]\tLoss: 3.759105 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [512/3951 (13%)]\tLoss: 3.789546 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [640/3951 (16%)]\tLoss: 3.555800 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [768/3951 (19%)]\tLoss: 3.780219 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [896/3951 (23%)]\tLoss: 3.692655 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1024/3951 (26%)]\tLoss: 3.789629 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1152/3951 (29%)]\tLoss: 3.744108 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1280/3951 (32%)]\tLoss: 3.926474 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1408/3951 (35%)]\tLoss: 3.752288 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1536/3951 (39%)]\tLoss: 3.703764 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1664/3951 (42%)]\tLoss: 3.740958 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1792/3951 (45%)]\tLoss: 3.733910 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [1920/3951 (48%)]\tLoss: 3.749063 Lr: 1.8090169943749477e-05\n",
            "\n",
            "Test set: Average loss: 0.0369, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 35 [2048/3951 (52%)]\tLoss: 3.769182 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2176/3951 (55%)]\tLoss: 3.729193 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2304/3951 (58%)]\tLoss: 3.713569 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2432/3951 (61%)]\tLoss: 3.783693 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2560/3951 (65%)]\tLoss: 3.695596 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2688/3951 (68%)]\tLoss: 3.837470 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2816/3951 (71%)]\tLoss: 3.779441 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [2944/3951 (74%)]\tLoss: 3.806745 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3072/3951 (77%)]\tLoss: 3.733509 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3200/3951 (81%)]\tLoss: 3.772928 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3328/3951 (84%)]\tLoss: 3.730714 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3456/3951 (87%)]\tLoss: 3.750884 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3584/3951 (90%)]\tLoss: 3.717579 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3712/3951 (94%)]\tLoss: 3.739086 Lr: 1.8090169943749477e-05\n",
            "Train Epoch: 35 [3330/3951 (97%)]\tLoss: 3.720181 Lr: 1.8090169943749477e-05\n",
            "\n",
            "Test set: Average loss: 0.0369, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 36 [0/3951 (0%)]\tLoss: 3.682327 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [128/3951 (3%)]\tLoss: 3.725725 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [256/3951 (6%)]\tLoss: 3.670628 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [384/3951 (10%)]\tLoss: 3.687864 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [512/3951 (13%)]\tLoss: 3.716449 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [640/3951 (16%)]\tLoss: 3.729254 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [768/3951 (19%)]\tLoss: 3.802473 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [896/3951 (23%)]\tLoss: 3.760636 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1024/3951 (26%)]\tLoss: 3.747140 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1152/3951 (29%)]\tLoss: 3.645988 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1280/3951 (32%)]\tLoss: 3.776540 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1408/3951 (35%)]\tLoss: 3.701651 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1536/3951 (39%)]\tLoss: 3.676958 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1664/3951 (42%)]\tLoss: 3.808126 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1792/3951 (45%)]\tLoss: 3.759909 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [1920/3951 (48%)]\tLoss: 3.669418 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2048/3951 (52%)]\tLoss: 3.749589 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2176/3951 (55%)]\tLoss: 3.878819 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2304/3951 (58%)]\tLoss: 3.736467 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2432/3951 (61%)]\tLoss: 3.775025 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2560/3951 (65%)]\tLoss: 3.659447 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2688/3951 (68%)]\tLoss: 3.843815 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2816/3951 (71%)]\tLoss: 3.694541 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [2944/3951 (74%)]\tLoss: 3.760623 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3072/3951 (77%)]\tLoss: 3.728303 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3200/3951 (81%)]\tLoss: 3.746617 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3328/3951 (84%)]\tLoss: 3.754719 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3456/3951 (87%)]\tLoss: 3.725859 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3584/3951 (90%)]\tLoss: 3.790793 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3712/3951 (94%)]\tLoss: 3.742620 Lr: 1.785316930880745e-05\n",
            "Train Epoch: 36 [3330/3951 (97%)]\tLoss: 3.798216 Lr: 1.785316930880745e-05\n",
            "\n",
            "Test set: Average loss: 0.0368, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 37 [0/3951 (0%)]\tLoss: 3.755206 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [128/3951 (3%)]\tLoss: 3.769893 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [256/3951 (6%)]\tLoss: 3.795740 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [384/3951 (10%)]\tLoss: 3.741499 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [512/3951 (13%)]\tLoss: 3.775622 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [640/3951 (16%)]\tLoss: 3.615960 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [768/3951 (19%)]\tLoss: 3.704299 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [896/3951 (23%)]\tLoss: 3.755542 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1024/3951 (26%)]\tLoss: 3.747556 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1152/3951 (29%)]\tLoss: 3.709935 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1280/3951 (32%)]\tLoss: 3.747083 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1408/3951 (35%)]\tLoss: 3.727083 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1536/3951 (39%)]\tLoss: 3.719071 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1664/3951 (42%)]\tLoss: 3.721766 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1792/3951 (45%)]\tLoss: 3.837743 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [1920/3951 (48%)]\tLoss: 3.751475 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2048/3951 (52%)]\tLoss: 3.759352 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2176/3951 (55%)]\tLoss: 3.827946 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2304/3951 (58%)]\tLoss: 3.781770 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2432/3951 (61%)]\tLoss: 3.760482 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2560/3951 (65%)]\tLoss: 3.763983 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2688/3951 (68%)]\tLoss: 3.662158 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2816/3951 (71%)]\tLoss: 3.803622 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [2944/3951 (74%)]\tLoss: 3.771504 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3072/3951 (77%)]\tLoss: 3.722558 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3200/3951 (81%)]\tLoss: 3.734214 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3328/3951 (84%)]\tLoss: 3.686320 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3456/3951 (87%)]\tLoss: 3.696939 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3584/3951 (90%)]\tLoss: 3.689427 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3712/3951 (94%)]\tLoss: 3.697226 Lr: 1.7604059656000313e-05\n",
            "Train Epoch: 37 [3330/3951 (97%)]\tLoss: 3.762139 Lr: 1.7604059656000313e-05\n",
            "\n",
            "Test set: Average loss: 0.0367, Accuracy: 60/406 (15%)\n",
            "\n",
            "Train Epoch: 38 [0/3951 (0%)]\tLoss: 3.745163 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [128/3951 (3%)]\tLoss: 3.791074 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [256/3951 (6%)]\tLoss: 3.761020 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [384/3951 (10%)]\tLoss: 3.719354 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [512/3951 (13%)]\tLoss: 3.733617 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [640/3951 (16%)]\tLoss: 3.788283 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [768/3951 (19%)]\tLoss: 3.703243 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [896/3951 (23%)]\tLoss: 3.792546 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1024/3951 (26%)]\tLoss: 3.721540 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1152/3951 (29%)]\tLoss: 3.743994 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1280/3951 (32%)]\tLoss: 3.731552 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1408/3951 (35%)]\tLoss: 3.698801 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1536/3951 (39%)]\tLoss: 3.753318 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1664/3951 (42%)]\tLoss: 3.750953 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1792/3951 (45%)]\tLoss: 3.801238 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [1920/3951 (48%)]\tLoss: 3.766429 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2048/3951 (52%)]\tLoss: 3.639225 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2176/3951 (55%)]\tLoss: 3.685203 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2304/3951 (58%)]\tLoss: 3.738230 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2432/3951 (61%)]\tLoss: 3.767091 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2560/3951 (65%)]\tLoss: 3.810285 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2688/3951 (68%)]\tLoss: 3.766775 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [2816/3951 (71%)]\tLoss: 3.733298 Lr: 1.7343225094356857e-05\n",
            "\n",
            "Test set: Average loss: 0.0366, Accuracy: 60/406 (15%)\n",
            "\n",
            "Train Epoch: 38 [2944/3951 (74%)]\tLoss: 3.629885 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3072/3951 (77%)]\tLoss: 3.658467 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3200/3951 (81%)]\tLoss: 3.646425 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3328/3951 (84%)]\tLoss: 3.718247 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3456/3951 (87%)]\tLoss: 3.658798 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3584/3951 (90%)]\tLoss: 3.757154 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3712/3951 (94%)]\tLoss: 3.673098 Lr: 1.7343225094356857e-05\n",
            "Train Epoch: 38 [3330/3951 (97%)]\tLoss: 3.740258 Lr: 1.7343225094356857e-05\n",
            "\n",
            "Test set: Average loss: 0.0366, Accuracy: 60/406 (15%)\n",
            "\n",
            "Train Epoch: 39 [0/3951 (0%)]\tLoss: 3.761324 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [128/3951 (3%)]\tLoss: 3.748457 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [256/3951 (6%)]\tLoss: 3.708521 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [384/3951 (10%)]\tLoss: 3.702729 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [512/3951 (13%)]\tLoss: 3.665905 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [640/3951 (16%)]\tLoss: 3.787960 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [768/3951 (19%)]\tLoss: 3.673789 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [896/3951 (23%)]\tLoss: 3.735447 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1024/3951 (26%)]\tLoss: 3.721392 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1152/3951 (29%)]\tLoss: 3.754820 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1280/3951 (32%)]\tLoss: 3.655911 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1408/3951 (35%)]\tLoss: 3.704895 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1536/3951 (39%)]\tLoss: 3.636310 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1664/3951 (42%)]\tLoss: 3.802676 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1792/3951 (45%)]\tLoss: 3.744078 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [1920/3951 (48%)]\tLoss: 3.691376 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2048/3951 (52%)]\tLoss: 3.736813 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2176/3951 (55%)]\tLoss: 3.653550 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2304/3951 (58%)]\tLoss: 3.744308 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2432/3951 (61%)]\tLoss: 3.674762 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2560/3951 (65%)]\tLoss: 3.748755 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2688/3951 (68%)]\tLoss: 3.700153 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2816/3951 (71%)]\tLoss: 3.867889 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [2944/3951 (74%)]\tLoss: 3.680015 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3072/3951 (77%)]\tLoss: 3.708004 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3200/3951 (81%)]\tLoss: 3.758561 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3328/3951 (84%)]\tLoss: 3.615698 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3456/3951 (87%)]\tLoss: 3.664516 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3584/3951 (90%)]\tLoss: 3.719096 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3712/3951 (94%)]\tLoss: 3.700628 Lr: 1.7071067811865477e-05\n",
            "Train Epoch: 39 [3330/3951 (97%)]\tLoss: 3.733777 Lr: 1.7071067811865477e-05\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 40 [0/3951 (0%)]\tLoss: 3.814843 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [128/3951 (3%)]\tLoss: 3.659567 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [256/3951 (6%)]\tLoss: 3.677793 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [384/3951 (10%)]\tLoss: 3.676779 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [512/3951 (13%)]\tLoss: 3.709713 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [640/3951 (16%)]\tLoss: 3.742798 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [768/3951 (19%)]\tLoss: 3.839454 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [896/3951 (23%)]\tLoss: 3.629864 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1024/3951 (26%)]\tLoss: 3.673829 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1152/3951 (29%)]\tLoss: 3.729102 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1280/3951 (32%)]\tLoss: 3.718574 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1408/3951 (35%)]\tLoss: 3.672860 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1536/3951 (39%)]\tLoss: 3.737020 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1664/3951 (42%)]\tLoss: 3.737599 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1792/3951 (45%)]\tLoss: 3.790271 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [1920/3951 (48%)]\tLoss: 3.650701 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2048/3951 (52%)]\tLoss: 3.604064 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2176/3951 (55%)]\tLoss: 3.778117 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2304/3951 (58%)]\tLoss: 3.664614 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2432/3951 (61%)]\tLoss: 3.735746 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2560/3951 (65%)]\tLoss: 3.599679 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2688/3951 (68%)]\tLoss: 3.754482 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2816/3951 (71%)]\tLoss: 3.687568 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [2944/3951 (74%)]\tLoss: 3.621491 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3072/3951 (77%)]\tLoss: 3.713175 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3200/3951 (81%)]\tLoss: 3.687528 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3328/3951 (84%)]\tLoss: 3.693741 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3456/3951 (87%)]\tLoss: 3.685339 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3584/3951 (90%)]\tLoss: 3.713246 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3712/3951 (94%)]\tLoss: 3.720661 Lr: 1.678800745532942e-05\n",
            "Train Epoch: 40 [3330/3951 (97%)]\tLoss: 3.714434 Lr: 1.678800745532942e-05\n",
            "\n",
            "Test set: Average loss: 0.0364, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 41 [0/3951 (0%)]\tLoss: 3.671079 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [128/3951 (3%)]\tLoss: 3.696402 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [256/3951 (6%)]\tLoss: 3.714286 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [384/3951 (10%)]\tLoss: 3.754910 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [512/3951 (13%)]\tLoss: 3.707144 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [640/3951 (16%)]\tLoss: 3.590338 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [768/3951 (19%)]\tLoss: 3.602189 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [896/3951 (23%)]\tLoss: 3.686429 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1024/3951 (26%)]\tLoss: 3.831271 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1152/3951 (29%)]\tLoss: 3.670321 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1280/3951 (32%)]\tLoss: 3.718797 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1408/3951 (35%)]\tLoss: 3.650567 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1536/3951 (39%)]\tLoss: 3.757189 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1664/3951 (42%)]\tLoss: 3.623717 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1792/3951 (45%)]\tLoss: 3.708198 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [1920/3951 (48%)]\tLoss: 3.769017 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2048/3951 (52%)]\tLoss: 3.688697 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2176/3951 (55%)]\tLoss: 3.690093 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2304/3951 (58%)]\tLoss: 3.590940 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2432/3951 (61%)]\tLoss: 3.704381 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2560/3951 (65%)]\tLoss: 3.736716 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2688/3951 (68%)]\tLoss: 3.738175 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2816/3951 (71%)]\tLoss: 3.701308 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [2944/3951 (74%)]\tLoss: 3.623944 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [3072/3951 (77%)]\tLoss: 3.787130 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [3200/3951 (81%)]\tLoss: 3.727718 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [3328/3951 (84%)]\tLoss: 3.784924 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [3456/3951 (87%)]\tLoss: 3.729708 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [3584/3951 (90%)]\tLoss: 3.695837 Lr: 1.6494480483301836e-05\n",
            "Train Epoch: 41 [3712/3951 (94%)]\tLoss: 3.790984 Lr: 1.6494480483301836e-05\n",
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 41 [3330/3951 (97%)]\tLoss: 3.693133 Lr: 1.6494480483301836e-05\n",
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 42 [0/3951 (0%)]\tLoss: 3.705024 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [128/3951 (3%)]\tLoss: 3.703426 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [256/3951 (6%)]\tLoss: 3.722981 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [384/3951 (10%)]\tLoss: 3.716996 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [512/3951 (13%)]\tLoss: 3.698224 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [640/3951 (16%)]\tLoss: 3.798617 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [768/3951 (19%)]\tLoss: 3.680137 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [896/3951 (23%)]\tLoss: 3.674993 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1024/3951 (26%)]\tLoss: 3.661213 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1152/3951 (29%)]\tLoss: 3.648561 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1280/3951 (32%)]\tLoss: 3.653342 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1408/3951 (35%)]\tLoss: 3.738451 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1536/3951 (39%)]\tLoss: 3.667385 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1664/3951 (42%)]\tLoss: 3.644090 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1792/3951 (45%)]\tLoss: 3.647755 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [1920/3951 (48%)]\tLoss: 3.737573 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2048/3951 (52%)]\tLoss: 3.645589 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2176/3951 (55%)]\tLoss: 3.813007 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2304/3951 (58%)]\tLoss: 3.657965 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2432/3951 (61%)]\tLoss: 3.616098 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2560/3951 (65%)]\tLoss: 3.742246 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2688/3951 (68%)]\tLoss: 3.670027 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2816/3951 (71%)]\tLoss: 3.632602 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [2944/3951 (74%)]\tLoss: 3.714581 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3072/3951 (77%)]\tLoss: 3.775974 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3200/3951 (81%)]\tLoss: 3.685764 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3328/3951 (84%)]\tLoss: 3.741394 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3456/3951 (87%)]\tLoss: 3.763843 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3584/3951 (90%)]\tLoss: 3.657114 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3712/3951 (94%)]\tLoss: 3.753419 Lr: 1.6190939493098344e-05\n",
            "Train Epoch: 42 [3330/3951 (97%)]\tLoss: 3.644044 Lr: 1.6190939493098344e-05\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 43 [0/3951 (0%)]\tLoss: 3.746673 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [128/3951 (3%)]\tLoss: 3.769856 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [256/3951 (6%)]\tLoss: 3.685986 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [384/3951 (10%)]\tLoss: 3.592716 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [512/3951 (13%)]\tLoss: 3.683455 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [640/3951 (16%)]\tLoss: 3.691355 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [768/3951 (19%)]\tLoss: 3.747477 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [896/3951 (23%)]\tLoss: 3.739008 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1024/3951 (26%)]\tLoss: 3.700568 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1152/3951 (29%)]\tLoss: 3.664818 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1280/3951 (32%)]\tLoss: 3.684066 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1408/3951 (35%)]\tLoss: 3.696990 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1536/3951 (39%)]\tLoss: 3.669714 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1664/3951 (42%)]\tLoss: 3.694746 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1792/3951 (45%)]\tLoss: 3.592428 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [1920/3951 (48%)]\tLoss: 3.674612 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2048/3951 (52%)]\tLoss: 3.671648 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2176/3951 (55%)]\tLoss: 3.785255 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2304/3951 (58%)]\tLoss: 3.602418 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2432/3951 (61%)]\tLoss: 3.615765 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2560/3951 (65%)]\tLoss: 3.622543 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2688/3951 (68%)]\tLoss: 3.556983 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2816/3951 (71%)]\tLoss: 3.708129 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [2944/3951 (74%)]\tLoss: 3.686183 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3072/3951 (77%)]\tLoss: 3.648410 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3200/3951 (81%)]\tLoss: 3.670004 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3328/3951 (84%)]\tLoss: 3.667426 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3456/3951 (87%)]\tLoss: 3.701290 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3584/3951 (90%)]\tLoss: 3.580560 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3712/3951 (94%)]\tLoss: 3.627718 Lr: 1.5877852522924733e-05\n",
            "Train Epoch: 43 [3330/3951 (97%)]\tLoss: 3.726997 Lr: 1.5877852522924733e-05\n",
            "\n",
            "Test set: Average loss: 0.0361, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 44 [0/3951 (0%)]\tLoss: 3.814589 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [128/3951 (3%)]\tLoss: 3.736248 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [256/3951 (6%)]\tLoss: 3.657592 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [384/3951 (10%)]\tLoss: 3.601337 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [512/3951 (13%)]\tLoss: 3.664484 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [640/3951 (16%)]\tLoss: 3.722128 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [768/3951 (19%)]\tLoss: 3.710037 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [896/3951 (23%)]\tLoss: 3.764785 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1024/3951 (26%)]\tLoss: 3.720628 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1152/3951 (29%)]\tLoss: 3.661611 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1280/3951 (32%)]\tLoss: 3.626759 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1408/3951 (35%)]\tLoss: 3.648762 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1536/3951 (39%)]\tLoss: 3.621594 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1664/3951 (42%)]\tLoss: 3.694886 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1792/3951 (45%)]\tLoss: 3.712636 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [1920/3951 (48%)]\tLoss: 3.693392 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2048/3951 (52%)]\tLoss: 3.672378 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2176/3951 (55%)]\tLoss: 3.603947 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2304/3951 (58%)]\tLoss: 3.708468 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2432/3951 (61%)]\tLoss: 3.555623 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2560/3951 (65%)]\tLoss: 3.703452 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2688/3951 (68%)]\tLoss: 3.734495 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2816/3951 (71%)]\tLoss: 3.767955 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [2944/3951 (74%)]\tLoss: 3.622306 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3072/3951 (77%)]\tLoss: 3.702258 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3200/3951 (81%)]\tLoss: 3.598088 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3328/3951 (84%)]\tLoss: 3.615911 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3456/3951 (87%)]\tLoss: 3.671520 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3584/3951 (90%)]\tLoss: 3.689864 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3712/3951 (94%)]\tLoss: 3.685077 Lr: 1.5555702330196024e-05\n",
            "Train Epoch: 44 [3330/3951 (97%)]\tLoss: 3.563786 Lr: 1.5555702330196024e-05\n",
            "\n",
            "Test set: Average loss: 0.0361, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 45 [0/3951 (0%)]\tLoss: 3.733298 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [128/3951 (3%)]\tLoss: 3.696649 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [256/3951 (6%)]\tLoss: 3.611778 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [384/3951 (10%)]\tLoss: 3.760101 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [512/3951 (13%)]\tLoss: 3.658877 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [640/3951 (16%)]\tLoss: 3.671657 Lr: 1.5224985647159489e-05\n",
            "\n",
            "Test set: Average loss: 0.0360, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 45 [768/3951 (19%)]\tLoss: 3.539408 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [896/3951 (23%)]\tLoss: 3.669225 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1024/3951 (26%)]\tLoss: 3.578034 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1152/3951 (29%)]\tLoss: 3.579449 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1280/3951 (32%)]\tLoss: 3.639146 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1408/3951 (35%)]\tLoss: 3.706537 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1536/3951 (39%)]\tLoss: 3.591601 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1664/3951 (42%)]\tLoss: 3.717972 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1792/3951 (45%)]\tLoss: 3.725942 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [1920/3951 (48%)]\tLoss: 3.732127 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2048/3951 (52%)]\tLoss: 3.541920 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2176/3951 (55%)]\tLoss: 3.695715 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2304/3951 (58%)]\tLoss: 4.007669 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2432/3951 (61%)]\tLoss: 3.674397 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2560/3951 (65%)]\tLoss: 3.754725 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2688/3951 (68%)]\tLoss: 3.729234 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2816/3951 (71%)]\tLoss: 3.786115 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [2944/3951 (74%)]\tLoss: 3.551654 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3072/3951 (77%)]\tLoss: 3.693181 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3200/3951 (81%)]\tLoss: 3.643442 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3328/3951 (84%)]\tLoss: 3.677345 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3456/3951 (87%)]\tLoss: 3.663783 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3584/3951 (90%)]\tLoss: 3.732858 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3712/3951 (94%)]\tLoss: 3.676155 Lr: 1.5224985647159489e-05\n",
            "Train Epoch: 45 [3330/3951 (97%)]\tLoss: 3.633306 Lr: 1.5224985647159489e-05\n",
            "\n",
            "Test set: Average loss: 0.0360, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 46 [0/3951 (0%)]\tLoss: 3.639920 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [128/3951 (3%)]\tLoss: 3.704443 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [256/3951 (6%)]\tLoss: 3.591026 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [384/3951 (10%)]\tLoss: 3.637846 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [512/3951 (13%)]\tLoss: 3.698988 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [640/3951 (16%)]\tLoss: 3.669178 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [768/3951 (19%)]\tLoss: 3.731416 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [896/3951 (23%)]\tLoss: 3.787174 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1024/3951 (26%)]\tLoss: 3.702279 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1152/3951 (29%)]\tLoss: 3.702708 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1280/3951 (32%)]\tLoss: 3.639801 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1408/3951 (35%)]\tLoss: 3.732380 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1536/3951 (39%)]\tLoss: 3.642791 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1664/3951 (42%)]\tLoss: 3.603486 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1792/3951 (45%)]\tLoss: 3.630161 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [1920/3951 (48%)]\tLoss: 3.672681 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2048/3951 (52%)]\tLoss: 3.639499 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2176/3951 (55%)]\tLoss: 3.733694 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2304/3951 (58%)]\tLoss: 3.621438 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2432/3951 (61%)]\tLoss: 3.623226 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2560/3951 (65%)]\tLoss: 3.625703 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2688/3951 (68%)]\tLoss: 3.676142 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2816/3951 (71%)]\tLoss: 3.624002 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [2944/3951 (74%)]\tLoss: 3.729613 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3072/3951 (77%)]\tLoss: 3.662611 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3200/3951 (81%)]\tLoss: 3.670467 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3328/3951 (84%)]\tLoss: 3.547711 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3456/3951 (87%)]\tLoss: 3.636582 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3584/3951 (90%)]\tLoss: 3.685347 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3712/3951 (94%)]\tLoss: 3.690764 Lr: 1.4886212414969551e-05\n",
            "Train Epoch: 46 [3330/3951 (97%)]\tLoss: 3.755606 Lr: 1.4886212414969551e-05\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 47 [0/3951 (0%)]\tLoss: 3.679745 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [128/3951 (3%)]\tLoss: 3.501030 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [256/3951 (6%)]\tLoss: 3.752092 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [384/3951 (10%)]\tLoss: 3.625531 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [512/3951 (13%)]\tLoss: 3.780312 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [640/3951 (16%)]\tLoss: 3.655272 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [768/3951 (19%)]\tLoss: 3.539060 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [896/3951 (23%)]\tLoss: 3.704554 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1024/3951 (26%)]\tLoss: 3.603969 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1152/3951 (29%)]\tLoss: 3.636655 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1280/3951 (32%)]\tLoss: 3.669180 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1408/3951 (35%)]\tLoss: 3.691936 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1536/3951 (39%)]\tLoss: 3.644381 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1664/3951 (42%)]\tLoss: 3.690641 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1792/3951 (45%)]\tLoss: 3.695240 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [1920/3951 (48%)]\tLoss: 3.577309 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2048/3951 (52%)]\tLoss: 3.720168 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2176/3951 (55%)]\tLoss: 3.780721 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2304/3951 (58%)]\tLoss: 3.663332 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2432/3951 (61%)]\tLoss: 3.680954 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2560/3951 (65%)]\tLoss: 3.653962 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2688/3951 (68%)]\tLoss: 3.675960 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2816/3951 (71%)]\tLoss: 3.589224 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [2944/3951 (74%)]\tLoss: 3.643336 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3072/3951 (77%)]\tLoss: 3.708123 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3200/3951 (81%)]\tLoss: 3.721535 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3328/3951 (84%)]\tLoss: 3.610081 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3456/3951 (87%)]\tLoss: 3.653627 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3584/3951 (90%)]\tLoss: 3.569968 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3712/3951 (94%)]\tLoss: 3.626647 Lr: 1.4539904997395468e-05\n",
            "Train Epoch: 47 [3330/3951 (97%)]\tLoss: 3.734161 Lr: 1.4539904997395468e-05\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 48 [0/3951 (0%)]\tLoss: 3.624746 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [128/3951 (3%)]\tLoss: 3.654356 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [256/3951 (6%)]\tLoss: 3.779415 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [384/3951 (10%)]\tLoss: 3.696184 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [512/3951 (13%)]\tLoss: 3.634275 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [640/3951 (16%)]\tLoss: 3.575725 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [768/3951 (19%)]\tLoss: 3.625037 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [896/3951 (23%)]\tLoss: 3.563368 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1024/3951 (26%)]\tLoss: 3.649885 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1152/3951 (29%)]\tLoss: 3.586811 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1280/3951 (32%)]\tLoss: 3.601263 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1408/3951 (35%)]\tLoss: 3.690145 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1536/3951 (39%)]\tLoss: 3.637597 Lr: 1.4186597375374283e-05\n",
            "\n",
            "Test set: Average loss: 0.0358, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 48 [1664/3951 (42%)]\tLoss: 3.543677 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1792/3951 (45%)]\tLoss: 3.655793 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [1920/3951 (48%)]\tLoss: 3.748902 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2048/3951 (52%)]\tLoss: 3.709255 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2176/3951 (55%)]\tLoss: 3.648836 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2304/3951 (58%)]\tLoss: 3.666124 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2432/3951 (61%)]\tLoss: 3.672611 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2560/3951 (65%)]\tLoss: 3.744995 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2688/3951 (68%)]\tLoss: 3.581886 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2816/3951 (71%)]\tLoss: 3.741735 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [2944/3951 (74%)]\tLoss: 3.700730 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3072/3951 (77%)]\tLoss: 3.622736 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3200/3951 (81%)]\tLoss: 3.741580 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3328/3951 (84%)]\tLoss: 3.742992 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3456/3951 (87%)]\tLoss: 3.755685 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3584/3951 (90%)]\tLoss: 3.663265 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3712/3951 (94%)]\tLoss: 3.722743 Lr: 1.4186597375374283e-05\n",
            "Train Epoch: 48 [3330/3951 (97%)]\tLoss: 3.567725 Lr: 1.4186597375374283e-05\n",
            "\n",
            "Test set: Average loss: 0.0358, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 49 [0/3951 (0%)]\tLoss: 3.666990 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [128/3951 (3%)]\tLoss: 3.645402 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [256/3951 (6%)]\tLoss: 3.570532 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [384/3951 (10%)]\tLoss: 3.636558 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [512/3951 (13%)]\tLoss: 3.652636 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [640/3951 (16%)]\tLoss: 3.680721 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [768/3951 (19%)]\tLoss: 3.665898 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [896/3951 (23%)]\tLoss: 3.627777 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1024/3951 (26%)]\tLoss: 3.567583 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1152/3951 (29%)]\tLoss: 3.668847 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1280/3951 (32%)]\tLoss: 3.628969 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1408/3951 (35%)]\tLoss: 3.721434 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1536/3951 (39%)]\tLoss: 3.629648 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1664/3951 (42%)]\tLoss: 3.727570 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1792/3951 (45%)]\tLoss: 3.696222 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [1920/3951 (48%)]\tLoss: 3.748751 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2048/3951 (52%)]\tLoss: 3.516716 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2176/3951 (55%)]\tLoss: 3.682504 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2304/3951 (58%)]\tLoss: 3.725771 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2432/3951 (61%)]\tLoss: 3.647128 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2560/3951 (65%)]\tLoss: 3.675999 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2688/3951 (68%)]\tLoss: 3.640373 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2816/3951 (71%)]\tLoss: 3.684822 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [2944/3951 (74%)]\tLoss: 3.584525 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3072/3951 (77%)]\tLoss: 3.717864 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3200/3951 (81%)]\tLoss: 3.618821 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3328/3951 (84%)]\tLoss: 3.658798 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3456/3951 (87%)]\tLoss: 3.729108 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3584/3951 (90%)]\tLoss: 3.565864 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3712/3951 (94%)]\tLoss: 3.819192 Lr: 1.3826834323650899e-05\n",
            "Train Epoch: 49 [3330/3951 (97%)]\tLoss: 3.629331 Lr: 1.3826834323650899e-05\n",
            "\n",
            "Test set: Average loss: 0.0357, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 50 [0/3951 (0%)]\tLoss: 3.575005 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [128/3951 (3%)]\tLoss: 3.681263 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [256/3951 (6%)]\tLoss: 3.686594 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [384/3951 (10%)]\tLoss: 3.615968 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [512/3951 (13%)]\tLoss: 3.652441 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [640/3951 (16%)]\tLoss: 3.708808 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [768/3951 (19%)]\tLoss: 3.651925 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [896/3951 (23%)]\tLoss: 3.707783 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1024/3951 (26%)]\tLoss: 3.644637 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1152/3951 (29%)]\tLoss: 3.584602 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1280/3951 (32%)]\tLoss: 3.690297 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1408/3951 (35%)]\tLoss: 3.559526 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1536/3951 (39%)]\tLoss: 3.560653 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1664/3951 (42%)]\tLoss: 3.519092 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1792/3951 (45%)]\tLoss: 3.584884 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [1920/3951 (48%)]\tLoss: 3.752908 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2048/3951 (52%)]\tLoss: 3.626832 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2176/3951 (55%)]\tLoss: 3.576462 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2304/3951 (58%)]\tLoss: 3.743575 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2432/3951 (61%)]\tLoss: 3.662104 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2560/3951 (65%)]\tLoss: 3.503948 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2688/3951 (68%)]\tLoss: 3.653012 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2816/3951 (71%)]\tLoss: 3.658382 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [2944/3951 (74%)]\tLoss: 3.619522 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3072/3951 (77%)]\tLoss: 3.561181 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3200/3951 (81%)]\tLoss: 3.594907 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3328/3951 (84%)]\tLoss: 3.757964 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3456/3951 (87%)]\tLoss: 3.692028 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3584/3951 (90%)]\tLoss: 3.663828 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3712/3951 (94%)]\tLoss: 3.582495 Lr: 1.346117057077493e-05\n",
            "Train Epoch: 50 [3330/3951 (97%)]\tLoss: 3.635284 Lr: 1.346117057077493e-05\n",
            "\n",
            "Test set: Average loss: 0.0357, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 51 [0/3951 (0%)]\tLoss: 3.616229 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [128/3951 (3%)]\tLoss: 3.662832 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [256/3951 (6%)]\tLoss: 3.610953 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [384/3951 (10%)]\tLoss: 3.633140 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [512/3951 (13%)]\tLoss: 3.709946 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [640/3951 (16%)]\tLoss: 3.649652 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [768/3951 (19%)]\tLoss: 3.614064 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [896/3951 (23%)]\tLoss: 3.678242 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1024/3951 (26%)]\tLoss: 3.674730 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1152/3951 (29%)]\tLoss: 3.545027 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1280/3951 (32%)]\tLoss: 3.630182 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1408/3951 (35%)]\tLoss: 3.563386 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1536/3951 (39%)]\tLoss: 3.687176 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1664/3951 (42%)]\tLoss: 3.587381 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1792/3951 (45%)]\tLoss: 3.753835 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [1920/3951 (48%)]\tLoss: 3.627160 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2048/3951 (52%)]\tLoss: 3.703810 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2176/3951 (55%)]\tLoss: 3.564139 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2304/3951 (58%)]\tLoss: 3.639024 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2432/3951 (61%)]\tLoss: 3.666190 Lr: 1.3090169943749475e-05\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 51 [2560/3951 (65%)]\tLoss: 3.762234 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2688/3951 (68%)]\tLoss: 3.666133 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2816/3951 (71%)]\tLoss: 3.608822 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [2944/3951 (74%)]\tLoss: 3.659528 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3072/3951 (77%)]\tLoss: 3.708778 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3200/3951 (81%)]\tLoss: 3.720820 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3328/3951 (84%)]\tLoss: 3.514599 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3456/3951 (87%)]\tLoss: 3.682831 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3584/3951 (90%)]\tLoss: 3.665033 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3712/3951 (94%)]\tLoss: 3.719793 Lr: 1.3090169943749475e-05\n",
            "Train Epoch: 51 [3330/3951 (97%)]\tLoss: 3.649975 Lr: 1.3090169943749475e-05\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 52 [0/3951 (0%)]\tLoss: 3.497630 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [128/3951 (3%)]\tLoss: 3.534032 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [256/3951 (6%)]\tLoss: 3.639645 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [384/3951 (10%)]\tLoss: 3.618708 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [512/3951 (13%)]\tLoss: 3.657073 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [640/3951 (16%)]\tLoss: 3.681382 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [768/3951 (19%)]\tLoss: 3.616905 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [896/3951 (23%)]\tLoss: 3.594799 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1024/3951 (26%)]\tLoss: 3.656067 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1152/3951 (29%)]\tLoss: 3.628495 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1280/3951 (32%)]\tLoss: 3.681271 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1408/3951 (35%)]\tLoss: 3.612419 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1536/3951 (39%)]\tLoss: 3.626027 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1664/3951 (42%)]\tLoss: 3.662964 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1792/3951 (45%)]\tLoss: 3.629954 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [1920/3951 (48%)]\tLoss: 3.785804 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2048/3951 (52%)]\tLoss: 3.663637 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2176/3951 (55%)]\tLoss: 3.683178 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2304/3951 (58%)]\tLoss: 3.674774 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2432/3951 (61%)]\tLoss: 3.609992 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2560/3951 (65%)]\tLoss: 3.504523 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2688/3951 (68%)]\tLoss: 3.759310 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2816/3951 (71%)]\tLoss: 3.646631 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [2944/3951 (74%)]\tLoss: 3.676004 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3072/3951 (77%)]\tLoss: 3.613419 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3200/3951 (81%)]\tLoss: 3.651195 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3328/3951 (84%)]\tLoss: 3.638641 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3456/3951 (87%)]\tLoss: 3.452274 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3584/3951 (90%)]\tLoss: 3.600275 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3712/3951 (94%)]\tLoss: 3.596275 Lr: 1.2714404498650743e-05\n",
            "Train Epoch: 52 [3330/3951 (97%)]\tLoss: 3.573549 Lr: 1.2714404498650743e-05\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 61/406 (15%)\n",
            "\n",
            "Train Epoch: 53 [0/3951 (0%)]\tLoss: 3.479921 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [128/3951 (3%)]\tLoss: 3.595821 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [256/3951 (6%)]\tLoss: 3.555498 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [384/3951 (10%)]\tLoss: 3.633175 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [512/3951 (13%)]\tLoss: 3.654422 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [640/3951 (16%)]\tLoss: 3.662064 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [768/3951 (19%)]\tLoss: 3.545059 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [896/3951 (23%)]\tLoss: 3.710007 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1024/3951 (26%)]\tLoss: 3.694082 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1152/3951 (29%)]\tLoss: 3.727389 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1280/3951 (32%)]\tLoss: 3.630156 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1408/3951 (35%)]\tLoss: 3.696034 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1536/3951 (39%)]\tLoss: 3.704550 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1664/3951 (42%)]\tLoss: 3.582605 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1792/3951 (45%)]\tLoss: 3.598083 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [1920/3951 (48%)]\tLoss: 3.572508 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2048/3951 (52%)]\tLoss: 3.597349 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2176/3951 (55%)]\tLoss: 3.648677 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2304/3951 (58%)]\tLoss: 3.586563 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2432/3951 (61%)]\tLoss: 3.519411 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2560/3951 (65%)]\tLoss: 3.608202 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2688/3951 (68%)]\tLoss: 3.679114 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2816/3951 (71%)]\tLoss: 3.668076 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [2944/3951 (74%)]\tLoss: 3.598157 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3072/3951 (77%)]\tLoss: 3.586975 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3200/3951 (81%)]\tLoss: 3.605643 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3328/3951 (84%)]\tLoss: 3.755540 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3456/3951 (87%)]\tLoss: 3.610347 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3584/3951 (90%)]\tLoss: 3.612846 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3712/3951 (94%)]\tLoss: 3.548603 Lr: 1.2334453638559057e-05\n",
            "Train Epoch: 53 [3330/3951 (97%)]\tLoss: 3.750251 Lr: 1.2334453638559057e-05\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 54 [0/3951 (0%)]\tLoss: 3.620564 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [128/3951 (3%)]\tLoss: 3.553048 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [256/3951 (6%)]\tLoss: 3.625557 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [384/3951 (10%)]\tLoss: 3.667981 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [512/3951 (13%)]\tLoss: 3.650426 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [640/3951 (16%)]\tLoss: 3.723541 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [768/3951 (19%)]\tLoss: 3.546627 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [896/3951 (23%)]\tLoss: 3.796243 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1024/3951 (26%)]\tLoss: 3.590427 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1152/3951 (29%)]\tLoss: 3.656945 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1280/3951 (32%)]\tLoss: 3.622485 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1408/3951 (35%)]\tLoss: 3.610823 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1536/3951 (39%)]\tLoss: 3.696092 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1664/3951 (42%)]\tLoss: 3.681067 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1792/3951 (45%)]\tLoss: 3.641250 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [1920/3951 (48%)]\tLoss: 3.509164 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2048/3951 (52%)]\tLoss: 3.664713 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2176/3951 (55%)]\tLoss: 3.554196 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2304/3951 (58%)]\tLoss: 3.623412 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2432/3951 (61%)]\tLoss: 3.578257 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2560/3951 (65%)]\tLoss: 3.583616 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2688/3951 (68%)]\tLoss: 3.621292 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2816/3951 (71%)]\tLoss: 3.666603 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [2944/3951 (74%)]\tLoss: 3.605606 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [3072/3951 (77%)]\tLoss: 3.665226 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [3200/3951 (81%)]\tLoss: 3.602849 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [3328/3951 (84%)]\tLoss: 3.616277 Lr: 1.1950903220161286e-05\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 54 [3456/3951 (87%)]\tLoss: 3.578860 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [3584/3951 (90%)]\tLoss: 3.644012 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [3712/3951 (94%)]\tLoss: 3.552717 Lr: 1.1950903220161286e-05\n",
            "Train Epoch: 54 [3330/3951 (97%)]\tLoss: 3.623080 Lr: 1.1950903220161286e-05\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 55 [0/3951 (0%)]\tLoss: 3.623015 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [128/3951 (3%)]\tLoss: 3.640079 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [256/3951 (6%)]\tLoss: 3.677794 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [384/3951 (10%)]\tLoss: 3.688706 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [512/3951 (13%)]\tLoss: 3.650823 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [640/3951 (16%)]\tLoss: 3.626933 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [768/3951 (19%)]\tLoss: 3.496924 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [896/3951 (23%)]\tLoss: 3.502530 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1024/3951 (26%)]\tLoss: 3.591885 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1152/3951 (29%)]\tLoss: 3.544957 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1280/3951 (32%)]\tLoss: 3.673536 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1408/3951 (35%)]\tLoss: 3.567592 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1536/3951 (39%)]\tLoss: 3.594954 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1664/3951 (42%)]\tLoss: 3.618659 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1792/3951 (45%)]\tLoss: 3.716331 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [1920/3951 (48%)]\tLoss: 3.486903 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2048/3951 (52%)]\tLoss: 3.541923 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2176/3951 (55%)]\tLoss: 3.666233 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2304/3951 (58%)]\tLoss: 3.711068 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2432/3951 (61%)]\tLoss: 3.557269 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2560/3951 (65%)]\tLoss: 3.640807 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2688/3951 (68%)]\tLoss: 3.700535 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2816/3951 (71%)]\tLoss: 3.617037 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [2944/3951 (74%)]\tLoss: 3.531662 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3072/3951 (77%)]\tLoss: 3.691765 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3200/3951 (81%)]\tLoss: 3.617906 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3328/3951 (84%)]\tLoss: 3.685153 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3456/3951 (87%)]\tLoss: 3.692717 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3584/3951 (90%)]\tLoss: 3.659743 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3712/3951 (94%)]\tLoss: 3.637724 Lr: 1.156434465040231e-05\n",
            "Train Epoch: 55 [3330/3951 (97%)]\tLoss: 3.603609 Lr: 1.156434465040231e-05\n",
            "\n",
            "Test set: Average loss: 0.0354, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 56 [0/3951 (0%)]\tLoss: 3.693885 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [128/3951 (3%)]\tLoss: 3.697316 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [256/3951 (6%)]\tLoss: 3.670610 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [384/3951 (10%)]\tLoss: 3.514818 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [512/3951 (13%)]\tLoss: 3.551135 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [640/3951 (16%)]\tLoss: 3.583038 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [768/3951 (19%)]\tLoss: 3.640772 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [896/3951 (23%)]\tLoss: 3.637017 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1024/3951 (26%)]\tLoss: 3.591000 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1152/3951 (29%)]\tLoss: 3.619244 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1280/3951 (32%)]\tLoss: 3.510380 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1408/3951 (35%)]\tLoss: 3.570816 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1536/3951 (39%)]\tLoss: 3.521940 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1664/3951 (42%)]\tLoss: 3.516309 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1792/3951 (45%)]\tLoss: 3.590214 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [1920/3951 (48%)]\tLoss: 3.581458 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2048/3951 (52%)]\tLoss: 3.585167 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2176/3951 (55%)]\tLoss: 3.600167 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2304/3951 (58%)]\tLoss: 3.697741 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2432/3951 (61%)]\tLoss: 3.601874 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2560/3951 (65%)]\tLoss: 3.537541 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2688/3951 (68%)]\tLoss: 3.633871 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2816/3951 (71%)]\tLoss: 3.636017 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [2944/3951 (74%)]\tLoss: 3.654295 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3072/3951 (77%)]\tLoss: 3.651194 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3200/3951 (81%)]\tLoss: 3.639586 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3328/3951 (84%)]\tLoss: 3.675605 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3456/3951 (87%)]\tLoss: 3.545485 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3584/3951 (90%)]\tLoss: 3.735112 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3712/3951 (94%)]\tLoss: 3.676681 Lr: 1.1175373974578378e-05\n",
            "Train Epoch: 56 [3330/3951 (97%)]\tLoss: 3.642782 Lr: 1.1175373974578378e-05\n",
            "\n",
            "Test set: Average loss: 0.0354, Accuracy: 62/406 (15%)\n",
            "\n",
            "Train Epoch: 57 [0/3951 (0%)]\tLoss: 3.639073 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [128/3951 (3%)]\tLoss: 3.601702 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [256/3951 (6%)]\tLoss: 3.686356 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [384/3951 (10%)]\tLoss: 3.613520 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [512/3951 (13%)]\tLoss: 3.646302 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [640/3951 (16%)]\tLoss: 3.595932 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [768/3951 (19%)]\tLoss: 3.574821 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [896/3951 (23%)]\tLoss: 3.668758 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1024/3951 (26%)]\tLoss: 3.582941 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1152/3951 (29%)]\tLoss: 3.608269 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1280/3951 (32%)]\tLoss: 3.626231 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1408/3951 (35%)]\tLoss: 3.563753 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1536/3951 (39%)]\tLoss: 3.515538 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1664/3951 (42%)]\tLoss: 3.679030 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1792/3951 (45%)]\tLoss: 3.572500 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [1920/3951 (48%)]\tLoss: 3.615543 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2048/3951 (52%)]\tLoss: 3.597382 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2176/3951 (55%)]\tLoss: 3.720923 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2304/3951 (58%)]\tLoss: 3.714007 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2432/3951 (61%)]\tLoss: 3.617412 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2560/3951 (65%)]\tLoss: 3.572719 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2688/3951 (68%)]\tLoss: 3.574844 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2816/3951 (71%)]\tLoss: 3.632142 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [2944/3951 (74%)]\tLoss: 3.730956 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3072/3951 (77%)]\tLoss: 3.627759 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3200/3951 (81%)]\tLoss: 3.643868 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3328/3951 (84%)]\tLoss: 3.689379 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3456/3951 (87%)]\tLoss: 3.538694 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3584/3951 (90%)]\tLoss: 3.611789 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3712/3951 (94%)]\tLoss: 3.637892 Lr: 1.0784590957278452e-05\n",
            "Train Epoch: 57 [3330/3951 (97%)]\tLoss: 3.515250 Lr: 1.0784590957278452e-05\n",
            "\n",
            "Test set: Average loss: 0.0354, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 58 [0/3951 (0%)]\tLoss: 3.626895 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [128/3951 (3%)]\tLoss: 3.600424 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [256/3951 (6%)]\tLoss: 3.562121 Lr: 1.0392598157590687e-05\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 58 [384/3951 (10%)]\tLoss: 3.601247 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [512/3951 (13%)]\tLoss: 3.546972 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [640/3951 (16%)]\tLoss: 3.547429 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [768/3951 (19%)]\tLoss: 3.611954 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [896/3951 (23%)]\tLoss: 3.546743 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1024/3951 (26%)]\tLoss: 3.602432 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1152/3951 (29%)]\tLoss: 3.523383 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1280/3951 (32%)]\tLoss: 3.614162 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1408/3951 (35%)]\tLoss: 3.606755 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1536/3951 (39%)]\tLoss: 3.585085 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1664/3951 (42%)]\tLoss: 3.575832 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1792/3951 (45%)]\tLoss: 3.573853 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [1920/3951 (48%)]\tLoss: 3.655251 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2048/3951 (52%)]\tLoss: 3.610175 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2176/3951 (55%)]\tLoss: 3.621902 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2304/3951 (58%)]\tLoss: 3.622767 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2432/3951 (61%)]\tLoss: 3.726640 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2560/3951 (65%)]\tLoss: 3.531766 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2688/3951 (68%)]\tLoss: 3.588542 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2816/3951 (71%)]\tLoss: 3.727206 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [2944/3951 (74%)]\tLoss: 3.535789 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3072/3951 (77%)]\tLoss: 3.601842 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3200/3951 (81%)]\tLoss: 3.566570 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3328/3951 (84%)]\tLoss: 3.591938 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3456/3951 (87%)]\tLoss: 3.624929 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3584/3951 (90%)]\tLoss: 3.602489 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3712/3951 (94%)]\tLoss: 3.529406 Lr: 1.0392598157590687e-05\n",
            "Train Epoch: 58 [3330/3951 (97%)]\tLoss: 3.607101 Lr: 1.0392598157590687e-05\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 59 [0/3951 (0%)]\tLoss: 3.661669 Lr: 1e-05\n",
            "Train Epoch: 59 [128/3951 (3%)]\tLoss: 3.646003 Lr: 1e-05\n",
            "Train Epoch: 59 [256/3951 (6%)]\tLoss: 3.645941 Lr: 1e-05\n",
            "Train Epoch: 59 [384/3951 (10%)]\tLoss: 3.699173 Lr: 1e-05\n",
            "Train Epoch: 59 [512/3951 (13%)]\tLoss: 3.590589 Lr: 1e-05\n",
            "Train Epoch: 59 [640/3951 (16%)]\tLoss: 3.562541 Lr: 1e-05\n",
            "Train Epoch: 59 [768/3951 (19%)]\tLoss: 3.599894 Lr: 1e-05\n",
            "Train Epoch: 59 [896/3951 (23%)]\tLoss: 3.654250 Lr: 1e-05\n",
            "Train Epoch: 59 [1024/3951 (26%)]\tLoss: 3.513759 Lr: 1e-05\n",
            "Train Epoch: 59 [1152/3951 (29%)]\tLoss: 3.537713 Lr: 1e-05\n",
            "Train Epoch: 59 [1280/3951 (32%)]\tLoss: 3.692306 Lr: 1e-05\n",
            "Train Epoch: 59 [1408/3951 (35%)]\tLoss: 3.577819 Lr: 1e-05\n",
            "Train Epoch: 59 [1536/3951 (39%)]\tLoss: 3.693871 Lr: 1e-05\n",
            "Train Epoch: 59 [1664/3951 (42%)]\tLoss: 3.462261 Lr: 1e-05\n",
            "Train Epoch: 59 [1792/3951 (45%)]\tLoss: 3.484991 Lr: 1e-05\n",
            "Train Epoch: 59 [1920/3951 (48%)]\tLoss: 3.650699 Lr: 1e-05\n",
            "Train Epoch: 59 [2048/3951 (52%)]\tLoss: 3.643695 Lr: 1e-05\n",
            "Train Epoch: 59 [2176/3951 (55%)]\tLoss: 3.710737 Lr: 1e-05\n",
            "Train Epoch: 59 [2304/3951 (58%)]\tLoss: 3.598896 Lr: 1e-05\n",
            "Train Epoch: 59 [2432/3951 (61%)]\tLoss: 3.512630 Lr: 1e-05\n",
            "Train Epoch: 59 [2560/3951 (65%)]\tLoss: 3.576965 Lr: 1e-05\n",
            "Train Epoch: 59 [2688/3951 (68%)]\tLoss: 3.535727 Lr: 1e-05\n",
            "Train Epoch: 59 [2816/3951 (71%)]\tLoss: 3.609684 Lr: 1e-05\n",
            "Train Epoch: 59 [2944/3951 (74%)]\tLoss: 3.533978 Lr: 1e-05\n",
            "Train Epoch: 59 [3072/3951 (77%)]\tLoss: 3.546416 Lr: 1e-05\n",
            "Train Epoch: 59 [3200/3951 (81%)]\tLoss: 3.725041 Lr: 1e-05\n",
            "Train Epoch: 59 [3328/3951 (84%)]\tLoss: 3.598868 Lr: 1e-05\n",
            "Train Epoch: 59 [3456/3951 (87%)]\tLoss: 3.627641 Lr: 1e-05\n",
            "Train Epoch: 59 [3584/3951 (90%)]\tLoss: 3.573321 Lr: 1e-05\n",
            "Train Epoch: 59 [3712/3951 (94%)]\tLoss: 3.704142 Lr: 1e-05\n",
            "Train Epoch: 59 [3330/3951 (97%)]\tLoss: 3.628107 Lr: 1e-05\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 63/406 (16%)\n",
            "\n",
            "Train Epoch: 60 [0/3951 (0%)]\tLoss: 3.616349 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [128/3951 (3%)]\tLoss: 3.535892 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [256/3951 (6%)]\tLoss: 3.577488 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [384/3951 (10%)]\tLoss: 3.631453 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [512/3951 (13%)]\tLoss: 3.573913 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [640/3951 (16%)]\tLoss: 3.647403 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [768/3951 (19%)]\tLoss: 3.668347 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [896/3951 (23%)]\tLoss: 3.533744 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1024/3951 (26%)]\tLoss: 3.509492 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1152/3951 (29%)]\tLoss: 3.590619 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1280/3951 (32%)]\tLoss: 3.708457 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1408/3951 (35%)]\tLoss: 3.485841 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1536/3951 (39%)]\tLoss: 3.673289 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1664/3951 (42%)]\tLoss: 3.575849 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1792/3951 (45%)]\tLoss: 3.710371 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [1920/3951 (48%)]\tLoss: 3.650036 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2048/3951 (52%)]\tLoss: 3.555071 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2176/3951 (55%)]\tLoss: 3.599671 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2304/3951 (58%)]\tLoss: 3.600546 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2432/3951 (61%)]\tLoss: 3.520800 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2560/3951 (65%)]\tLoss: 3.622469 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2688/3951 (68%)]\tLoss: 3.546802 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2816/3951 (71%)]\tLoss: 3.646079 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [2944/3951 (74%)]\tLoss: 3.603681 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3072/3951 (77%)]\tLoss: 3.573888 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3200/3951 (81%)]\tLoss: 3.612291 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3328/3951 (84%)]\tLoss: 3.518691 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3456/3951 (87%)]\tLoss: 3.635206 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3584/3951 (90%)]\tLoss: 3.655727 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3712/3951 (94%)]\tLoss: 3.780957 Lr: 9.607401842409318e-06\n",
            "Train Epoch: 60 [3330/3951 (97%)]\tLoss: 3.664056 Lr: 9.607401842409318e-06\n",
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 64/406 (16%)\n",
            "\n",
            "Train Epoch: 61 [0/3951 (0%)]\tLoss: 3.531127 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [128/3951 (3%)]\tLoss: 3.605445 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [256/3951 (6%)]\tLoss: 3.505164 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [384/3951 (10%)]\tLoss: 3.633782 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [512/3951 (13%)]\tLoss: 3.619533 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [640/3951 (16%)]\tLoss: 3.555805 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [768/3951 (19%)]\tLoss: 3.571197 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [896/3951 (23%)]\tLoss: 3.583183 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1024/3951 (26%)]\tLoss: 3.753357 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1152/3951 (29%)]\tLoss: 3.649268 Lr: 9.215409042721553e-06\n",
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 64/406 (16%)\n",
            "\n",
            "Train Epoch: 61 [1280/3951 (32%)]\tLoss: 3.724104 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1408/3951 (35%)]\tLoss: 3.631363 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1536/3951 (39%)]\tLoss: 3.668822 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1664/3951 (42%)]\tLoss: 3.709513 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1792/3951 (45%)]\tLoss: 3.710340 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [1920/3951 (48%)]\tLoss: 3.537204 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2048/3951 (52%)]\tLoss: 3.649817 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2176/3951 (55%)]\tLoss: 3.572461 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2304/3951 (58%)]\tLoss: 3.628997 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2432/3951 (61%)]\tLoss: 3.522759 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2560/3951 (65%)]\tLoss: 3.712845 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2688/3951 (68%)]\tLoss: 3.582011 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2816/3951 (71%)]\tLoss: 3.462329 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [2944/3951 (74%)]\tLoss: 3.499329 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3072/3951 (77%)]\tLoss: 3.529389 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3200/3951 (81%)]\tLoss: 3.552916 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3328/3951 (84%)]\tLoss: 3.678957 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3456/3951 (87%)]\tLoss: 3.526426 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3584/3951 (90%)]\tLoss: 3.575019 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3712/3951 (94%)]\tLoss: 3.576906 Lr: 9.215409042721553e-06\n",
            "Train Epoch: 61 [3330/3951 (97%)]\tLoss: 3.510964 Lr: 9.215409042721553e-06\n",
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 64/406 (16%)\n",
            "\n",
            "Train Epoch: 62 [0/3951 (0%)]\tLoss: 3.522224 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [128/3951 (3%)]\tLoss: 3.534652 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [256/3951 (6%)]\tLoss: 3.365348 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [384/3951 (10%)]\tLoss: 3.634728 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [512/3951 (13%)]\tLoss: 3.517311 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [640/3951 (16%)]\tLoss: 3.615651 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [768/3951 (19%)]\tLoss: 3.639191 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [896/3951 (23%)]\tLoss: 3.595268 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1024/3951 (26%)]\tLoss: 3.585464 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1152/3951 (29%)]\tLoss: 3.590426 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1280/3951 (32%)]\tLoss: 3.487007 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1408/3951 (35%)]\tLoss: 3.638623 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1536/3951 (39%)]\tLoss: 3.635383 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1664/3951 (42%)]\tLoss: 3.695062 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1792/3951 (45%)]\tLoss: 3.557772 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [1920/3951 (48%)]\tLoss: 3.662060 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2048/3951 (52%)]\tLoss: 3.659261 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2176/3951 (55%)]\tLoss: 3.618813 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2304/3951 (58%)]\tLoss: 3.478436 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2432/3951 (61%)]\tLoss: 3.520509 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2560/3951 (65%)]\tLoss: 3.502865 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2688/3951 (68%)]\tLoss: 3.751056 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2816/3951 (71%)]\tLoss: 3.651411 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [2944/3951 (74%)]\tLoss: 3.598379 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3072/3951 (77%)]\tLoss: 3.567951 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3200/3951 (81%)]\tLoss: 3.633985 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3328/3951 (84%)]\tLoss: 3.558256 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3456/3951 (87%)]\tLoss: 3.622199 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3584/3951 (90%)]\tLoss: 3.712176 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3712/3951 (94%)]\tLoss: 3.676982 Lr: 8.824626025421625e-06\n",
            "Train Epoch: 62 [3330/3951 (97%)]\tLoss: 3.551016 Lr: 8.824626025421625e-06\n",
            "\n",
            "Test set: Average loss: 0.0352, Accuracy: 65/406 (16%)\n",
            "\n",
            "Train Epoch: 63 [0/3951 (0%)]\tLoss: 3.492447 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [128/3951 (3%)]\tLoss: 3.736037 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [256/3951 (6%)]\tLoss: 3.687966 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [384/3951 (10%)]\tLoss: 3.647295 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [512/3951 (13%)]\tLoss: 3.512969 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [640/3951 (16%)]\tLoss: 3.533042 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [768/3951 (19%)]\tLoss: 3.530695 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [896/3951 (23%)]\tLoss: 3.594148 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1024/3951 (26%)]\tLoss: 3.576373 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1152/3951 (29%)]\tLoss: 3.497981 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1280/3951 (32%)]\tLoss: 3.558408 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1408/3951 (35%)]\tLoss: 3.569638 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1536/3951 (39%)]\tLoss: 3.611563 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1664/3951 (42%)]\tLoss: 3.630147 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1792/3951 (45%)]\tLoss: 3.601995 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [1920/3951 (48%)]\tLoss: 3.736264 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2048/3951 (52%)]\tLoss: 3.492237 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2176/3951 (55%)]\tLoss: 3.563262 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2304/3951 (58%)]\tLoss: 3.629426 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2432/3951 (61%)]\tLoss: 3.578520 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2560/3951 (65%)]\tLoss: 3.550609 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2688/3951 (68%)]\tLoss: 3.618462 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2816/3951 (71%)]\tLoss: 3.697184 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [2944/3951 (74%)]\tLoss: 3.638470 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3072/3951 (77%)]\tLoss: 3.516284 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3200/3951 (81%)]\tLoss: 3.559526 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3328/3951 (84%)]\tLoss: 3.696507 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3456/3951 (87%)]\tLoss: 3.534652 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3584/3951 (90%)]\tLoss: 3.586348 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3712/3951 (94%)]\tLoss: 3.572467 Lr: 8.43565534959769e-06\n",
            "Train Epoch: 63 [3330/3951 (97%)]\tLoss: 3.625167 Lr: 8.43565534959769e-06\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 64 [0/3951 (0%)]\tLoss: 3.665756 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [128/3951 (3%)]\tLoss: 3.623362 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [256/3951 (6%)]\tLoss: 3.719057 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [384/3951 (10%)]\tLoss: 3.528876 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [512/3951 (13%)]\tLoss: 3.626587 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [640/3951 (16%)]\tLoss: 3.613092 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [768/3951 (19%)]\tLoss: 3.610117 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [896/3951 (23%)]\tLoss: 3.686151 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1024/3951 (26%)]\tLoss: 3.660382 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1152/3951 (29%)]\tLoss: 3.506717 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1280/3951 (32%)]\tLoss: 3.586678 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1408/3951 (35%)]\tLoss: 3.774593 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1536/3951 (39%)]\tLoss: 3.638350 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1664/3951 (42%)]\tLoss: 3.598150 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1792/3951 (45%)]\tLoss: 3.595418 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [1920/3951 (48%)]\tLoss: 3.593450 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2048/3951 (52%)]\tLoss: 3.663178 Lr: 8.04909677983872e-06\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 64 [2176/3951 (55%)]\tLoss: 3.622936 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2304/3951 (58%)]\tLoss: 3.564381 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2432/3951 (61%)]\tLoss: 3.582710 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2560/3951 (65%)]\tLoss: 3.485063 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2688/3951 (68%)]\tLoss: 3.521235 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2816/3951 (71%)]\tLoss: 3.654381 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [2944/3951 (74%)]\tLoss: 3.607571 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3072/3951 (77%)]\tLoss: 3.470951 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3200/3951 (81%)]\tLoss: 3.683990 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3328/3951 (84%)]\tLoss: 3.528748 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3456/3951 (87%)]\tLoss: 3.587243 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3584/3951 (90%)]\tLoss: 3.582930 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3712/3951 (94%)]\tLoss: 3.577261 Lr: 8.04909677983872e-06\n",
            "Train Epoch: 64 [3330/3951 (97%)]\tLoss: 3.597475 Lr: 8.04909677983872e-06\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 65 [0/3951 (0%)]\tLoss: 3.604998 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [128/3951 (3%)]\tLoss: 3.628045 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [256/3951 (6%)]\tLoss: 3.638422 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [384/3951 (10%)]\tLoss: 3.652980 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [512/3951 (13%)]\tLoss: 3.600153 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [640/3951 (16%)]\tLoss: 3.677492 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [768/3951 (19%)]\tLoss: 3.573211 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [896/3951 (23%)]\tLoss: 3.550356 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1024/3951 (26%)]\tLoss: 3.598126 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1152/3951 (29%)]\tLoss: 3.534847 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1280/3951 (32%)]\tLoss: 3.537621 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1408/3951 (35%)]\tLoss: 3.676905 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1536/3951 (39%)]\tLoss: 3.578375 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1664/3951 (42%)]\tLoss: 3.521194 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1792/3951 (45%)]\tLoss: 3.739064 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [1920/3951 (48%)]\tLoss: 3.592405 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2048/3951 (52%)]\tLoss: 3.703694 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2176/3951 (55%)]\tLoss: 3.587027 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2304/3951 (58%)]\tLoss: 3.584049 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2432/3951 (61%)]\tLoss: 3.604452 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2560/3951 (65%)]\tLoss: 3.701813 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2688/3951 (68%)]\tLoss: 3.596883 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2816/3951 (71%)]\tLoss: 3.678418 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [2944/3951 (74%)]\tLoss: 3.673433 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3072/3951 (77%)]\tLoss: 3.675078 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3200/3951 (81%)]\tLoss: 3.634037 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3328/3951 (84%)]\tLoss: 3.690703 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3456/3951 (87%)]\tLoss: 3.506693 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3584/3951 (90%)]\tLoss: 3.584969 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3712/3951 (94%)]\tLoss: 3.541783 Lr: 7.66554636144095e-06\n",
            "Train Epoch: 65 [3330/3951 (97%)]\tLoss: 3.548436 Lr: 7.66554636144095e-06\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 66 [0/3951 (0%)]\tLoss: 3.548798 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [128/3951 (3%)]\tLoss: 3.716412 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [256/3951 (6%)]\tLoss: 3.609708 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [384/3951 (10%)]\tLoss: 3.508602 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [512/3951 (13%)]\tLoss: 3.563055 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [640/3951 (16%)]\tLoss: 3.581642 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [768/3951 (19%)]\tLoss: 3.606353 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [896/3951 (23%)]\tLoss: 3.629779 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1024/3951 (26%)]\tLoss: 3.715718 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1152/3951 (29%)]\tLoss: 3.646446 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1280/3951 (32%)]\tLoss: 3.493703 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1408/3951 (35%)]\tLoss: 3.530393 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1536/3951 (39%)]\tLoss: 3.484489 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1664/3951 (42%)]\tLoss: 3.550754 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1792/3951 (45%)]\tLoss: 3.495672 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [1920/3951 (48%)]\tLoss: 3.587068 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2048/3951 (52%)]\tLoss: 3.727832 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2176/3951 (55%)]\tLoss: 3.587917 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2304/3951 (58%)]\tLoss: 3.593702 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2432/3951 (61%)]\tLoss: 3.700484 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2560/3951 (65%)]\tLoss: 3.460256 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2688/3951 (68%)]\tLoss: 3.653382 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2816/3951 (71%)]\tLoss: 3.627599 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [2944/3951 (74%)]\tLoss: 3.615525 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3072/3951 (77%)]\tLoss: 3.570061 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3200/3951 (81%)]\tLoss: 3.548089 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3328/3951 (84%)]\tLoss: 3.501595 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3456/3951 (87%)]\tLoss: 3.609566 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3584/3951 (90%)]\tLoss: 3.551231 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3712/3951 (94%)]\tLoss: 3.560071 Lr: 7.285595501349259e-06\n",
            "Train Epoch: 66 [3330/3951 (97%)]\tLoss: 3.574836 Lr: 7.285595501349259e-06\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 67 [0/3951 (0%)]\tLoss: 3.669787 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [128/3951 (3%)]\tLoss: 3.614024 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [256/3951 (6%)]\tLoss: 3.536193 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [384/3951 (10%)]\tLoss: 3.568024 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [512/3951 (13%)]\tLoss: 3.634994 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [640/3951 (16%)]\tLoss: 3.564874 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [768/3951 (19%)]\tLoss: 3.530643 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [896/3951 (23%)]\tLoss: 3.568218 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1024/3951 (26%)]\tLoss: 3.530706 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1152/3951 (29%)]\tLoss: 3.736066 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1280/3951 (32%)]\tLoss: 3.792765 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1408/3951 (35%)]\tLoss: 3.480826 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1536/3951 (39%)]\tLoss: 3.521839 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1664/3951 (42%)]\tLoss: 3.426928 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1792/3951 (45%)]\tLoss: 3.641411 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [1920/3951 (48%)]\tLoss: 3.560270 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2048/3951 (52%)]\tLoss: 3.597960 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2176/3951 (55%)]\tLoss: 3.636784 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2304/3951 (58%)]\tLoss: 3.522206 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2432/3951 (61%)]\tLoss: 3.637588 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2560/3951 (65%)]\tLoss: 3.736688 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2688/3951 (68%)]\tLoss: 3.588792 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2816/3951 (71%)]\tLoss: 3.726298 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [2944/3951 (74%)]\tLoss: 3.574883 Lr: 6.909830056250527e-06\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 67 [3072/3951 (77%)]\tLoss: 3.614431 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [3200/3951 (81%)]\tLoss: 3.581407 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [3328/3951 (84%)]\tLoss: 3.577336 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [3456/3951 (87%)]\tLoss: 3.582732 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [3584/3951 (90%)]\tLoss: 3.556118 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [3712/3951 (94%)]\tLoss: 3.615147 Lr: 6.909830056250527e-06\n",
            "Train Epoch: 67 [3330/3951 (97%)]\tLoss: 3.508306 Lr: 6.909830056250527e-06\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 68 [0/3951 (0%)]\tLoss: 3.487322 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [128/3951 (3%)]\tLoss: 3.592585 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [256/3951 (6%)]\tLoss: 3.639240 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [384/3951 (10%)]\tLoss: 3.565671 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [512/3951 (13%)]\tLoss: 3.708269 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [640/3951 (16%)]\tLoss: 3.682346 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [768/3951 (19%)]\tLoss: 3.603846 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [896/3951 (23%)]\tLoss: 3.504575 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1024/3951 (26%)]\tLoss: 3.506971 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1152/3951 (29%)]\tLoss: 3.629032 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1280/3951 (32%)]\tLoss: 3.620425 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1408/3951 (35%)]\tLoss: 3.575698 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1536/3951 (39%)]\tLoss: 3.623967 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1664/3951 (42%)]\tLoss: 3.558528 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1792/3951 (45%)]\tLoss: 3.471606 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [1920/3951 (48%)]\tLoss: 3.586951 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2048/3951 (52%)]\tLoss: 3.590515 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2176/3951 (55%)]\tLoss: 3.538909 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2304/3951 (58%)]\tLoss: 3.667907 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2432/3951 (61%)]\tLoss: 3.619463 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2560/3951 (65%)]\tLoss: 3.685924 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2688/3951 (68%)]\tLoss: 3.528310 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2816/3951 (71%)]\tLoss: 3.661154 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [2944/3951 (74%)]\tLoss: 3.584354 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3072/3951 (77%)]\tLoss: 3.595074 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3200/3951 (81%)]\tLoss: 3.576033 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3328/3951 (84%)]\tLoss: 3.518837 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3456/3951 (87%)]\tLoss: 3.645557 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3584/3951 (90%)]\tLoss: 3.544315 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3712/3951 (94%)]\tLoss: 3.691210 Lr: 6.538829429225068e-06\n",
            "Train Epoch: 68 [3330/3951 (97%)]\tLoss: 3.578854 Lr: 6.538829429225068e-06\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 69 [0/3951 (0%)]\tLoss: 3.644023 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [128/3951 (3%)]\tLoss: 3.591750 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [256/3951 (6%)]\tLoss: 3.499484 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [384/3951 (10%)]\tLoss: 3.676592 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [512/3951 (13%)]\tLoss: 3.576745 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [640/3951 (16%)]\tLoss: 3.488067 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [768/3951 (19%)]\tLoss: 3.582353 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [896/3951 (23%)]\tLoss: 3.643265 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1024/3951 (26%)]\tLoss: 3.536566 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1152/3951 (29%)]\tLoss: 3.759489 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1280/3951 (32%)]\tLoss: 3.544417 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1408/3951 (35%)]\tLoss: 3.675174 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1536/3951 (39%)]\tLoss: 3.419472 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1664/3951 (42%)]\tLoss: 3.580872 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1792/3951 (45%)]\tLoss: 3.579473 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [1920/3951 (48%)]\tLoss: 3.658131 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2048/3951 (52%)]\tLoss: 3.583940 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2176/3951 (55%)]\tLoss: 3.641744 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2304/3951 (58%)]\tLoss: 3.513422 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2432/3951 (61%)]\tLoss: 3.559479 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2560/3951 (65%)]\tLoss: 3.532251 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2688/3951 (68%)]\tLoss: 3.658372 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2816/3951 (71%)]\tLoss: 3.488656 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [2944/3951 (74%)]\tLoss: 3.558285 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3072/3951 (77%)]\tLoss: 3.634716 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3200/3951 (81%)]\tLoss: 3.439722 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3328/3951 (84%)]\tLoss: 3.576085 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3456/3951 (87%)]\tLoss: 3.538174 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3584/3951 (90%)]\tLoss: 3.568772 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3712/3951 (94%)]\tLoss: 3.623953 Lr: 6.173165676349103e-06\n",
            "Train Epoch: 69 [3330/3951 (97%)]\tLoss: 3.489514 Lr: 6.173165676349103e-06\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 70 [0/3951 (0%)]\tLoss: 3.422273 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [128/3951 (3%)]\tLoss: 3.600352 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [256/3951 (6%)]\tLoss: 3.678708 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [384/3951 (10%)]\tLoss: 3.581457 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [512/3951 (13%)]\tLoss: 3.528232 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [640/3951 (16%)]\tLoss: 3.552825 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [768/3951 (19%)]\tLoss: 3.567358 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [896/3951 (23%)]\tLoss: 3.485139 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1024/3951 (26%)]\tLoss: 3.610741 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1152/3951 (29%)]\tLoss: 3.578686 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1280/3951 (32%)]\tLoss: 3.484829 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1408/3951 (35%)]\tLoss: 3.557986 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1536/3951 (39%)]\tLoss: 3.654577 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1664/3951 (42%)]\tLoss: 3.505477 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1792/3951 (45%)]\tLoss: 3.612768 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [1920/3951 (48%)]\tLoss: 3.573524 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2048/3951 (52%)]\tLoss: 3.513171 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2176/3951 (55%)]\tLoss: 3.666733 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2304/3951 (58%)]\tLoss: 3.626756 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2432/3951 (61%)]\tLoss: 3.582290 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2560/3951 (65%)]\tLoss: 3.723983 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2688/3951 (68%)]\tLoss: 3.576691 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2816/3951 (71%)]\tLoss: 3.624383 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [2944/3951 (74%)]\tLoss: 3.689929 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3072/3951 (77%)]\tLoss: 3.512892 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3200/3951 (81%)]\tLoss: 3.623301 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3328/3951 (84%)]\tLoss: 3.677927 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3456/3951 (87%)]\tLoss: 3.422730 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3584/3951 (90%)]\tLoss: 3.665612 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3712/3951 (94%)]\tLoss: 3.528474 Lr: 5.813402624625722e-06\n",
            "Train Epoch: 70 [3330/3951 (97%)]\tLoss: 3.611360 Lr: 5.813402624625722e-06\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 66/406 (16%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 71 [0/3951 (0%)]\tLoss: 3.558990 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [128/3951 (3%)]\tLoss: 3.630685 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [256/3951 (6%)]\tLoss: 3.562334 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [384/3951 (10%)]\tLoss: 3.685290 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [512/3951 (13%)]\tLoss: 3.504472 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [640/3951 (16%)]\tLoss: 3.482430 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [768/3951 (19%)]\tLoss: 3.671639 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [896/3951 (23%)]\tLoss: 3.717897 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1024/3951 (26%)]\tLoss: 3.720438 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1152/3951 (29%)]\tLoss: 3.550392 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1280/3951 (32%)]\tLoss: 3.457443 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1408/3951 (35%)]\tLoss: 3.532633 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1536/3951 (39%)]\tLoss: 3.569600 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1664/3951 (42%)]\tLoss: 3.553962 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1792/3951 (45%)]\tLoss: 3.540143 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [1920/3951 (48%)]\tLoss: 3.672172 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2048/3951 (52%)]\tLoss: 3.511323 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2176/3951 (55%)]\tLoss: 3.589770 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2304/3951 (58%)]\tLoss: 3.511994 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2432/3951 (61%)]\tLoss: 3.550879 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2560/3951 (65%)]\tLoss: 3.583282 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2688/3951 (68%)]\tLoss: 3.546024 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2816/3951 (71%)]\tLoss: 3.561007 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [2944/3951 (74%)]\tLoss: 3.501967 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3072/3951 (77%)]\tLoss: 3.565704 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3200/3951 (81%)]\tLoss: 3.521103 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3328/3951 (84%)]\tLoss: 3.597694 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3456/3951 (87%)]\tLoss: 3.654595 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3584/3951 (90%)]\tLoss: 3.557925 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3712/3951 (94%)]\tLoss: 3.625727 Lr: 5.460095002604533e-06\n",
            "Train Epoch: 71 [3330/3951 (97%)]\tLoss: 4.125920 Lr: 5.460095002604533e-06\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 72 [0/3951 (0%)]\tLoss: 3.538573 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [128/3951 (3%)]\tLoss: 3.687758 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [256/3951 (6%)]\tLoss: 3.567487 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [384/3951 (10%)]\tLoss: 3.630850 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [512/3951 (13%)]\tLoss: 3.512664 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [640/3951 (16%)]\tLoss: 3.467901 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [768/3951 (19%)]\tLoss: 3.604755 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [896/3951 (23%)]\tLoss: 3.496852 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1024/3951 (26%)]\tLoss: 3.638209 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1152/3951 (29%)]\tLoss: 3.522147 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1280/3951 (32%)]\tLoss: 3.487123 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1408/3951 (35%)]\tLoss: 3.520315 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1536/3951 (39%)]\tLoss: 3.533373 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1664/3951 (42%)]\tLoss: 3.673213 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1792/3951 (45%)]\tLoss: 3.729937 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [1920/3951 (48%)]\tLoss: 3.657265 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2048/3951 (52%)]\tLoss: 3.596786 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2176/3951 (55%)]\tLoss: 3.570711 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2304/3951 (58%)]\tLoss: 3.536831 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2432/3951 (61%)]\tLoss: 3.608881 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2560/3951 (65%)]\tLoss: 3.629517 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2688/3951 (68%)]\tLoss: 3.571378 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2816/3951 (71%)]\tLoss: 3.621353 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [2944/3951 (74%)]\tLoss: 3.578347 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3072/3951 (77%)]\tLoss: 3.618421 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3200/3951 (81%)]\tLoss: 3.499544 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3328/3951 (84%)]\tLoss: 3.525602 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3456/3951 (87%)]\tLoss: 3.425624 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3584/3951 (90%)]\tLoss: 3.647369 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3712/3951 (94%)]\tLoss: 3.636354 Lr: 5.1137875850304545e-06\n",
            "Train Epoch: 72 [3330/3951 (97%)]\tLoss: 3.632407 Lr: 5.1137875850304545e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 73 [0/3951 (0%)]\tLoss: 3.668723 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [128/3951 (3%)]\tLoss: 3.561450 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [256/3951 (6%)]\tLoss: 3.534392 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [384/3951 (10%)]\tLoss: 3.537180 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [512/3951 (13%)]\tLoss: 3.520972 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [640/3951 (16%)]\tLoss: 3.581763 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [768/3951 (19%)]\tLoss: 3.656016 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [896/3951 (23%)]\tLoss: 3.498651 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1024/3951 (26%)]\tLoss: 3.440091 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1152/3951 (29%)]\tLoss: 3.713778 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1280/3951 (32%)]\tLoss: 3.493297 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1408/3951 (35%)]\tLoss: 3.588139 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1536/3951 (39%)]\tLoss: 3.570728 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1664/3951 (42%)]\tLoss: 3.602280 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1792/3951 (45%)]\tLoss: 3.539932 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [1920/3951 (48%)]\tLoss: 3.735756 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2048/3951 (52%)]\tLoss: 3.773852 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2176/3951 (55%)]\tLoss: 3.592715 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2304/3951 (58%)]\tLoss: 3.542204 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2432/3951 (61%)]\tLoss: 3.543505 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2560/3951 (65%)]\tLoss: 3.664436 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2688/3951 (68%)]\tLoss: 3.645780 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2816/3951 (71%)]\tLoss: 3.577523 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [2944/3951 (74%)]\tLoss: 3.497229 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3072/3951 (77%)]\tLoss: 3.530429 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3200/3951 (81%)]\tLoss: 3.539939 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3328/3951 (84%)]\tLoss: 3.559170 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3456/3951 (87%)]\tLoss: 3.692538 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3584/3951 (90%)]\tLoss: 3.621684 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3712/3951 (94%)]\tLoss: 3.591447 Lr: 4.775014352840512e-06\n",
            "Train Epoch: 73 [3330/3951 (97%)]\tLoss: 3.652911 Lr: 4.775014352840512e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 74 [0/3951 (0%)]\tLoss: 3.623379 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [128/3951 (3%)]\tLoss: 3.663735 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [256/3951 (6%)]\tLoss: 3.588245 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [384/3951 (10%)]\tLoss: 3.606752 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [512/3951 (13%)]\tLoss: 3.490327 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [640/3951 (16%)]\tLoss: 3.579924 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [768/3951 (19%)]\tLoss: 3.492826 Lr: 4.444297669803981e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 74 [896/3951 (23%)]\tLoss: 3.586934 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1024/3951 (26%)]\tLoss: 3.646839 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1152/3951 (29%)]\tLoss: 3.523995 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1280/3951 (32%)]\tLoss: 3.531372 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1408/3951 (35%)]\tLoss: 3.507862 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1536/3951 (39%)]\tLoss: 3.539180 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1664/3951 (42%)]\tLoss: 3.688994 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1792/3951 (45%)]\tLoss: 3.611682 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [1920/3951 (48%)]\tLoss: 3.509690 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2048/3951 (52%)]\tLoss: 3.565832 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2176/3951 (55%)]\tLoss: 3.608038 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2304/3951 (58%)]\tLoss: 3.647738 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2432/3951 (61%)]\tLoss: 3.566638 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2560/3951 (65%)]\tLoss: 3.491099 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2688/3951 (68%)]\tLoss: 3.622450 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2816/3951 (71%)]\tLoss: 3.560124 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [2944/3951 (74%)]\tLoss: 3.656011 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3072/3951 (77%)]\tLoss: 3.572647 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3200/3951 (81%)]\tLoss: 3.472271 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3328/3951 (84%)]\tLoss: 3.546774 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3456/3951 (87%)]\tLoss: 3.730229 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3584/3951 (90%)]\tLoss: 3.551207 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3712/3951 (94%)]\tLoss: 3.566989 Lr: 4.444297669803981e-06\n",
            "Train Epoch: 74 [3330/3951 (97%)]\tLoss: 3.610561 Lr: 4.444297669803981e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 75 [0/3951 (0%)]\tLoss: 3.756838 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [128/3951 (3%)]\tLoss: 3.527924 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [256/3951 (6%)]\tLoss: 3.540235 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [384/3951 (10%)]\tLoss: 3.494827 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [512/3951 (13%)]\tLoss: 3.602781 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [640/3951 (16%)]\tLoss: 3.588584 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [768/3951 (19%)]\tLoss: 3.588126 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [896/3951 (23%)]\tLoss: 3.565624 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1024/3951 (26%)]\tLoss: 3.511987 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1152/3951 (29%)]\tLoss: 3.602453 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1280/3951 (32%)]\tLoss: 3.591427 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1408/3951 (35%)]\tLoss: 3.495600 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1536/3951 (39%)]\tLoss: 3.571747 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1664/3951 (42%)]\tLoss: 3.620463 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1792/3951 (45%)]\tLoss: 3.540027 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [1920/3951 (48%)]\tLoss: 3.626086 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2048/3951 (52%)]\tLoss: 3.644913 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2176/3951 (55%)]\tLoss: 3.633923 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2304/3951 (58%)]\tLoss: 3.573499 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2432/3951 (61%)]\tLoss: 3.527318 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2560/3951 (65%)]\tLoss: 3.544750 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2688/3951 (68%)]\tLoss: 3.638275 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2816/3951 (71%)]\tLoss: 4.048540 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [2944/3951 (74%)]\tLoss: 3.548381 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3072/3951 (77%)]\tLoss: 3.520798 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3200/3951 (81%)]\tLoss: 3.535825 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3328/3951 (84%)]\tLoss: 3.495084 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3456/3951 (87%)]\tLoss: 3.533883 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3584/3951 (90%)]\tLoss: 3.709864 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3712/3951 (94%)]\tLoss: 3.779668 Lr: 4.12214747707527e-06\n",
            "Train Epoch: 75 [3330/3951 (97%)]\tLoss: 3.633811 Lr: 4.12214747707527e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 76 [0/3951 (0%)]\tLoss: 3.744770 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [128/3951 (3%)]\tLoss: 3.658907 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [256/3951 (6%)]\tLoss: 3.499924 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [384/3951 (10%)]\tLoss: 3.565574 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [512/3951 (13%)]\tLoss: 3.589935 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [640/3951 (16%)]\tLoss: 3.470933 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [768/3951 (19%)]\tLoss: 3.540366 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [896/3951 (23%)]\tLoss: 3.698022 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1024/3951 (26%)]\tLoss: 3.697747 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1152/3951 (29%)]\tLoss: 3.462121 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1280/3951 (32%)]\tLoss: 3.633116 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1408/3951 (35%)]\tLoss: 3.575416 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1536/3951 (39%)]\tLoss: 3.588773 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1664/3951 (42%)]\tLoss: 3.629457 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1792/3951 (45%)]\tLoss: 3.498621 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [1920/3951 (48%)]\tLoss: 3.506845 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2048/3951 (52%)]\tLoss: 3.539236 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2176/3951 (55%)]\tLoss: 3.513112 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2304/3951 (58%)]\tLoss: 3.607256 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2432/3951 (61%)]\tLoss: 3.583573 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2560/3951 (65%)]\tLoss: 3.624490 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2688/3951 (68%)]\tLoss: 3.552191 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2816/3951 (71%)]\tLoss: 3.495170 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [2944/3951 (74%)]\tLoss: 3.574886 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3072/3951 (77%)]\tLoss: 3.515220 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3200/3951 (81%)]\tLoss: 3.526115 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3328/3951 (84%)]\tLoss: 3.569421 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3456/3951 (87%)]\tLoss: 3.590947 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3584/3951 (90%)]\tLoss: 3.621131 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3712/3951 (94%)]\tLoss: 3.523184 Lr: 3.8090605069016596e-06\n",
            "Train Epoch: 76 [3330/3951 (97%)]\tLoss: 3.445628 Lr: 3.8090605069016596e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 77 [0/3951 (0%)]\tLoss: 3.503827 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [128/3951 (3%)]\tLoss: 3.567853 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [256/3951 (6%)]\tLoss: 3.595069 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [384/3951 (10%)]\tLoss: 3.524527 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [512/3951 (13%)]\tLoss: 3.573595 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [640/3951 (16%)]\tLoss: 3.575968 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [768/3951 (19%)]\tLoss: 3.529351 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [896/3951 (23%)]\tLoss: 3.631594 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1024/3951 (26%)]\tLoss: 3.624290 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1152/3951 (29%)]\tLoss: 3.493052 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1280/3951 (32%)]\tLoss: 3.680031 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1408/3951 (35%)]\tLoss: 3.529181 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1536/3951 (39%)]\tLoss: 3.700497 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1664/3951 (42%)]\tLoss: 3.596715 Lr: 3.505519516698165e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 77 [1792/3951 (45%)]\tLoss: 3.601305 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [1920/3951 (48%)]\tLoss: 3.511879 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2048/3951 (52%)]\tLoss: 3.558812 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2176/3951 (55%)]\tLoss: 3.519254 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2304/3951 (58%)]\tLoss: 3.606065 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2432/3951 (61%)]\tLoss: 3.550204 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2560/3951 (65%)]\tLoss: 3.594500 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2688/3951 (68%)]\tLoss: 3.683122 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2816/3951 (71%)]\tLoss: 3.550904 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [2944/3951 (74%)]\tLoss: 3.574635 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3072/3951 (77%)]\tLoss: 3.492249 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3200/3951 (81%)]\tLoss: 3.540032 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3328/3951 (84%)]\tLoss: 3.451741 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3456/3951 (87%)]\tLoss: 3.511631 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3584/3951 (90%)]\tLoss: 3.617918 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3712/3951 (94%)]\tLoss: 3.539290 Lr: 3.505519516698165e-06\n",
            "Train Epoch: 77 [3330/3951 (97%)]\tLoss: 3.671191 Lr: 3.505519516698165e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 78 [0/3951 (0%)]\tLoss: 3.487898 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [128/3951 (3%)]\tLoss: 3.519229 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [256/3951 (6%)]\tLoss: 3.686330 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [384/3951 (10%)]\tLoss: 3.610610 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [512/3951 (13%)]\tLoss: 3.632740 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [640/3951 (16%)]\tLoss: 3.539989 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [768/3951 (19%)]\tLoss: 3.537262 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [896/3951 (23%)]\tLoss: 3.541469 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1024/3951 (26%)]\tLoss: 3.532867 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1152/3951 (29%)]\tLoss: 3.653611 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1280/3951 (32%)]\tLoss: 3.596224 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1408/3951 (35%)]\tLoss: 3.537491 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1536/3951 (39%)]\tLoss: 3.585534 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1664/3951 (42%)]\tLoss: 3.644231 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1792/3951 (45%)]\tLoss: 3.456547 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [1920/3951 (48%)]\tLoss: 3.596580 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2048/3951 (52%)]\tLoss: 3.648023 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2176/3951 (55%)]\tLoss: 3.600487 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2304/3951 (58%)]\tLoss: 3.683379 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2432/3951 (61%)]\tLoss: 3.584223 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2560/3951 (65%)]\tLoss: 3.537422 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2688/3951 (68%)]\tLoss: 3.496077 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2816/3951 (71%)]\tLoss: 3.579640 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [2944/3951 (74%)]\tLoss: 3.538381 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3072/3951 (77%)]\tLoss: 3.652038 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3200/3951 (81%)]\tLoss: 3.508764 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3328/3951 (84%)]\tLoss: 3.420377 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3456/3951 (87%)]\tLoss: 3.549897 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3584/3951 (90%)]\tLoss: 3.644038 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3712/3951 (94%)]\tLoss: 3.562586 Lr: 3.2119925446705824e-06\n",
            "Train Epoch: 78 [3330/3951 (97%)]\tLoss: 3.632992 Lr: 3.2119925446705824e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 79 [0/3951 (0%)]\tLoss: 3.652143 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [128/3951 (3%)]\tLoss: 3.348392 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [256/3951 (6%)]\tLoss: 3.557782 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [384/3951 (10%)]\tLoss: 3.507268 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [512/3951 (13%)]\tLoss: 3.575227 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [640/3951 (16%)]\tLoss: 3.539442 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [768/3951 (19%)]\tLoss: 3.639069 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [896/3951 (23%)]\tLoss: 3.549642 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1024/3951 (26%)]\tLoss: 3.527111 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1152/3951 (29%)]\tLoss: 3.590815 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1280/3951 (32%)]\tLoss: 3.609818 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1408/3951 (35%)]\tLoss: 3.507994 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1536/3951 (39%)]\tLoss: 3.659857 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1664/3951 (42%)]\tLoss: 3.576795 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1792/3951 (45%)]\tLoss: 3.594388 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [1920/3951 (48%)]\tLoss: 3.605033 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2048/3951 (52%)]\tLoss: 4.166964 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2176/3951 (55%)]\tLoss: 3.498086 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2304/3951 (58%)]\tLoss: 3.537056 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2432/3951 (61%)]\tLoss: 3.553551 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2560/3951 (65%)]\tLoss: 3.623186 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2688/3951 (68%)]\tLoss: 3.633626 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2816/3951 (71%)]\tLoss: 3.610693 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [2944/3951 (74%)]\tLoss: 3.663855 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3072/3951 (77%)]\tLoss: 3.632249 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3200/3951 (81%)]\tLoss: 3.524515 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3328/3951 (84%)]\tLoss: 3.505420 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3456/3951 (87%)]\tLoss: 3.467921 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3584/3951 (90%)]\tLoss: 3.559580 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3712/3951 (94%)]\tLoss: 3.673226 Lr: 2.9289321881345257e-06\n",
            "Train Epoch: 79 [3330/3951 (97%)]\tLoss: 3.516907 Lr: 2.9289321881345257e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 80 [0/3951 (0%)]\tLoss: 3.631019 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [128/3951 (3%)]\tLoss: 3.578397 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [256/3951 (6%)]\tLoss: 3.655050 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [384/3951 (10%)]\tLoss: 3.500938 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [512/3951 (13%)]\tLoss: 3.541791 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [640/3951 (16%)]\tLoss: 3.660949 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [768/3951 (19%)]\tLoss: 3.464052 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [896/3951 (23%)]\tLoss: 3.505316 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1024/3951 (26%)]\tLoss: 3.586957 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1152/3951 (29%)]\tLoss: 3.546208 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1280/3951 (32%)]\tLoss: 3.667578 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1408/3951 (35%)]\tLoss: 3.553382 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1536/3951 (39%)]\tLoss: 3.557054 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1664/3951 (42%)]\tLoss: 3.608686 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1792/3951 (45%)]\tLoss: 3.510635 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [1920/3951 (48%)]\tLoss: 3.540893 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2048/3951 (52%)]\tLoss: 3.608509 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2176/3951 (55%)]\tLoss: 3.596699 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2304/3951 (58%)]\tLoss: 3.435995 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2432/3951 (61%)]\tLoss: 3.451211 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2560/3951 (65%)]\tLoss: 3.554667 Lr: 2.656774905643147e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 80 [2688/3951 (68%)]\tLoss: 3.577946 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2816/3951 (71%)]\tLoss: 3.580067 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [2944/3951 (74%)]\tLoss: 3.617316 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3072/3951 (77%)]\tLoss: 3.586908 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3200/3951 (81%)]\tLoss: 3.520331 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3328/3951 (84%)]\tLoss: 3.596514 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3456/3951 (87%)]\tLoss: 3.632573 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3584/3951 (90%)]\tLoss: 3.707030 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3712/3951 (94%)]\tLoss: 3.561134 Lr: 2.656774905643147e-06\n",
            "Train Epoch: 80 [3330/3951 (97%)]\tLoss: 3.657151 Lr: 2.656774905643147e-06\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 81 [0/3951 (0%)]\tLoss: 3.677984 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [128/3951 (3%)]\tLoss: 3.552728 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [256/3951 (6%)]\tLoss: 3.468295 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [384/3951 (10%)]\tLoss: 3.702695 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [512/3951 (13%)]\tLoss: 3.498089 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [640/3951 (16%)]\tLoss: 3.525025 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [768/3951 (19%)]\tLoss: 3.684307 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [896/3951 (23%)]\tLoss: 3.550833 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1024/3951 (26%)]\tLoss: 3.502952 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1152/3951 (29%)]\tLoss: 3.652973 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1280/3951 (32%)]\tLoss: 3.509063 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1408/3951 (35%)]\tLoss: 3.516230 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1536/3951 (39%)]\tLoss: 3.570105 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1664/3951 (42%)]\tLoss: 3.436070 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1792/3951 (45%)]\tLoss: 3.651217 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [1920/3951 (48%)]\tLoss: 3.628730 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2048/3951 (52%)]\tLoss: 3.476021 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2176/3951 (55%)]\tLoss: 4.051519 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2304/3951 (58%)]\tLoss: 3.532058 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2432/3951 (61%)]\tLoss: 3.613855 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2560/3951 (65%)]\tLoss: 3.494893 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2688/3951 (68%)]\tLoss: 3.524245 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2816/3951 (71%)]\tLoss: 3.594054 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [2944/3951 (74%)]\tLoss: 3.452150 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3072/3951 (77%)]\tLoss: 3.742185 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3200/3951 (81%)]\tLoss: 3.550268 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3328/3951 (84%)]\tLoss: 3.574806 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3456/3951 (87%)]\tLoss: 3.536181 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3584/3951 (90%)]\tLoss: 3.594156 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3712/3951 (94%)]\tLoss: 3.621625 Lr: 2.395940343999691e-06\n",
            "Train Epoch: 81 [3330/3951 (97%)]\tLoss: 3.515099 Lr: 2.395940343999691e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 82 [0/3951 (0%)]\tLoss: 3.536772 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [128/3951 (3%)]\tLoss: 3.690338 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [256/3951 (6%)]\tLoss: 3.474011 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [384/3951 (10%)]\tLoss: 3.420519 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [512/3951 (13%)]\tLoss: 3.652057 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [640/3951 (16%)]\tLoss: 3.576327 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [768/3951 (19%)]\tLoss: 3.550272 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [896/3951 (23%)]\tLoss: 3.635941 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1024/3951 (26%)]\tLoss: 3.609813 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1152/3951 (29%)]\tLoss: 3.456158 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1280/3951 (32%)]\tLoss: 3.619775 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1408/3951 (35%)]\tLoss: 3.513193 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1536/3951 (39%)]\tLoss: 3.528688 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1664/3951 (42%)]\tLoss: 3.613085 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1792/3951 (45%)]\tLoss: 3.661675 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [1920/3951 (48%)]\tLoss: 3.606674 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2048/3951 (52%)]\tLoss: 3.578466 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2176/3951 (55%)]\tLoss: 3.530417 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2304/3951 (58%)]\tLoss: 3.515230 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2432/3951 (61%)]\tLoss: 3.506415 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2560/3951 (65%)]\tLoss: 3.553713 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2688/3951 (68%)]\tLoss: 3.649344 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2816/3951 (71%)]\tLoss: 3.553111 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [2944/3951 (74%)]\tLoss: 3.634495 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3072/3951 (77%)]\tLoss: 3.479285 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3200/3951 (81%)]\tLoss: 3.627203 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3328/3951 (84%)]\tLoss: 3.667094 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3456/3951 (87%)]\tLoss: 3.732524 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3584/3951 (90%)]\tLoss: 3.638595 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3712/3951 (94%)]\tLoss: 3.686927 Lr: 2.146830691192553e-06\n",
            "Train Epoch: 82 [3330/3951 (97%)]\tLoss: 3.512446 Lr: 2.146830691192553e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 83 [0/3951 (0%)]\tLoss: 3.630535 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [128/3951 (3%)]\tLoss: 3.502291 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [256/3951 (6%)]\tLoss: 3.478696 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [384/3951 (10%)]\tLoss: 3.494881 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [512/3951 (13%)]\tLoss: 3.542576 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [640/3951 (16%)]\tLoss: 3.626795 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [768/3951 (19%)]\tLoss: 3.533394 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [896/3951 (23%)]\tLoss: 3.726635 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1024/3951 (26%)]\tLoss: 3.503485 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1152/3951 (29%)]\tLoss: 3.586475 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1280/3951 (32%)]\tLoss: 3.522549 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1408/3951 (35%)]\tLoss: 3.500536 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1536/3951 (39%)]\tLoss: 3.568891 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1664/3951 (42%)]\tLoss: 3.493585 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1792/3951 (45%)]\tLoss: 3.634080 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [1920/3951 (48%)]\tLoss: 3.608711 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2048/3951 (52%)]\tLoss: 3.511794 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2176/3951 (55%)]\tLoss: 3.555124 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2304/3951 (58%)]\tLoss: 3.519522 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2432/3951 (61%)]\tLoss: 3.529865 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2560/3951 (65%)]\tLoss: 3.515866 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2688/3951 (68%)]\tLoss: 3.598127 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2816/3951 (71%)]\tLoss: 3.609762 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [2944/3951 (74%)]\tLoss: 3.628433 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [3072/3951 (77%)]\tLoss: 3.641744 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [3200/3951 (81%)]\tLoss: 3.649731 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [3328/3951 (84%)]\tLoss: 3.592586 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [3456/3951 (87%)]\tLoss: 3.623538 Lr: 1.9098300562505266e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 83 [3584/3951 (90%)]\tLoss: 3.628212 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [3712/3951 (94%)]\tLoss: 3.503314 Lr: 1.9098300562505266e-06\n",
            "Train Epoch: 83 [3330/3951 (97%)]\tLoss: 3.589443 Lr: 1.9098300562505266e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 84 [0/3951 (0%)]\tLoss: 3.510366 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [128/3951 (3%)]\tLoss: 3.463623 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [256/3951 (6%)]\tLoss: 3.608350 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [384/3951 (10%)]\tLoss: 3.666599 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [512/3951 (13%)]\tLoss: 3.664116 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [640/3951 (16%)]\tLoss: 3.466415 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [768/3951 (19%)]\tLoss: 3.518763 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [896/3951 (23%)]\tLoss: 3.584933 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1024/3951 (26%)]\tLoss: 3.505124 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1152/3951 (29%)]\tLoss: 3.527962 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1280/3951 (32%)]\tLoss: 3.561777 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1408/3951 (35%)]\tLoss: 3.440490 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1536/3951 (39%)]\tLoss: 3.552413 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1664/3951 (42%)]\tLoss: 3.643256 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1792/3951 (45%)]\tLoss: 3.613741 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [1920/3951 (48%)]\tLoss: 3.434162 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2048/3951 (52%)]\tLoss: 3.577186 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2176/3951 (55%)]\tLoss: 3.523514 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2304/3951 (58%)]\tLoss: 3.577424 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2432/3951 (61%)]\tLoss: 3.591837 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2560/3951 (65%)]\tLoss: 3.628202 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2688/3951 (68%)]\tLoss: 3.517909 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2816/3951 (71%)]\tLoss: 3.468950 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [2944/3951 (74%)]\tLoss: 3.567444 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3072/3951 (77%)]\tLoss: 3.594927 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3200/3951 (81%)]\tLoss: 3.619199 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3328/3951 (84%)]\tLoss: 3.616729 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3456/3951 (87%)]\tLoss: 3.634501 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3584/3951 (90%)]\tLoss: 3.562845 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3712/3951 (94%)]\tLoss: 3.601668 Lr: 1.6853038769745466e-06\n",
            "Train Epoch: 84 [3330/3951 (97%)]\tLoss: 3.737059 Lr: 1.6853038769745466e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 66/406 (16%)\n",
            "\n",
            "Train Epoch: 85 [0/3951 (0%)]\tLoss: 3.561223 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [128/3951 (3%)]\tLoss: 3.592055 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [256/3951 (6%)]\tLoss: 3.530090 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [384/3951 (10%)]\tLoss: 3.546408 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [512/3951 (13%)]\tLoss: 3.684886 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [640/3951 (16%)]\tLoss: 3.489505 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [768/3951 (19%)]\tLoss: 3.452744 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [896/3951 (23%)]\tLoss: 3.590889 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1024/3951 (26%)]\tLoss: 3.412009 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1152/3951 (29%)]\tLoss: 3.630189 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1280/3951 (32%)]\tLoss: 3.559963 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1408/3951 (35%)]\tLoss: 3.546289 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1536/3951 (39%)]\tLoss: 3.580369 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1664/3951 (42%)]\tLoss: 3.535827 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1792/3951 (45%)]\tLoss: 3.560826 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [1920/3951 (48%)]\tLoss: 3.635930 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2048/3951 (52%)]\tLoss: 3.500818 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2176/3951 (55%)]\tLoss: 3.483306 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2304/3951 (58%)]\tLoss: 3.469169 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2432/3951 (61%)]\tLoss: 3.683226 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2560/3951 (65%)]\tLoss: 3.556411 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2688/3951 (68%)]\tLoss: 3.596873 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2816/3951 (71%)]\tLoss: 3.635917 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [2944/3951 (74%)]\tLoss: 3.531154 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3072/3951 (77%)]\tLoss: 3.630370 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3200/3951 (81%)]\tLoss: 3.588156 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3328/3951 (84%)]\tLoss: 3.637609 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3456/3951 (87%)]\tLoss: 3.597793 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3584/3951 (90%)]\tLoss: 3.614476 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3712/3951 (94%)]\tLoss: 3.554018 Lr: 1.4735983564590784e-06\n",
            "Train Epoch: 85 [3330/3951 (97%)]\tLoss: 3.522094 Lr: 1.4735983564590784e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 86 [0/3951 (0%)]\tLoss: 3.546899 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [128/3951 (3%)]\tLoss: 3.533220 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [256/3951 (6%)]\tLoss: 3.490527 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [384/3951 (10%)]\tLoss: 3.600949 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [512/3951 (13%)]\tLoss: 3.594444 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [640/3951 (16%)]\tLoss: 3.616803 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [768/3951 (19%)]\tLoss: 3.721860 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [896/3951 (23%)]\tLoss: 3.451785 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1024/3951 (26%)]\tLoss: 3.619759 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1152/3951 (29%)]\tLoss: 3.496499 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1280/3951 (32%)]\tLoss: 3.542746 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1408/3951 (35%)]\tLoss: 3.515324 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1536/3951 (39%)]\tLoss: 3.463666 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1664/3951 (42%)]\tLoss: 3.685369 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1792/3951 (45%)]\tLoss: 3.633177 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [1920/3951 (48%)]\tLoss: 3.681665 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2048/3951 (52%)]\tLoss: 3.588244 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2176/3951 (55%)]\tLoss: 3.517003 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2304/3951 (58%)]\tLoss: 3.540930 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2432/3951 (61%)]\tLoss: 3.602856 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2560/3951 (65%)]\tLoss: 3.585169 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2688/3951 (68%)]\tLoss: 3.558726 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2816/3951 (71%)]\tLoss: 3.603759 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [2944/3951 (74%)]\tLoss: 3.605623 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3072/3951 (77%)]\tLoss: 3.525495 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3200/3951 (81%)]\tLoss: 3.513072 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3328/3951 (84%)]\tLoss: 3.574183 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3456/3951 (87%)]\tLoss: 3.604876 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3584/3951 (90%)]\tLoss: 3.522537 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3712/3951 (94%)]\tLoss: 3.545451 Lr: 1.2750399292720284e-06\n",
            "Train Epoch: 86 [3330/3951 (97%)]\tLoss: 3.520798 Lr: 1.2750399292720284e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 87 [0/3951 (0%)]\tLoss: 3.596160 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [128/3951 (3%)]\tLoss: 3.573591 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [256/3951 (6%)]\tLoss: 3.462104 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [384/3951 (10%)]\tLoss: 3.519866 Lr: 1.0899347581163222e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 87 [512/3951 (13%)]\tLoss: 3.609098 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [640/3951 (16%)]\tLoss: 3.549228 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [768/3951 (19%)]\tLoss: 3.586885 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [896/3951 (23%)]\tLoss: 3.465780 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1024/3951 (26%)]\tLoss: 4.060282 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1152/3951 (29%)]\tLoss: 3.556716 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1280/3951 (32%)]\tLoss: 3.508769 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1408/3951 (35%)]\tLoss: 3.473170 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1536/3951 (39%)]\tLoss: 3.586128 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1664/3951 (42%)]\tLoss: 3.507496 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1792/3951 (45%)]\tLoss: 3.598243 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [1920/3951 (48%)]\tLoss: 3.602019 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2048/3951 (52%)]\tLoss: 3.664254 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2176/3951 (55%)]\tLoss: 3.702701 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2304/3951 (58%)]\tLoss: 3.557565 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2432/3951 (61%)]\tLoss: 3.581818 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2560/3951 (65%)]\tLoss: 3.625194 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2688/3951 (68%)]\tLoss: 3.565515 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2816/3951 (71%)]\tLoss: 3.482252 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [2944/3951 (74%)]\tLoss: 3.650259 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3072/3951 (77%)]\tLoss: 3.484823 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3200/3951 (81%)]\tLoss: 3.519233 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3328/3951 (84%)]\tLoss: 3.604356 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3456/3951 (87%)]\tLoss: 3.494716 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3584/3951 (90%)]\tLoss: 3.543679 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3712/3951 (94%)]\tLoss: 3.612862 Lr: 1.0899347581163222e-06\n",
            "Train Epoch: 87 [3330/3951 (97%)]\tLoss: 3.506326 Lr: 1.0899347581163222e-06\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 88 [0/3951 (0%)]\tLoss: 3.673417 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [128/3951 (3%)]\tLoss: 3.533905 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [256/3951 (6%)]\tLoss: 3.535330 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [384/3951 (10%)]\tLoss: 3.461472 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [512/3951 (13%)]\tLoss: 3.514374 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [640/3951 (16%)]\tLoss: 3.605274 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [768/3951 (19%)]\tLoss: 3.472462 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [896/3951 (23%)]\tLoss: 3.457965 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1024/3951 (26%)]\tLoss: 3.694176 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1152/3951 (29%)]\tLoss: 3.604303 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1280/3951 (32%)]\tLoss: 3.610030 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1408/3951 (35%)]\tLoss: 3.486993 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1536/3951 (39%)]\tLoss: 3.580699 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1664/3951 (42%)]\tLoss: 3.639726 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1792/3951 (45%)]\tLoss: 3.554669 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [1920/3951 (48%)]\tLoss: 3.497509 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2048/3951 (52%)]\tLoss: 3.490135 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2176/3951 (55%)]\tLoss: 3.556662 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2304/3951 (58%)]\tLoss: 3.570759 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2432/3951 (61%)]\tLoss: 3.537502 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2560/3951 (65%)]\tLoss: 3.449623 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2688/3951 (68%)]\tLoss: 3.616236 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2816/3951 (71%)]\tLoss: 3.468532 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [2944/3951 (74%)]\tLoss: 3.733600 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3072/3951 (77%)]\tLoss: 3.631650 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3200/3951 (81%)]\tLoss: 3.593600 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3328/3951 (84%)]\tLoss: 3.611012 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3456/3951 (87%)]\tLoss: 3.500532 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3584/3951 (90%)]\tLoss: 3.607347 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3712/3951 (94%)]\tLoss: 3.444881 Lr: 9.185682617491865e-07\n",
            "Train Epoch: 88 [3330/3951 (97%)]\tLoss: 3.527267 Lr: 9.185682617491865e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 89 [0/3951 (0%)]\tLoss: 3.647220 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [128/3951 (3%)]\tLoss: 3.665213 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [256/3951 (6%)]\tLoss: 3.521285 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [384/3951 (10%)]\tLoss: 3.550785 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [512/3951 (13%)]\tLoss: 3.531598 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [640/3951 (16%)]\tLoss: 3.522564 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [768/3951 (19%)]\tLoss: 3.611709 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [896/3951 (23%)]\tLoss: 3.507792 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1024/3951 (26%)]\tLoss: 3.574806 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1152/3951 (29%)]\tLoss: 3.605490 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1280/3951 (32%)]\tLoss: 3.427744 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1408/3951 (35%)]\tLoss: 3.612501 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1536/3951 (39%)]\tLoss: 3.490681 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1664/3951 (42%)]\tLoss: 3.504154 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1792/3951 (45%)]\tLoss: 3.472409 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [1920/3951 (48%)]\tLoss: 3.598569 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2048/3951 (52%)]\tLoss: 3.532716 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2176/3951 (55%)]\tLoss: 3.685963 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2304/3951 (58%)]\tLoss: 3.607638 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2432/3951 (61%)]\tLoss: 3.539985 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2560/3951 (65%)]\tLoss: 3.611755 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2688/3951 (68%)]\tLoss: 3.487548 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2816/3951 (71%)]\tLoss: 3.572412 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [2944/3951 (74%)]\tLoss: 3.456967 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3072/3951 (77%)]\tLoss: 3.500827 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3200/3951 (81%)]\tLoss: 3.369416 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3328/3951 (84%)]\tLoss: 3.617667 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3456/3951 (87%)]\tLoss: 3.606611 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3584/3951 (90%)]\tLoss: 3.521261 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3712/3951 (94%)]\tLoss: 3.496993 Lr: 7.612046748871327e-07\n",
            "Train Epoch: 89 [3330/3951 (97%)]\tLoss: 3.583419 Lr: 7.612046748871327e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 90 [0/3951 (0%)]\tLoss: 3.623688 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [128/3951 (3%)]\tLoss: 3.567472 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [256/3951 (6%)]\tLoss: 3.522302 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [384/3951 (10%)]\tLoss: 3.545128 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [512/3951 (13%)]\tLoss: 3.486706 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [640/3951 (16%)]\tLoss: 3.546932 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [768/3951 (19%)]\tLoss: 3.518806 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [896/3951 (23%)]\tLoss: 3.495823 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1024/3951 (26%)]\tLoss: 3.534380 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1152/3951 (29%)]\tLoss: 3.641037 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1280/3951 (32%)]\tLoss: 3.531047 Lr: 6.180866407751595e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 90 [1408/3951 (35%)]\tLoss: 3.523001 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1536/3951 (39%)]\tLoss: 3.489159 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1664/3951 (42%)]\tLoss: 3.628497 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1792/3951 (45%)]\tLoss: 3.605005 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [1920/3951 (48%)]\tLoss: 3.476355 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2048/3951 (52%)]\tLoss: 3.673257 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2176/3951 (55%)]\tLoss: 3.505837 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2304/3951 (58%)]\tLoss: 3.444851 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2432/3951 (61%)]\tLoss: 3.478544 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2560/3951 (65%)]\tLoss: 3.574983 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2688/3951 (68%)]\tLoss: 3.591042 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2816/3951 (71%)]\tLoss: 3.501708 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [2944/3951 (74%)]\tLoss: 3.542884 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3072/3951 (77%)]\tLoss: 3.653628 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3200/3951 (81%)]\tLoss: 3.618425 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3328/3951 (84%)]\tLoss: 3.594369 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3456/3951 (87%)]\tLoss: 3.485049 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3584/3951 (90%)]\tLoss: 3.566845 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3712/3951 (94%)]\tLoss: 3.630779 Lr: 6.180866407751595e-07\n",
            "Train Epoch: 90 [3330/3951 (97%)]\tLoss: 3.513782 Lr: 6.180866407751595e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 67/406 (17%)\n",
            "\n",
            "Train Epoch: 91 [0/3951 (0%)]\tLoss: 3.585294 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [128/3951 (3%)]\tLoss: 3.628407 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [256/3951 (6%)]\tLoss: 3.670471 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [384/3951 (10%)]\tLoss: 3.407614 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [512/3951 (13%)]\tLoss: 3.693561 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [640/3951 (16%)]\tLoss: 3.413550 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [768/3951 (19%)]\tLoss: 3.479139 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [896/3951 (23%)]\tLoss: 3.520651 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1024/3951 (26%)]\tLoss: 3.596688 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1152/3951 (29%)]\tLoss: 3.615669 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1280/3951 (32%)]\tLoss: 3.481757 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1408/3951 (35%)]\tLoss: 3.635536 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1536/3951 (39%)]\tLoss: 3.478190 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1664/3951 (42%)]\tLoss: 3.381080 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1792/3951 (45%)]\tLoss: 3.535611 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [1920/3951 (48%)]\tLoss: 3.617649 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2048/3951 (52%)]\tLoss: 3.598578 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2176/3951 (55%)]\tLoss: 3.594266 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2304/3951 (58%)]\tLoss: 3.540460 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2432/3951 (61%)]\tLoss: 3.483092 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2560/3951 (65%)]\tLoss: 3.631705 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2688/3951 (68%)]\tLoss: 3.533032 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2816/3951 (71%)]\tLoss: 3.546451 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [2944/3951 (74%)]\tLoss: 3.524746 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3072/3951 (77%)]\tLoss: 3.629660 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3200/3951 (81%)]\tLoss: 3.627262 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3328/3951 (84%)]\tLoss: 3.633552 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3456/3951 (87%)]\tLoss: 3.543827 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3584/3951 (90%)]\tLoss: 3.545079 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3712/3951 (94%)]\tLoss: 3.446813 Lr: 4.894348370484648e-07\n",
            "Train Epoch: 91 [3330/3951 (97%)]\tLoss: 3.613642 Lr: 4.894348370484648e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 92 [0/3951 (0%)]\tLoss: 3.454123 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [128/3951 (3%)]\tLoss: 3.661738 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [256/3951 (6%)]\tLoss: 3.699277 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [384/3951 (10%)]\tLoss: 3.641133 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [512/3951 (13%)]\tLoss: 3.578739 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [640/3951 (16%)]\tLoss: 3.633742 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [768/3951 (19%)]\tLoss: 3.503947 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [896/3951 (23%)]\tLoss: 3.559091 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1024/3951 (26%)]\tLoss: 3.509735 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1152/3951 (29%)]\tLoss: 3.489265 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1280/3951 (32%)]\tLoss: 3.700214 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1408/3951 (35%)]\tLoss: 3.579505 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1536/3951 (39%)]\tLoss: 3.443476 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1664/3951 (42%)]\tLoss: 3.559316 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1792/3951 (45%)]\tLoss: 3.529704 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [1920/3951 (48%)]\tLoss: 3.595182 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2048/3951 (52%)]\tLoss: 3.574446 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2176/3951 (55%)]\tLoss: 3.558604 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2304/3951 (58%)]\tLoss: 3.543015 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2432/3951 (61%)]\tLoss: 3.562456 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2560/3951 (65%)]\tLoss: 3.585568 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2688/3951 (68%)]\tLoss: 3.588763 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2816/3951 (71%)]\tLoss: 3.531394 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [2944/3951 (74%)]\tLoss: 3.554040 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3072/3951 (77%)]\tLoss: 3.603069 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3200/3951 (81%)]\tLoss: 3.558427 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3328/3951 (84%)]\tLoss: 3.500974 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3456/3951 (87%)]\tLoss: 3.601553 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3584/3951 (90%)]\tLoss: 3.478471 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3712/3951 (94%)]\tLoss: 3.534014 Lr: 3.7544763546352834e-07\n",
            "Train Epoch: 92 [3330/3951 (97%)]\tLoss: 3.552047 Lr: 3.7544763546352834e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 93 [0/3951 (0%)]\tLoss: 3.636330 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [128/3951 (3%)]\tLoss: 3.542964 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [256/3951 (6%)]\tLoss: 3.479061 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [384/3951 (10%)]\tLoss: 3.502081 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [512/3951 (13%)]\tLoss: 3.568888 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [640/3951 (16%)]\tLoss: 3.543764 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [768/3951 (19%)]\tLoss: 3.686110 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [896/3951 (23%)]\tLoss: 3.578529 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1024/3951 (26%)]\tLoss: 3.601860 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1152/3951 (29%)]\tLoss: 3.583992 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1280/3951 (32%)]\tLoss: 3.617430 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1408/3951 (35%)]\tLoss: 3.627342 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1536/3951 (39%)]\tLoss: 3.474227 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1664/3951 (42%)]\tLoss: 3.499654 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1792/3951 (45%)]\tLoss: 3.762614 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [1920/3951 (48%)]\tLoss: 3.628921 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2048/3951 (52%)]\tLoss: 3.528634 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2176/3951 (55%)]\tLoss: 3.525949 Lr: 2.7630079602323447e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 93 [2304/3951 (58%)]\tLoss: 3.517625 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2432/3951 (61%)]\tLoss: 3.483687 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2560/3951 (65%)]\tLoss: 3.588231 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2688/3951 (68%)]\tLoss: 3.499282 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2816/3951 (71%)]\tLoss: 3.549682 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [2944/3951 (74%)]\tLoss: 3.585296 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3072/3951 (77%)]\tLoss: 3.448361 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3200/3951 (81%)]\tLoss: 3.545334 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3328/3951 (84%)]\tLoss: 3.626443 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3456/3951 (87%)]\tLoss: 3.588254 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3584/3951 (90%)]\tLoss: 3.586230 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3712/3951 (94%)]\tLoss: 3.473097 Lr: 2.7630079602323447e-07\n",
            "Train Epoch: 93 [3330/3951 (97%)]\tLoss: 3.535178 Lr: 2.7630079602323447e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 68/406 (17%)\n",
            "\n",
            "Train Epoch: 94 [0/3951 (0%)]\tLoss: 3.648659 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [128/3951 (3%)]\tLoss: 3.566607 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [256/3951 (6%)]\tLoss: 3.432139 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [384/3951 (10%)]\tLoss: 3.601375 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [512/3951 (13%)]\tLoss: 3.541697 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [640/3951 (16%)]\tLoss: 3.589472 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [768/3951 (19%)]\tLoss: 3.659714 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [896/3951 (23%)]\tLoss: 3.700933 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1024/3951 (26%)]\tLoss: 3.481863 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1152/3951 (29%)]\tLoss: 3.462884 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1280/3951 (32%)]\tLoss: 3.577739 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1408/3951 (35%)]\tLoss: 3.549001 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1536/3951 (39%)]\tLoss: 3.503479 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1664/3951 (42%)]\tLoss: 3.671159 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1792/3951 (45%)]\tLoss: 3.551661 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [1920/3951 (48%)]\tLoss: 3.497625 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2048/3951 (52%)]\tLoss: 3.639407 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2176/3951 (55%)]\tLoss: 3.501301 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2304/3951 (58%)]\tLoss: 3.583683 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2432/3951 (61%)]\tLoss: 3.543109 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2560/3951 (65%)]\tLoss: 3.501547 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2688/3951 (68%)]\tLoss: 3.621251 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2816/3951 (71%)]\tLoss: 3.494215 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [2944/3951 (74%)]\tLoss: 3.467995 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3072/3951 (77%)]\tLoss: 3.548824 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3200/3951 (81%)]\tLoss: 3.710969 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3328/3951 (84%)]\tLoss: 3.616749 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3456/3951 (87%)]\tLoss: 3.629245 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3584/3951 (90%)]\tLoss: 3.718015 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3712/3951 (94%)]\tLoss: 3.632467 Lr: 1.921471959676957e-07\n",
            "Train Epoch: 94 [3330/3951 (97%)]\tLoss: 3.614098 Lr: 1.921471959676957e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n",
            "Train Epoch: 95 [0/3951 (0%)]\tLoss: 3.526701 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [128/3951 (3%)]\tLoss: 3.497271 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [256/3951 (6%)]\tLoss: 3.616907 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [384/3951 (10%)]\tLoss: 3.619114 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [512/3951 (13%)]\tLoss: 3.493554 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [640/3951 (16%)]\tLoss: 3.657891 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [768/3951 (19%)]\tLoss: 3.534845 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [896/3951 (23%)]\tLoss: 3.569630 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1024/3951 (26%)]\tLoss: 3.557518 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1152/3951 (29%)]\tLoss: 3.512336 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1280/3951 (32%)]\tLoss: 3.499602 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1408/3951 (35%)]\tLoss: 3.636485 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1536/3951 (39%)]\tLoss: 3.538763 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1664/3951 (42%)]\tLoss: 3.560982 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1792/3951 (45%)]\tLoss: 3.561858 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [1920/3951 (48%)]\tLoss: 3.556402 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2048/3951 (52%)]\tLoss: 3.603511 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2176/3951 (55%)]\tLoss: 3.518903 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2304/3951 (58%)]\tLoss: 3.527272 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2432/3951 (61%)]\tLoss: 3.468908 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2560/3951 (65%)]\tLoss: 3.591457 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2688/3951 (68%)]\tLoss: 3.451826 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2816/3951 (71%)]\tLoss: 3.539660 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [2944/3951 (74%)]\tLoss: 3.622931 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3072/3951 (77%)]\tLoss: 3.402397 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3200/3951 (81%)]\tLoss: 3.668598 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3328/3951 (84%)]\tLoss: 3.516617 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3456/3951 (87%)]\tLoss: 3.969211 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3584/3951 (90%)]\tLoss: 3.459392 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3712/3951 (94%)]\tLoss: 3.617150 Lr: 1.231165940486234e-07\n",
            "Train Epoch: 95 [3330/3951 (97%)]\tLoss: 3.564810 Lr: 1.231165940486234e-07\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n",
            "Train Epoch: 96 [0/3951 (0%)]\tLoss: 3.675804 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [128/3951 (3%)]\tLoss: 3.565065 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [256/3951 (6%)]\tLoss: 3.520575 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [384/3951 (10%)]\tLoss: 3.594357 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [512/3951 (13%)]\tLoss: 3.578486 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [640/3951 (16%)]\tLoss: 3.640283 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [768/3951 (19%)]\tLoss: 3.594005 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [896/3951 (23%)]\tLoss: 3.643137 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1024/3951 (26%)]\tLoss: 3.534404 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1152/3951 (29%)]\tLoss: 3.600121 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1280/3951 (32%)]\tLoss: 3.581455 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1408/3951 (35%)]\tLoss: 3.538683 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1536/3951 (39%)]\tLoss: 3.590497 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1664/3951 (42%)]\tLoss: 3.616566 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1792/3951 (45%)]\tLoss: 3.415002 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [1920/3951 (48%)]\tLoss: 3.579239 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2048/3951 (52%)]\tLoss: 3.465384 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2176/3951 (55%)]\tLoss: 3.567983 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2304/3951 (58%)]\tLoss: 3.617923 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2432/3951 (61%)]\tLoss: 3.534066 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2560/3951 (65%)]\tLoss: 3.552978 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2688/3951 (68%)]\tLoss: 3.548419 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2816/3951 (71%)]\tLoss: 3.471625 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [2944/3951 (74%)]\tLoss: 3.504783 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [3072/3951 (77%)]\tLoss: 3.540375 Lr: 6.931543045073708e-08\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n",
            "Train Epoch: 96 [3200/3951 (81%)]\tLoss: 3.549812 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [3328/3951 (84%)]\tLoss: 3.562271 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [3456/3951 (87%)]\tLoss: 3.580398 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [3584/3951 (90%)]\tLoss: 3.560613 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [3712/3951 (94%)]\tLoss: 3.576680 Lr: 6.931543045073708e-08\n",
            "Train Epoch: 96 [3330/3951 (97%)]\tLoss: 3.625775 Lr: 6.931543045073708e-08\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n",
            "Train Epoch: 97 [0/3951 (0%)]\tLoss: 3.563663 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [128/3951 (3%)]\tLoss: 3.558177 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [256/3951 (6%)]\tLoss: 3.494979 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [384/3951 (10%)]\tLoss: 3.651336 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [512/3951 (13%)]\tLoss: 3.534004 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [640/3951 (16%)]\tLoss: 3.480179 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [768/3951 (19%)]\tLoss: 3.637369 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [896/3951 (23%)]\tLoss: 3.627614 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1024/3951 (26%)]\tLoss: 3.634739 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1152/3951 (29%)]\tLoss: 3.556355 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1280/3951 (32%)]\tLoss: 3.617800 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1408/3951 (35%)]\tLoss: 3.481072 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1536/3951 (39%)]\tLoss: 3.613356 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1664/3951 (42%)]\tLoss: 3.502950 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1792/3951 (45%)]\tLoss: 3.598129 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [1920/3951 (48%)]\tLoss: 3.581809 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2048/3951 (52%)]\tLoss: 3.557102 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2176/3951 (55%)]\tLoss: 3.654460 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2304/3951 (58%)]\tLoss: 3.485927 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2432/3951 (61%)]\tLoss: 3.579996 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2560/3951 (65%)]\tLoss: 3.610998 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2688/3951 (68%)]\tLoss: 3.641888 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2816/3951 (71%)]\tLoss: 3.365176 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [2944/3951 (74%)]\tLoss: 3.483700 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3072/3951 (77%)]\tLoss: 3.566385 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3200/3951 (81%)]\tLoss: 3.555837 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3328/3951 (84%)]\tLoss: 3.674445 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3456/3951 (87%)]\tLoss: 3.509382 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3584/3951 (90%)]\tLoss: 3.490902 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3712/3951 (94%)]\tLoss: 3.621729 Lr: 3.082666266872036e-08\n",
            "Train Epoch: 97 [3330/3951 (97%)]\tLoss: 3.485971 Lr: 3.082666266872036e-08\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n",
            "Train Epoch: 98 [0/3951 (0%)]\tLoss: 3.447293 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [128/3951 (3%)]\tLoss: 3.552150 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [256/3951 (6%)]\tLoss: 3.575506 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [384/3951 (10%)]\tLoss: 3.560883 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [512/3951 (13%)]\tLoss: 3.604699 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [640/3951 (16%)]\tLoss: 3.493351 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [768/3951 (19%)]\tLoss: 3.571124 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [896/3951 (23%)]\tLoss: 3.592901 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1024/3951 (26%)]\tLoss: 3.601742 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1152/3951 (29%)]\tLoss: 3.689396 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1280/3951 (32%)]\tLoss: 3.611740 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1408/3951 (35%)]\tLoss: 3.670115 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1536/3951 (39%)]\tLoss: 3.557076 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1664/3951 (42%)]\tLoss: 3.543056 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1792/3951 (45%)]\tLoss: 3.562483 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [1920/3951 (48%)]\tLoss: 3.488510 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2048/3951 (52%)]\tLoss: 3.546008 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2176/3951 (55%)]\tLoss: 3.538048 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2304/3951 (58%)]\tLoss: 3.551662 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2432/3951 (61%)]\tLoss: 3.507899 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2560/3951 (65%)]\tLoss: 3.634976 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2688/3951 (68%)]\tLoss: 3.713211 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2816/3951 (71%)]\tLoss: 3.594742 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [2944/3951 (74%)]\tLoss: 3.640447 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3072/3951 (77%)]\tLoss: 3.502492 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3200/3951 (81%)]\tLoss: 3.523248 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3328/3951 (84%)]\tLoss: 3.486894 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3456/3951 (87%)]\tLoss: 3.562466 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3584/3951 (90%)]\tLoss: 3.407336 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3712/3951 (94%)]\tLoss: 3.519458 Lr: 7.70963759277099e-09\n",
            "Train Epoch: 98 [3330/3951 (97%)]\tLoss: 3.523170 Lr: 7.70963759277099e-09\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n",
            "Train Epoch: 99 [0/3951 (0%)]\tLoss: 3.671310 Lr: 0.0\n",
            "Train Epoch: 99 [128/3951 (3%)]\tLoss: 3.521410 Lr: 0.0\n",
            "Train Epoch: 99 [256/3951 (6%)]\tLoss: 3.455045 Lr: 0.0\n",
            "Train Epoch: 99 [384/3951 (10%)]\tLoss: 3.528702 Lr: 0.0\n",
            "Train Epoch: 99 [512/3951 (13%)]\tLoss: 3.644734 Lr: 0.0\n",
            "Train Epoch: 99 [640/3951 (16%)]\tLoss: 3.578177 Lr: 0.0\n",
            "Train Epoch: 99 [768/3951 (19%)]\tLoss: 3.517468 Lr: 0.0\n",
            "Train Epoch: 99 [896/3951 (23%)]\tLoss: 3.461076 Lr: 0.0\n",
            "Train Epoch: 99 [1024/3951 (26%)]\tLoss: 3.532680 Lr: 0.0\n",
            "Train Epoch: 99 [1152/3951 (29%)]\tLoss: 3.557193 Lr: 0.0\n",
            "Train Epoch: 99 [1280/3951 (32%)]\tLoss: 3.707054 Lr: 0.0\n",
            "Train Epoch: 99 [1408/3951 (35%)]\tLoss: 3.501834 Lr: 0.0\n",
            "Train Epoch: 99 [1536/3951 (39%)]\tLoss: 3.535702 Lr: 0.0\n",
            "Train Epoch: 99 [1664/3951 (42%)]\tLoss: 3.523625 Lr: 0.0\n",
            "Train Epoch: 99 [1792/3951 (45%)]\tLoss: 3.722684 Lr: 0.0\n",
            "Train Epoch: 99 [1920/3951 (48%)]\tLoss: 3.489486 Lr: 0.0\n",
            "Train Epoch: 99 [2048/3951 (52%)]\tLoss: 3.510609 Lr: 0.0\n",
            "Train Epoch: 99 [2176/3951 (55%)]\tLoss: 3.616230 Lr: 0.0\n",
            "Train Epoch: 99 [2304/3951 (58%)]\tLoss: 3.661825 Lr: 0.0\n",
            "Train Epoch: 99 [2432/3951 (61%)]\tLoss: 3.533631 Lr: 0.0\n",
            "Train Epoch: 99 [2560/3951 (65%)]\tLoss: 3.523598 Lr: 0.0\n",
            "Train Epoch: 99 [2688/3951 (68%)]\tLoss: 3.708216 Lr: 0.0\n",
            "Train Epoch: 99 [2816/3951 (71%)]\tLoss: 3.651890 Lr: 0.0\n",
            "Train Epoch: 99 [2944/3951 (74%)]\tLoss: 3.670732 Lr: 0.0\n",
            "Train Epoch: 99 [3072/3951 (77%)]\tLoss: 3.382885 Lr: 0.0\n",
            "Train Epoch: 99 [3200/3951 (81%)]\tLoss: 3.493326 Lr: 0.0\n",
            "Train Epoch: 99 [3328/3951 (84%)]\tLoss: 3.636770 Lr: 0.0\n",
            "Train Epoch: 99 [3456/3951 (87%)]\tLoss: 3.538836 Lr: 0.0\n",
            "Train Epoch: 99 [3584/3951 (90%)]\tLoss: 3.580673 Lr: 0.0\n",
            "Train Epoch: 99 [3712/3951 (94%)]\tLoss: 3.524587 Lr: 0.0\n",
            "Train Epoch: 99 [3330/3951 (97%)]\tLoss: 3.530746 Lr: 0.0\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 69/406 (17%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8c8oyfhRxEQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
        "model = Net().to('cuda')\n",
        "state = torch.load('/content/checkpoint/model4433.pth')\n",
        "model.load_state_dict(state['state_dict'])\n",
        "optimizer.load_state_dict(state['optimizer'])"
      ],
      "metadata": {
        "id": "XoYcHzfWosQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate(model) #try #2232 #1178 #2387         \n",
        "#2046 2790\n",
        "#36 2511 1922 1426 1457 1829 2356\n",
        "#35 2700 2666 2604 2604 2500 2294 2232 1085 1674 1767 1800 2015 2821"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8szEYBD8ecB",
        "outputId": "9b1e7082-5ec8-4703-99c9-506cc7c6add9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0556, Accuracy: 130/406 (32%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/checkpoint_p2"
      ],
      "metadata": {
        "id": "iHXFhpGgFMc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r /content/checkpoint/model2511.pth /content/checkpoint_p2\n",
        "! cp -r /content/checkpoint/model1922.pth /content/checkpoint_p2\n",
        "! cp -r /content/checkpoint/model1426.pth /content/checkpoint_p2\n",
        "! cp -r /content/checkpoint/model1457.pth /content/checkpoint_p2\n",
        "! cp -r /content/checkpoint/model1829.pth /content/checkpoint_p2\n",
        "! cp -r /content/checkpoint/model2356.pth /content/checkpoint_p2"
      ],
      "metadata": {
        "id": "59JaBksXvNMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r /content/checkpoint/model2046.pth /content/drive/MyDrive/DLCV/HW4\n",
        "! cp -r /content/checkpoint/model2790.pth /content/drive/MyDrive/DLCV/HW4"
      ],
      "metadata": {
        "id": "6P4leOLJGE70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r /content/drive/MyDrive/DLCV/HW4/model2790.pth /content/checkpoint_p2"
      ],
      "metadata": {
        "id": "qFbEUQdBN4de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
        "model = Net().to('cuda')\n",
        "state = torch.load('/content/checkpoint_p2/model2790.pth')\n",
        "model.load_state_dict(state['state_dict'])\n",
        "\n",
        "validate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ20tFOoONgH",
        "outputId": "882ca937-f5d9-451e-876e-4699191562ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0235, Accuracy: 152/406 (37%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=False)\n",
        "\n",
        "model1 = Net().to('cuda')\n",
        "model2 = Net().to('cuda')\n",
        "model3 = Net().to('cuda')\n",
        "model4 = Net().to('cuda')\n",
        "model5 = Net().to('cuda')\n",
        "model6 = Net().to('cuda')\n",
        "\n",
        "state1 = torch.load('/content/checkpoint_p2/model2511.pth')\n",
        "state2 = torch.load('/content/checkpoint_p2/model1922.pth')\n",
        "state3 = torch.load('/content/checkpoint_p2/model1426.pth')\n",
        "state4 = torch.load('/content/checkpoint_p2/model1457.pth')\n",
        "state5 = torch.load('/content/checkpoint_p2/model1829.pth')\n",
        "state6 = torch.load('/content/checkpoint_p2/model2356.pth')\n",
        "\n",
        "model1.load_state_dict(state1['state_dict'])\n",
        "model2.load_state_dict(state2['state_dict'])\n",
        "model3.load_state_dict(state3['state_dict'])\n",
        "model4.load_state_dict(state4['state_dict'])\n",
        "model5.load_state_dict(state5['state_dict'])\n",
        "model6.load_state_dict(state6['state_dict'])\n",
        "\n",
        "# optimizer.load_state_dict(state1['optimizer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TIud3sWv4FY",
        "outputId": "3d4a35be-5695-4615-9137-091190f8f352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model1.eval()\n",
        "model2.eval()\n",
        "model3.eval()\n",
        "model4.eval()\n",
        "model5.eval()\n",
        "model6.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in valid_loader:\n",
        "        data, target = data.to('cuda'), target.to('cuda')\n",
        "        output1 = model1(data)\n",
        "        output2 = model2(data)\n",
        "        output3 = model3(data)\n",
        "        output4 = model4(data)\n",
        "        output5 = model5(data)\n",
        "        output6 = model6(data)\n",
        "        # print(output.shape, target.shape)\n",
        "        test_loss += criterion(output1, target).item()\n",
        "        pred1 = output1.max(1, keepdim=True)[1]\n",
        "        pred2 = output2.max(1, keepdim=True)[1]\n",
        "        pred3 = output3.max(1, keepdim=True)[1]\n",
        "        pred4 = output4.max(1, keepdim=True)[1]\n",
        "        pred5 = output5.max(1, keepdim=True)[1]\n",
        "        pred6 = output6.max(1, keepdim=True)[1]\n",
        "        # print(pred1.shape)\n",
        "        temp = torch.cat((pred1, pred2 ,pred3, pred3, pred3, pred3, pred4, pred5, pred6), 1)\n",
        "        # print(temp.shape)\n",
        "        pred_list = []\n",
        "        \n",
        "        for id, i in enumerate(temp):\n",
        "          i = i.to('cpu').numpy()\n",
        "          # print(i, np.bincount(i).argmax())\n",
        "          pred_list.append(np.bincount(i).argmax())\n",
        "          pred = torch.tensor(pred_list).to('cuda')\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "test_loss /= len(valid_loader.dataset)\n",
        "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(valid_loader.dataset),\n",
        "    100. * correct / len(valid_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JnWYNPJIEdA",
        "outputId": "551b70e6-d17a-49f5-dbb1-1d3f0023848a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0322, Accuracy: 148/406 (36%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv-bFUH0v-Fv",
        "outputId": "163a5ee6-6acb-4d77-fe43-6e6604bce3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 145/406 (36%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}